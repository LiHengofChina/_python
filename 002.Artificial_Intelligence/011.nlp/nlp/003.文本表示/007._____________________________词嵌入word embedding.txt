
//========================================================== 词嵌入（word embedding），双叫分布式表示
//========================================================== 词嵌入（word embedding），双叫分布式表示

/**
	把 "一张人脸图" 转换成128维的向量，这个过程叫 嵌入
*/

词嵌入（word embedding）是一种词的 "向量化表示方式"，能保留 "丰富的语义" 信息。

//=============================================
//=============================================

两个词的 "词向量" 相似度比较高，
我们就可以判断 它们的语义相似度比较高

//============================================= 示例：
//============================================= 示例：
		颜色	味道	功能 		
咖啡：   1		 1		  1
茶叶：   1		 1		  1
牛奶：	-1		-1		 -1


咖啡 和 茶叶 欧式距离为0，相似度就比较高 
咖啡 和 牛奶 欧式距离为sqrt(..)，相似度就比较低


// 如果 "词向量" 相似度比较高，那么 "它们的语义相似"度 就很高。


//==================
//==================
（1）上面长度是3，
	 真正用的时候，这个长度一般是 50~300 维之间
	 长度越长，语义信息越丰富
	 长度越短，语义信息越少

（2）在使用时，看不到

	 咖啡、茶叶、牛奶这些，

	 只看得到一组词向量，属性代表的是什么 "是不知道的" 
	 虽然不知道，但是可以比较 "相似性"，

	 这两个 "向量的相似性" 比较高，那么 "它代表的词" 相似度就比较高。
	 
	 在训练阶段 "两个词" 相似度比较高，
	 那么就尽可能的把 "它每个值" 调整的 "非常相似"。
		如：
			 Apple    Orange
			  0.00     0.01
			 -0.01     0.00
			  0.03    -0.02
			  0.95     0.97







//============================================= 运算
//============================================= 运算
使用 "词嵌入" 表示后，就可以进行计算， 减法运算。

如： 男人-女人  约等于  国王-女人。


//============================================= 降维处理：词向量的几何表示
//============================================= 降维处理：词向量的几何表示

"词嵌入" 一般 表示为 "高维向量"
然后，我们可以通过 "降维处理"，
在 "二维空间" 或 "三维空间" 里面展示出来。

就可以很方便的观察出来，
"离得近的词" 语义相似度就 "比较高"

//============================================= 特点
//============================================= 特点 
（1）稠密向量：稠密矩阵，一个词就是一个向量，表示效率高
（2）能表征 相似度

//============================================= 自动化生成词向量表示
//============================================= 自动化生成词向量表示
这些值 （1，-1） 表示的是近似值，不是准确值。
一定有一种自动化的算法，去生成这种 "词向量" 表示。



"词向量表示" 要通过 "单独的、专门的模型"  训练才可以得到 "词向量"
//也就是 "神经网络语言模型" 才能训练出来




