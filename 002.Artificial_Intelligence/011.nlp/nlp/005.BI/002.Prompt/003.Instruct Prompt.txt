
上述 "通过举例" 让LLM来理解具体的任务是什么？
相当于给 "LLM进行示范" ，让他按照 "上述的模板" 来进行 "词语接龙"，

//====================================== Instruct Prompt （翻译：指导性、指示性提示）
//====================================== Instruct Prompt （翻译：指导性、指示性提示）




那为什么不能 "直接用语言" 来对我们想要的 "任务进行描述"，
让模型来 "理解我们想要做的事情" 是什么呢？

更何况这样的方法在多数场合中会更加有效，减少context的长度，降低模型推理的成本。

//====================================== chatgpt
//====================================== chatgpt
chatgpt对于 "之前的模型" 的 "很大的改进工作" 是体现在这一步，
具体的工作是RLHF（基于 "人类反馈" 的 "强化学习"），
		//RLHF代表的是“Reinforcement Learning from Human Feedback”（基于人类反馈的强化学习）


通过一些 "高质量的问答对" 和 "对于答案的回复的排序" 来训练一个 "人类偏好的奖励模型"，
通过 "这个奖励模型" 来近一步 "反馈指导大语言模型的输出" 的 "回答的质量"。

在 "chatgpt时代" 一般的 "提问方式" 如下所示:
	请为 "这段话的情绪" 做一个分类，是 "正向积极的" 还是 "消极的"。

//====================================== in-context instruction learning
//====================================== in-context instruction learning
将 "Few shot" 和 "Instruct" 结合起来的方式就是 "给出几个示例"，
并且示例是 "描述具体的任务"（而不是 "一步步来做具体的分析演示"）**********这句话重点***************
这个方法称为 in-context instruction learning，

该方法可以 "近一步提高LLM" 对任务的 "理解能力"，
提问方式如下:  每个例子包括三个部分
（1）定义（Definition）：明确任务的 "目的" 和需要 "区分的类别"。
（2）输入（Input）	   ：实际的例句或情境，模型需要 "根据定义" 来 "处理这些输入"。
（3）输出（Output）    ：基于 "输入" 和 "定义"，"预期模型" 应该给出的 "正确答案"。

//===示例1
定义：确定对话的说话人是 "代理人" 或 "客户"。	    
输入：我已经成功地为您订了票。 
输出：代理人。 //分析：因为通常是代理人为客户订票

//===示例2
定义：确定问题所要求的类别 "数量" 或 "位置"。 	    
输入：美国最古老的建筑是什么？ 
输出：位置。   //分析：问题是想知道这个建筑的位置，而不是数量

//===示例3

定义：将电影评论的情绪分类为 "积极的" 或 "消极的"。 
输入：我敢打赌，这个视频游戏比电影有趣多了。 
输出：积极的	//分析：根据句子积极的语气

/**
这段话描述：
	一种训练或指导人工智能（AI）模型 "理解和执行" 特定任务的 "方法"，
	通常用于自然语言处理（NLP）任务。
	这里的方法使用定义、输入、和输出（输出）的形式来 "示范" 怎样引导模型
*/


