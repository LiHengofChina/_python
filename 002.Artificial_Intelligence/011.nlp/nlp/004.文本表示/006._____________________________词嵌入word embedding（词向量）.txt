
//========================================================== 词嵌入（word embedding），又叫分布式表示
//========================================================== 词嵌入（word embedding），又叫分布式表示


/**
	把 "一张人脸图" 转换成128维的向量，这个过程叫 嵌入
*/

词嵌入（word embedding）是一种词的 "向量化表示方式"，能保留 "丰富的语义" 信息。




//=============================================
//=============================================

两个词的 "词向量" 相似度比较高，
我们就可以判断 它们的语义相似度比较高

//============================================= 示例1：
//============================================= 示例1：

		颜色	味道	功能 		
咖啡：   1		 1		  1
茶叶：   1		 1		  1
牛奶：	-1		-1		 -1


//咖啡黑色  1 ，牛奶白色  -1
//咖啡苦    1 ，牛奶甜    -1


咖啡 和 茶叶 欧式距离为0，相似度就比较高 
咖啡 和 牛奶 欧式距离为sqrt(..)，相似度就比较低

咖啡、茶叶、牛奶这些，
只看得到一组词向量，属性代表的是什么 "是不知道的" 
虽然不知道，但是可以比较 "相似性"，

//================================================================== 示例2：见PDF
//================================================================== 示例2：见PDF

把这些值拿出来作为这些词的 "词向量"
	如：把 0.00	-0.01	 0.03	0.95 作为 Apple  的词向量
		把 0.01	 0.00	-0.02	0.97 作为 Orange 的词向量

		//这里是长度为4的词向量

		这里，这4个值，分别：性别、名字、年龄、食物
		但是，在实际使用时，是看不到这些属性的，它们只是被训练出来，
		//不表示具体的含义
		虽然不知道，但是可以比较 "相似性"，


		 这两个 "向量的相似性" 比较高，那么 "它代表的词" 相似度就比较高。

		 在训练阶段 "两个词" 相似度比较高，
		 那么就尽可能的把 "它每个值" 调整的 "非常相似"。
			如：
				 Apple    Orange
				  0.00     0.01
				 -0.01     0.00
				  0.03    -0.02
				  0.95     0.97

		// 如果 "词向量" 相似度比较高，那么 "它们的语义相似"度 就很高。

		真正用的时候，这个长度一般是 50~300 维之间
		长度越长，语义信息越丰富
		长度越短，语义信息越少

//================================================================== 运算：语义计算
//================================================================== 运算：语义计算
使用 "词嵌入" 表示后，就可以进行计算， 减法运算。

//男人减去女人

					 [  -1  ]     [ 1    ]      [ -1    ]            [ -2  ]
                     [ 0.01 ]     [ 0.02 ]      [ -0.01 ]            [  0  ]
E_man - E_woman =    [ 0.03 ]  -  [ 0.02 ]  =   [ 0.01  ]   ~约等于  [  0  ]
                     [ 0.09 ]     [ 0.01 ]      [ 0.08  ]            [  0  ]

//国王减去女王


					 [ -0.95 ]    [ 0.97 ]     [ -1.92  ]            [ -2  ]
                     [  0.93 ]    [ 0.85 ]     [ -0.02  ]            [  0  ]
E_king - E_queen =   [  0.70 ]  - [ 0.69 ]  =  [  0.01  ]   ~约等于  [  0  ]
                     [  0.02 ]    [ 0.01 ]     [  0.01  ]            [  0  ]


"男人和女王"的差异 ，就相当于 "国王和女王" 的差异


语义计算，把 "向量计算的结果"，直接拿来当着 "语义信息"








如： 男人-女人  约等于  国王-女人。


//================================================================== 降维处理：词向量的几何表示
//================================================================== 降维处理：词向量的几何表示


"词嵌入" 一般 表示为 "高维向量"
然后，我们可以通过 "降维处理"，
在 "二维空间" 或 "三维空间" 里面展示出来。

就可以很方便的观察出来，
"离得近的词" 语义相似度就 "比较高"


//================================================================== 特点
//================================================================== 特点

（1）表示效率高：稠密向量——稠密矩阵，一个词就是一个向量，表示效率高

（2）能表征 相似度，
			//如：Apple 和 Orange 两个词向量的相似度比较高
			//不管是它们的 欧式距离 ，还是余弦相似度，都比较高。		

（3）泛化能力更好，支持语义级的计算


//================================================================== 缺点
//================================================================== 缺点

这些值：
		[  -1  ]
		[ 0.01 ]
		[ 0.03 ]
		[ 0.09 ]

不是人安的，是通过 "自动化算法" 生成 "词向量" 表示，

		//是通过专门的训练得到的，值是多少
		//通过 "神经网络"

使用  "神经网络语言模型（NNLM，Neural Network Language Model）" 在 "大型语料库" 上训练词向量，
如：
	NNLM
	Word2Vec//NNLM 的升级 版
	GloVe（Global Vectors for Word Representation）
	FastText
	等等

//=================
//=================

 "语义相似度" 很高，那么 "词向量相似度" 就很高，反之也成立。




//================================================================== 自动化生成词向量表示
//================================================================== 自动化生成词向量表示


这些值，表示的是近似值，不是准确值。
一定有一种自动化的算法，去生成这种 "词向量" 表示。

"词向量表示" 要通过 "单独的、专门的模型"  训练才可以得到 "词向量"

//也就是 "神经网络语言模型" 才能训练出来

使用
使用 神经网络语言模型-NNLM 放在一个庞大的语料库里面，才能训练出来

使用 "神经网络语言模型（NNLM，Neural Network Language Model）" 在 "大型语料库" 上 "训练词向量"

//================================================================== 总结
//================================================================== 总结


用一系列 "属性" 来描述一个词。
具体 "属性" 是训练出来的，没有具体含义。

如果 "某些词" 的 "属性相似度比较高"，那么它 "词也就比较相似"，
如果 "某些词" 的 "某些词语比较相似"，那么在 "训练的时候"，就让这些属性趋同。
			//就像生物进化一样，生活环境相同，最后进化就趋同。
			//词语也是一样的

训练完了以后，就可以把它用在 "特征提取" 里面，文本表示里面。

//======================
//======================

"语料库" 里面包含大量的词语搭配，前后关系，句子怎么使用，词语使用的语境，
用一个 "超大的语料库" 去训练，

（1）发现词语的用法相同，那么词向量就相同。
（2）反之，不知道这两个词是否相同，那么就看它的词向量是否相同。




