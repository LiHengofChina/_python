
//===================================================== N-Gram
//===================================================== N-Gram

把n个词作为一组， "1个组合" 就是 "1个gram"

所谓 N-gram(N元)模型，


************关键点*************
计算 "概率" 时，忽略 "长度大于N的上下文词" 的影响。


//==================== 基本思想
//==================== 基本思想

N-Gram基本思想是将 "文本里面的内容" 按照 "字节" 进行 "大小为n" 的 "滑动窗口" 操作，
形成了 "长度是n" 的 "字节片段序列"。

然后把 "低频率" 的 "组合"去掉。
然后把 "高频率" 的 "组合"保留。

//==================== N 的取值
//==================== N 的取值
N=1 时 			//称为 一元模型(Uni-gram Mode)。
N=2, Bi-gram	//称为 二元模型(Uni-gram Mode)。
N=3, Trr-gram	//称为 三元模型(Uni-gram Mode)。

//==================== 例如：I love deep learning
//==================== 例如：I love deep learning

Bi-gram:   //两个词一组
			{I, love}, 
			{love, deep}, 
			{deep, learning}

Tri-gram:  //三个词一组
			{I, love, deep}, 
			{love deep learning}

//===================================================== N-Gram 和 TextRank 比较
//===================================================== N-Gram 和 TextRank 比较
TextRank是一种基于图的排序算法，TextRank和N-Gram是两种不同的自然语言处理技术


TextRank 会利用滑动窗口在文本中捕捉词语之间的共现关系


N-Gram是一种基于统计的语言模型，用于文本的各种任务，如语言识别、文本生成、拼写检查等。
N-Gram模型通过统计和分析文本中N个连续项的出现频率来预测下一个项的出现概率。
