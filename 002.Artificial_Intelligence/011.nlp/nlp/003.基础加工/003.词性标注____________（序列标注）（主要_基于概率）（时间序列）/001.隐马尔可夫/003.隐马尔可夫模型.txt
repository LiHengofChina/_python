

隐马尔可夫模型（Hidden Markov Model，HMM）

"时间序列"的"概率模型"，

它是：传统机器 学习中 "最复杂模型之一"

//===========================================================
//===========================================================



//=========================================================== 隐马尔可夫模型
//=========================================================== 隐马尔可夫模型
它描述  "两个随机过程"
		（1）一个看不到：//隐藏序列
			 （1）后面有 "三个色子" 在抓 "后面的色子" 的时候，不知道抓 "哪一个"，每一个概率是 1/3， //这是一个随机过程
			 （2）抓到色子之后，打出几点也不确定
					（1）6面体：   1 2 3 4 5 6
					（2）4面休：   1 2 3 4 
					（3）8面体：   1 2 3 4 5 6 7 8	
			  //"隐藏序列": 抓到哪个色子，是看不到的称为隐藏序列。

		（2）一个看得到 //观测序列
			 （1）假如前面打出 2 5 8 ，这就是可以看到的序列
			  //"可观测序列"


//=========================================================== 隐马尔可夫模型 的 作用
//=========================================================== 隐马尔可夫模型 的 作用


根据 "可观测序列" 预测  "隐藏序列" 


//通过 "已知序列" 反推 ""

//============================== 怎么计算的？
//============================== 怎么计算的？

（1）抓 "每个色子" 的概率是 1/3

（2）每个色子的值的概率
		6 面体各个值   1/6
		4 面体各个值   1/4
		8 面体各个值   1/8

//====================
抓到 "第一个色子" 之后，能算出来打出 1 2 3 4 5 6 的概率是多少，各占 1/6
抓到 "第二个色子" 之后，能算出来打出 1 2 3 4 的概率是多少，各占 1/4
抓到 "第三个色子" 之后，能算出来打出 1 2 3 4 5 6 7 8 的概率是多少，各占 1/8

//==================== 计算：打出 2 5 8 的概率

（1）计算打出 2 的 概率
     (1/3 * 1/6 ) + (1/3 * 1/4 ) + (1/3 * 1/8 )  = p1
	 //这3个色子都可能打出 "2点"


（2）计算打出 5 的 概率
     (1/3 * 1/6 ) + (1/3 * 0 ) + (1/3 * 1/8 )   = p2
	 //只有第1个和第3个色子可能打出 "5点"

（3）计算打出 8 的 概率
     (1/3 * 0 ) + (1/3 * 0 ) + (1/3 * 1/8 )     = p3
	 //只有第 3个色子可能打出 "8点"

（4）计算：打出 2 5 8 的概率
	  p1* p2 * p3 = 2 5 8的概率

	  =0.0007314171810699588





//==================== 初始概率
//==================== 初始概率
要知识抓每个色子的概率：//就是每一个概率是 1/3 ，这个叫：初始概率

//==================== 转移概率
//==================== 转移概率

我抓了 "第一个色子" ，还抓 "第一个色子" 的概率是多少
我抓了 "第一个色子" ，再抓 "第二个色子" 的概率是多少

我抓了 "第二个色子" ，再抓 "第三个色子" 的概率是多少

在 "不同状态" 之间 "转移的概率"，可以使用转移矩阵来描述
		//=========
		一般描述成 转移矩阵如：
			1/3		1/3		1/3	
			/**
				第一次抓a下一次抓a 是1/3
				第一次抓a下一次抓b 是1/3
				第一次抓a下一次抓c 是1/3
			*/

			1/2		1/4		1/4	
			/**
				第一次抓b下一次抓a 是1/2
				第一次抓b下一次抓b 是1/4
				第一次抓b下一次抓c 是1/4
			*/

			1/6		1/2		1/3
			/**
				第一次抓c下一次抓a 是1/6
				第一次抓c下一次抓b 是1/4
				第一次抓c下一次抓c 是1/3
			*/
		//=========一般描述成
			P_a_a：上一次抓a下一次还是抓a
			P_a_b：上一次抓a下一次还是抓b
			P_a_c：上一次抓a下一次还是抓c
		//=========所有状态列出来就是转移矩阵
			P_a_a	P_a_b	P_a_c
			P_b_a	P_b_b	P_b_c
			P_c_a	P_c_b	P_c_c

//==================== 发射概率
//==================== 发射概率
	抓到 六面体，打出各个值的概率是  1/6
	抓到 四面体，打出各个值的概率是  1/4
	抓到 八面体，打出各个值的概率是  1/8

	这个概率就叫发射概率

//========================================= 重点
//========================================= 重点


知道前面 "这些概率" 之后，我们就可以通过  "这些概率"  算出 "输出序列（如2 5 8）" 的概率
前提是，你要知道 "初始概率、转移概率、发射概率" 是多少


如果 不知道 "初始概率、转移概率、发射概率" 是多少，
我们通过 "大量的观测数据" ，去把 "这些概率" 计算出来，这个过程就叫学习，
学习就是确定 "这个模型的参数" λ = （A,B,π）

	//A表示的是 "转移概率"
	//B表示的是 "发射概率"
	//π表示初始 概率

//========================================= HMM 三个基本问题
//========================================= HMM 三个基本问题

（1）概率计算问题：给定初始 λ = （A,B,π） 和 观测序列， 计算该模型 "观测序列" 出现的概率，
			  概率计算问题使用前向算法、后向算法

（2）学习问题：已知观测序列，估计模型参数 λ = （A,B,π），学习问题使用Baum-Welch算法

（3）解码问题：已知模型参数 λ = （A,B,π） 和 观测序列，求条件概率最大的状态序列，解码问题使用Viterbi算法
			   /**
				   根据看到的 2 5 8 ，
				   反过来推第一次抓的是哪个色子，
				   反过来推第二次抓的是哪个色子，
				   反过来推第三次抓的是哪个色子，
				   返回来推断的过程，这个就叫解码问题。

				   (解码问题)这个是HMM用的最多的，

				   通过看到的 2 5 8，反过来推你抓色子的这样一个序列，

				   如：看到 2 5 8 ，推出抓色子序列（假如）为：2  2 3，这个2 2 3 就是隐藏序列

				   通过 "可见序列" 推断 "隐藏序列" 这个过程叫 解码

				   这个带入 "贝叶斯公式"  里面通过一系列概率计算，是可以算出来的


				   //=========================== 应用场景
				   （1）假如输入的是 "语音"，把 "语音" 作为可观察序列，推断出后面的 "文字序列"，这就是 "语音识别"
								//在语音识别里面，如果使用HMM，则把 "语音序列" 作为可观测序列，"文字序列"作为隐藏序列，
								//它能通过 "语音序列" 推断出 "文字序列"
								//这就实现了 "语音识别"

				   （2）假如输入的是 "词语的序列"，输出的是 "词性序列"，这样就对 "词语的序列" 完成了词性标注任务
								//这样通过 "词语的序列" 推断出 "词性序列"

				   （2）假如输入的是 "一个句子"，输出的是 "位置信息（词的开头、结束）"，这样就可以使用HMM做分词。
			   */

//========================================= 
//=========================================

这个HMM的推导过程非常的复杂，涉及大量的概率计算在里面。

要带到 贝叶斯公式 里面去推。

现在 "隐马尔可夫模型HMM" 用的比较少，现在主要用 "循环神经网络"

因为"隐马尔可夫模型HMM"可以解决的问题， "循环神经网络"能解决的更好，更简单。


//========================================= 它能实现3个功能
//========================================= 它能实现3个功能


（1）它能计算 "可能观测序列" 的概率，叫概率计算

（2）它能通过大量的观测数据，去推断：初始概率、转移概率、发射概率，这叫学习过程

（3）解码问题，用的最多，通过  "可能观测序列"  推断 "不可观察的序列"


//===================== HMM的应用：
//===================== HMM的应用：
- 语音识别：输入语音序列（观测序列），输出文字序列（隐藏序列）， "1980~2000 处于统治地位"
- 分词：输入原始文本，输出分词序列
- 词性标记：输入词语列表，输出词性列表



//=========================================================== HMM 和 RNN 是不是有点像？
//=========================================================== HMM 和 RNN 是不是有点像？
确实有一些相似之处，尤其是在它们处理序列数据的方式上，但它们在底层原理和具体实现方面有着本质的不同。

相似之处
	（1）序列数据处理：HMM和RNN都是为处理序列数据（如时间序列数据、文本等）而设计的。
		 它们能够考虑到数据点之间的时间依赖性或顺序。

	（2）状态转移：两者都有 "状态" 概念，能够模拟从一个状态到另一个状态的转移。
		 在HMM中，状态转移是显式建模的，而在RNN中，状态转移是通过隐藏层的递归激活来隐式表示的。


不同之处
	（1）概率模型与神经网络：
			HMM是一种统计概率模型 它基于概率理论来模拟状态之间的转移以及状态和观测之间的关系。
			RNN是一种神经网络架构，通过训练权重来学习序列数据的模式，其学习能力不仅限于概率推断。

	（2）参数学习：
			HMM的学习过程通常涉及到估计状态转移概率、观测概率等统计参数，常用的方法有前向-后向算法等。
			RNN通过反向传播和梯度下降等方法来调整网络中的权重，以最小化预测误差。

	（3）处理长期依赖的能力：
			传统的RNN由于梯度消失或梯度爆炸问题，难以处理长期依赖问题。
			而虽然HMM能够模拟状态之间的转移，但其能力在捕捉长期依赖性方面通常也是有限的。
			长短时记忆网络（LSTM）和门控循环单元（GRU）是RNN的变种，专门设计来解决长期依赖问题。

	（4）灵活性和表达能力：
			RNN及其变体（如LSTM和GRU）通常具有更高的灵活性和表达能力，可以捕获更复杂的模式和依赖关系。
			HMM在某些特定任务上效果很好，尤其是当问题可以自然地用状态转移来建模时，但它的表达能力相对受限。


总的来说
		尽管HMM和RNN在处理序列数据方面有相似之处，但它们适用于不同类型的任务，
		并且在理论和实现上有着明显的区别。
		选择哪一种模型通常取决于具体任务的需求、数据的性质以及模型的复杂度。


