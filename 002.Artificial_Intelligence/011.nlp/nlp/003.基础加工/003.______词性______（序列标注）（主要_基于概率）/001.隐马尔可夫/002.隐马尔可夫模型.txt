


//===========================================================
//===========================================================

隐马尔可夫模型（Hidden Markov Model，HMM）

"时间序列"的"概率模型"，

它是：传统机器 学习中最复杂模型之一


//=========================================================== 隐马尔可夫模型
//=========================================================== 隐马尔可夫模型
它描述  "两个随机过程"
		（2）一个看不到，
			 （1）在抓 "后面的色子" 的时候，不知道抓 "哪一个"，每一个概率是 1/3
			 （2）抓到色子之后，打出几点也不确定
					（1）6面体：   1 2 3 4 5 6
					（2）4面休：   1 2 3 4 
					（3）8面体：   1 2 3 4 5 6 7 8	
			  //"隐藏序列": 抓到哪个色子，是看不到的称为隐藏序列。

		（1）一个看得到
			 （1）假如前面打出 2 5 8 ，这就是可以看到的序列
			  //"可观测序列"

//=========================================================== 隐马尔可夫模型 的 作用
//=========================================================== 隐马尔可夫模型 的 作用


根据 "可观测序列" 预测  "隐藏序列" //反推


//=========================================================== 计算过程
//=========================================================== 计算过程



//=========================================================== 前提：初始概率
//=========================================================== 前提：初始概率

（1）抓 "每个色子" 的概率是 1/3

（2）每个色子的值的概率
		6 面体各个值   1/6
		4 面体各个值   1/4
		8 面体各个值   1/8

//=========================================================== 转移概率
//=========================================================== 转移概率

抓了 6面体 之后，抓 4面休 的概率，叫 转移概率
抓了 6面体 之后，抓 8面休 的概率，叫 转移概率

//=========
一般描述成 转移矩阵......

P_a_a：上一次抓a下一次还是抓a
P_a_b：上一次抓a下一次还是抓b
P_a_c：上一次抓a下一次还是抓c
所有状态列出来就是转移矩阵



//=========================================================== 计算：打出 2 5 8 的概率（注意：抓了放回去）
//=========================================================== 计算：打出 2 5 8 的概率（注意：抓了放回去）

（1）计算打出 2 的 概率
     (1/3 * 1/6 ) + (1/3 * 1/4 ) + (1/3 * 1/8 )  = p1

（2）计算打出 5 的 概率
     (1/3 * 1/6 ) + (1/3 * 0 ) + (1/3 * 1/8 )   = p2

（3）计算打出 8 的 概率
     (1/3 * 0 ) + (1/3 * 0 ) + (1/3 * 1/8 )     = p3

（4）计算：打出 2 5 8 的概率
	  p1* p2 * p3 = 2 5 8的概率


//=========================================================== TODO
//=========================================================== TODO


		通过 "看得到的" 推 "测看不到的"

		//===================== 解码
		//===================== 解码
		通过 "可见序列" 推断 "隐藏序列" 这个过程叫 解码
		//===================== 解码 使用场景（早期使用很多）
		//===================== 解码 使用场景（早期使用很多）
		（1） 输入一个语音，把语音作为可观测序列，推断后面隐藏文字 ，这个就是语音识别。
		（2） 输入一个词语，输出的是词性，这就完成了记性的标注。

		//===================== HMM的应用：
		//===================== HMM的应用：
		- 语音识别：输入语音序列（观测序列），输出文字序列（隐藏序列）
		- 分词：输入原始文本，输出分词序列
		- 词性标记：输入词语列表，输出词性列表

		//=====================
		//=====================
		//现在用的少，现在主要用循环神经网络

//=========================================================== HMM 和RNN是不是有点像？
//=========================================================== HMM 和RNN是不是有点像？
确实有一些相似之处，尤其是在它们处理序列数据的方式上，但它们在底层原理和具体实现方面有着本质的不同。

相似之处
	（1）序列数据处理：HMM和RNN都是为处理序列数据（如时间序列数据、文本等）而设计的。它们能够考虑到数据点之间的时间依赖性或顺序。
	（2）状态转移：两者都有“状态”概念，能够模拟从一个状态到另一个状态的转移。在HMM中，状态转移是显式建模的，而在RNN中，状态转移是通过隐藏层的递归激活来隐式表示的。


不同之处
	（1）概率模型与神经网络：HMM是一种统计概率模型，它基于概率理论来模拟状态之间的转移以及状态和观测之间的关系。RNN是一种神经网络架构，通过训练权重来学习序列数据的模式，其学习能力不仅限于概率推断。
	（2）参数学习：HMM的学习过程通常涉及到估计状态转移概率、观测概率等统计参数，常用的方法有前向-后向算法等。RNN通过反向传播和梯度下降等方法来调整网络中的权重，以最小化预测误差。
	（3）处理长期依赖的能力：传统的RNN由于梯度消失或梯度爆炸问题，难以处理长期依赖问题。而虽然HMM能够模拟状态之间的转移，但其能力在捕捉长期依赖性方面通常也是有限的。长短时记忆网络（LSTM）和门控循环单元（GRU）是RNN的变种，专门设计来解决长期依赖问题。
	（4）灵活性和表达能力：RNN及其变体（如LSTM和GRU）通常具有更高的灵活性和表达能力，可以捕获更复杂的模式和依赖关系。HMM在某些特定任务上效果很好，尤其是当问题可以自然地用状态转移来建模时，但它的表达能力相对受限。


总的来说，尽管HMM和RNN在处理序列数据方面有相似之处，但它们适用于不同类型的任务，并且在理论和实现上有着明显的区别。选择哪一种模型通常取决于具体任务的需求、数据的性质以及模型的复杂度。


