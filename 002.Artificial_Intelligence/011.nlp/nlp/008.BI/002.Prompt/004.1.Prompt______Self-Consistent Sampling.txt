
//================================================== 
//================================================== 

我们在使用 "语言模型（LLM）" 时，经常会遇到 "两个关键参数"：
			（1）温度（Temperature）
			（2）Top_K。
这两个参数对 "模型输出的结果" 有着重要的影响。


//============== 温度（Temperature）
//============== 温度（Temperature）
温度是一个介于 "0 和 1" 之间的数值，这个参数用于控制 "模型输出的多样性"。
	当温度设置为 0 时，模型会输出最有可能的答案；
	当温度设置为 1 时，模型会输出更多样化，但可能不那么精确的答案。

/**
举例：
	温度为 0：	LLM 可能会输出 "苹果是一种水果。"
	温度为 1：	LLM 可能会输出 "苹果是一种多汁、美味的水果，常用于制作各种美食。"
*/

//============== Top_K
//============== Top_K

Top_K 是一个整数，用于 "限制模型" 在 "生成每个单词时" 考虑的 "候选单词数量"。
例如，如果 Top_K 设置为 50 ，模型在生成 "下一个单词" 时，只会考虑 "概率最高的前 50 个选项"。

举例：
	Top_K 为 10  ：模型输出可能更加一致和准确。
	Top_K 为 100 ：模型输出可能更加多样，但准确性可能会下降。

//================================================== Self-Consistent Sampling 自洽采样
//================================================== Self-Consistent Sampling 自洽采样

很自然的想法 "对于同一个回答"，LLM会输出 "不一样的答案"，
特别是针对一些 "数学题" 或 "逻辑问答" 题目，有一个标准答案，

当LLM的temperature设置大于0时，我们可以 "通过对同一个问题询问好几次" ，
然后统计 LLM 回复的答案，通过 "服从多数的原则" 来输出答案的内容。
这种方法称为 "自洽采样（Self-Consistent Sampling）"，它有助于识别模型认为 "最可能正确的答案"。


