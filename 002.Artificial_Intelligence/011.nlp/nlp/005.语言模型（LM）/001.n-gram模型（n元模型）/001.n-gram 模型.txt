
离得越远的词，关系越弱，

所以在计算时就忽略一定长度的词语，
太远就不考虑它的影响性，这就是n-gram模型

	是对前面 "句子的概率" 计算的简化

//===========================



N-gram 是一种文本特征表示方式，同时也是语言模型


//=========================== N-gram模型
//=========================== N-gram模型


一个句子中，距离越远的词关系越弱，越近的时，才是关系越强的
			//所以只考虑一定范围内容的词
			//N-gram模型就是这样一个模型

当 N=1 时，就好像每个词是不相互影响的。
当 N=2 时，就只考虑前一个词，这个就叫二元模型。
当 N=3 时，称为三元模型。

//===========================

N=1时， P(W1) * P(W2) * P(W3) * P(W4)

N=2时， P(W1) * P(W2|W1) * P(W3|W2) * P(W4|W3)   //只考虑前1个词的影响

N=3时， P(W1) * P(W2|W1) * P(W3|W1W2) * P(W4|W2W3) //只考虑前2个词的影响



n越大，语义信息越多，计算量越大，
n越小，语义信息越少，计算量越小，

当n为1时，识为每个词没有关联


//===========================
//===========================

/**

前面说过：语言模型是概率相乘
		  整个句子的概率就是：概率相乘
		  P(W1) * P(W2|W1) * P(W3|W2W1) * ...... * P(W_n)
N-gram 模型实际上是对 "语言模型的简化"，

*/


当 N=1 时，只考虑所有词的 "独立概率"，每个词没有关联
当 N=2 时，预测 "下一个词" 的时候，考虑 "上一个词"
当 N=2 时，预测 "下一个词" 的时候，考虑 "上两个词"

N越长，可见，N值越大，保留的词序信息（上下文）越丰富，但计算量也呈指数级增长。
N越短，计算量就越小。

