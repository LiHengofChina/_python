

//=================================================== 神经网络模型 NNLM
//=================================================== 神经网络模型 NNLM 

它还是一个 "N元结构" ，//但它是用神经网络来实现的

但它是利用 "神经网络"，根据  "前面几个词" ，对 "下一个词做预测"
		//也是 "概率估计"

//======================= 比如：
//======================= 比如：

输入前 3 个词，预测出第 4 个词
输入第 2 3 4 个词，预测出第 5 个词
输入第 3 4 5 个词，预测出第 6 个词


//======================= 重点
//======================= 重点

它还是 "根据前面几个词" 使用 "神经网络" 预测出下一个词，

它是一个 "全连接网络" 


自然语言处理里面 "很少有卷积出现"。
			//TextCNN是个例外。


//======================= 示例：
//======================= 示例：

呼伦贝 __ __ __

设词典大小为 1000 ，向量维度为25，N=3，先将前N个词表示成独热向量： 
//1000词典里面有1000个词

呼：[1,0,0,0,0,...,0]
伦：[0,1,0,0,0,...,0]
贝：[0,0,1,0,0,...,0]

//========================== 第一层
输入：[3,1000]
权重：[1000,25]  //25：表示希望  "每一个词输出25维的词嵌入"

矩阵相乘：[3,1000] * [1000,25] = [3,25]


输出：经过第一层会输出一组权重矩阵，这个 矩阵就是词嵌入表示
			//所以说 "词嵌入" 是NNLM的中间 产品，

//========================== 第二层
//第二层的输入是："第一层的输出词嵌入"


输入：[3,25]
权重：[25,1000]
矩阵相乘：[3,25] * [25,1000] = [3,1000]

拍扁[1,1000] //在列的方向上求和或求均值
再使用softmax得到1000个概率，
看这1000个概率哪一个最高，预测出来就是哪一个值


//=================================================== 总结：
//=================================================== 总结：


输入的是：前面几个词的 "独热表示"
输出的是：下一个词的概率。

		// "词嵌入" 是它的可以把它视为 "中间层输出"。 

