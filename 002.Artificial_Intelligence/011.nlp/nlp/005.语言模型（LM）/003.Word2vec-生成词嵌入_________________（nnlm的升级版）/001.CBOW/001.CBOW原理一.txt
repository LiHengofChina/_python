
//========================================================================= 基本版本
//========================================================================= 基本版本


假设：呼伦贝尔大草原，这是一个语料库。
		/**

			只有一句话，实际情况是 "很大的语料库"
			把语料库里面，所有文章连成一个非常大的字符串

		*/

轮流以 "每个词" 作为 "中心词"
		/**

		呼 _ _ 尔大草原		//P(伦|呼) P(贝|呼) 
		_ 伦 _ _ 大草原		//P(呼|伦) P(贝|伦) P(尔|伦) 
		_ _ 贝 _ _ 草原		//P(呼|贝) P(伦|贝) P(尔|贝) P(大|贝)
		呼 _ _ 尔 _ _原		//P(伦|尔) P(贝|尔) P(大|尔) P(草|尔)
		呼伦_ _ 大 _ _		//P(贝|大) P(尔|大) P(草|大) P(原|大)
		呼伦贝 _ _ 草 _		//P(尔|草) P(大|草) P(原|草) 
		呼伦贝尔 _ _ 原		//P(大|原) P(草|原) 

		*/

把所有概率连到一起，写成一个 "条件概率" 联合概率公式：
		/**

			P(伦|呼)*P(贝|呼)*P(呼|伦)*P(贝|伦)*P(尔|伦)*P(呼|贝)*P(伦|贝)*P(尔|贝)*P(大|贝)*P(伦|尔)*P(贝|尔)*P(大|尔)*P(草|尔)*P(贝|大)*P(尔|大)*P(草|大)*P(原|大)*P(尔|草)*P(大|草)*P(原|草)*P(大|原)*P(草|原)


		*/

因为这所有 "搭配和组件" 在 "语料库" 里面是出现过的，所以我希望，把这个概率 "优化的越大越好"。
		/**
			之前说过，可以向两个方向优化：

			（1）表示成损失和误差：越小越好
			（2）表示成概率可能性：越大越好
		*/



这个概率一般可以写成：
		Text = w1,w2,w3,...,wn		//设置有文本Text，由n个单词组成



目标函数可以写作：

			  ∏          ∏ 
	argmax 						P(c|w;θ)
			w∈Text    c∈c(w)


	/**

	表示：给定w为中心词的，可以预测出上下文c

	∏ 
		   //连成符号，求的是连乘的概率

	∈ 
			// 表示元素符号 某个集合
	w∈Text 
			//表示w属于 Text集合

	  ∏
	c∈c(w)
		    //对于集合“C(w)”中所有的“c”，进行某种连乘操作。

	  ∏
	w∈Text
		//表示对于文本 Text 中的每一个元素 w 进行连乘操作

	argmax
		//表示概率最大化

	c(w)
		//表示w作为中心词

	θ
		//表示模型参数

	c
		//c为w的上下文词

	*/

给定w为中心词的，可以预测出上下文c，这个概率要最高，


影响这个概率的变量是什么？
		w ，只要语料库确定了，这个w中心词就确定了，c也就确定了
		所以  c(w) 的c 和w来自语料库里面固定的值。

那么，调整哪个值，才能让这个概率最高呢？
		这就需要 "模型参数θ" 去学习了 ，
		就是神经网络要去调整的矩阵

//====== θ矩阵
//====== θ矩阵

w为当前词，c为w的下下文词，θ为要调整的参数，
这个参数即每个词（或字）的 "稠密向量" 表示，如：
		/**

		[呼: θ11  θ12  θ13   ...   θ1n  ]
		[伦: θ21  θ22  θ23   ...   θ2n  ]
		[贝: θ31  θ32  θ33   ...   θ3n  ]
		[尔: θ41  θ42  θ43   ...   θ4n  ]
		[大: θ51  θ52  θ53   ...   θ5n  ]
		[草: θ61  θ62  θ63   ...   θ6n  ]
		[原: θ71  θ72  θ73   ...   θ6n  ]

		*/


[呼: θ11  θ12  θ13   ...   θ1n  ] 表示： "呼" 作为 中心词，训练出一个向量
							//它的具体值是多少，我们可以忽略掉，统一看成θ

依次类推
		[伦:
		[贝:
		[尔:
		[大:
		[草:
		[原:


反正我希望能寻找到 "这样一组向量 "：
			/**

				我们把 "这样一组向量" 带上面的 "目标函数" 里面，
					能把概率最大化，
				我有中心词的情况下,能预测出中心词的上下文，然后将联合概率相乘

			*/

			/**
				之前用梯度下降法的时候，是： 把损失值和误差优化到最小

				现在，我们要求把概率优化到最大，因为它是在真实语料库里面出现过的

			*/

			/**
				所以，我们得到了一个 "概率最大化" 的模型。
				我们要调整的就 θ 值
				我们把 θ 矩阵里面的每一个值，调整成一个合适的值，
				带到上面的公式里面来，能得到一个最大化的概率
			*/

当 θ 调整好以后，这个词向量就确定了。


//====== 转成 log 相加的 公式
//====== 转成 log 相加的 公式
 


∏  表示连乘，它们的概率本来很小，连乘下来说法更小了，计算机，可能就表示不出来了。
			// 1*10^(-6)  就太小了

所以需要把  "∏连乘公式"  转换成 "对数相加" 的公式

			/**
				对每一个概率取一个对数，然后相加


					  Σ          Σ 
			argmax 						log(P(c|w;θ))
					w∈Text    c∈c(w)


			*/	

这种变化，经常会用到



为什么要 log(P) 呢？
			/**

				因为P很小嘛，
				取一个log就把它转换成一个大的负数，负数前面加一个 "负数"，
				就会变成 一个比较 大的正数

			*/

//====== 加入softmax
//====== 加入softmax


			/**

						  Σ          Σ 							  /   Σ
				argmax 						log ( (e^(u_c*u_w )) /    	   		(e^(u_c*u_w ))	)
						w∈Text    c∈c(w)						/   c'∈vocab



				(u_c*u_w ) 表示两个向量的 点乘
							表示 w 作为中心词，我去和上下文c 相似性
				"向量的点乘" 向量 "相似度"，
				相似度越高，出现的 "可能性越大" ，把它放在 "分子的部分"

				除以 当前  "这个中心词" 跟 "其它所有上下文词之和"，

				局部除以整体

			*/
//====== 再次转换
//====== 再次转换


		分数的对数，可以转换成：
			/**
				"分子的对数" - "分母的对数"
				log(b/a)  可以写成 log(b) -log(a)
			*/

		上面的公式 变成 ：
			/**
						  Σ          Σ 							       Σ
				argmax 						( log (e^(u_c*u_w )) - log(      	  (e^(u_c*u_w ))) )
						w∈Text    c∈c(w)						       c'∈vocab
			*/

		再次变形：
			/**
						  Σ          Σ 							       Σ
				argmax 						( u_c*u_w 			 - log(      	  (e^(u_c*u_w ))) )
						w∈Text    c∈c(w)						       c'∈vocab
			*/

		/**

		u_c*u_w  //两个向量相乘计算量比较小



			Σ
		log(      	  (e^(u_c*u_w )))	
			c'∈vocab
		而这部分计算量非常大，表示要去遍历整个词汇表，

		在英语里面，至少到循环几十万到100成次
		再做点乘，再做乘方，再做对数，都是非常慢的

		*/

		所以这个公式，训练越来效率非常低