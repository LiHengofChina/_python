
//========================================================================= 原理分析二 改进版：负采样
//========================================================================= 原理分析二 改进版：负采样

性能上的改进和优化版
 
于是出现了负采样格式，

先设置窗口大小：
	（1）如果两个词在 "同一个窗口" 出现过，把它放在正样本里面。
	（2）反之，没有在 "同一个窗口" 出现过的，就看成是 "负样本"。



正样本：D = { (呼, 伦),
			  (伦, 呼),
			  (伦, 贝),
			  (贝, 伦),
			  (贝, 尔),
			  (尔, 贝),
			  (尔, 大),
			  (大, 尔),
			  (大, 草),
			  (草, 大),
			  (草, 原),
			  (原, 草)
			}


负样本：D’= {(呼，贝),(呼，尔),(呼，大)，(呼，草)，(呼，原)，(伦，尔),
			(伦，大),(伦，草),(伦，原),(贝，呼),(贝，大),(贝，草),(贝，原),(尔，呼),(尔，伦)(尔，草),
			(尔，原),(大，呼),(大，伦),(大，原)，(草，呼)，(草，伦)，(草，贝)，(原，呼)，(原，伦)，(原，贝)，
			(原，尔)，(原，大)}

		  /**
		  	正样本 比负样本 小很多，
		  	就像在北京，有20000万人，但是你真正碰到过的人很多


		  */
因为在正样本出现过，所以期望全部预测成1

负样本 期望全部预测成0 ，负样本，由于 量很大，每次只拿一部分出来。
		//但是由于 负样本里面样本太多，每次取一小部分进行采样，这样大小 就变得很小了



这样就可以写成下面的公式：
			  ∏          				∏ 
	argmax (		logP(D=1|w,c;θ)				P(D=0|w,c;θ)      )
			w,c∈D   				  w,c∈D'
	/**
		从正样本取一些出来
				w作用中心词，c作为上下文，θ作为参数的情况下，全部预测成1
		从负样本取一些出来
				w作用中心词，c作为上下文，θ作为参数的情况下，全部预测成0
		
		所有概率还是连乘，将这个概率最大化

	*/

但是 D=0 ，预测成0是有问题的

1-0 ，就是越大越好，0 是越小越好

//====== 加入 softmax：
//====== 加入 softmax：
 

所以变成成softmax格式：

......

//====== 最后 变成后：
//====== 最后 变成后：

			  Σ          			    Σ 
	argmax (		logσ(U_c*V_w)	+ 		  logσ(-U_c*V_w)      )
			w,c∈D   				  w,c∈D'


/**

  Σ          			
		logσ(U_c*V_w)		
w,c∈D   				
	//表示成正样本遍历所有的，正样本小，所以计算量还行


  Σ 
		  logσ(-U_c*V_w)
w,c∈D'
	//负样本取部分，负样本比较大，但是只取部分，所以计算量也还行

*/


