

"神经网络模型 NNLM" 是一个比较原始的神经网络语言模型
 
Goolge 在 "它的基础" 上面发布了Word2vec 
Word2vec 库：专门用于词嵌入训练库



//Word2vec翻译就是：词到向量

//=========================== Word2vec 库
//=========================== Word2vec 库

Word2vec 包含两个  "改进版本的 '神经网络模型 NNLM'"

（1）CBOW 
		呼伦贝__大草原

		输入的是 "中心词的向量"，输出的是 "N个词的概率"。
		//输入：中心词的向量
		//输出：N个词的概率
		//以 每一个词作为中心词，根据 目标函数训练一组词向量
		//===================== CBOW 两种原理版本
		//===================== CBOW 两种原理版本
		（1）基本版本：这个版本使用了所谓的“连乘法”，
			 即轮流以每个词作为中心词，通过穷举法来计算上下文词的条件概率，然后取平均值。
			 这个方法的缺点是计算量大。

		（2）优化版本：为了改进性能并减少计算量，通常采用了负采样（Negative Sampling）的技术。
			 在这种改进下，如果两个词在同一个窗口（context window）中出现过，
			 将它们放入正样本中，否则将其视为负样本。
			 每次训练时，正样本会全部取出，而负样本只取一部分，从而减少了计算量。


（2）Skip-gram //Skip-gram 效果更好
		_ _ _ 尔 _ _ _


/**
	其实就是做完形填空
	CBOW 更好完成，
	但是它的效果就没有  Skip-gram 

*/

//================================================================
//================================================================
model = Word2Vec(LineSentence(in_file), # 输入
                 size=100, # 词向量维度(推荐25~300之间)
                 window=3, # 窗口大小
                 min_count=5, # 如果语料中单词出现次数小于5，忽略该词
                 workers=multiprocessing.cpu_count()) # 线程数量

//=========== 
//=========== 
 sg=0 时（默认值），使用CBOW算法。
 sg=1 时，使用Skip-gram算法。


