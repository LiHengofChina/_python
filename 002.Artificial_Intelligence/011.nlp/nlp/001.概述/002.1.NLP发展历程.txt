
//===================================================== NPL发展历程
//===================================================== NPL发展历程


第一阶段：基于规则
第二阶段：基于统计
第三阶段：基于深度学习



//================== 熵
//================== 熵
		/**
		1948年：
		Shannon把 "离散马尔可夫过程" 的概率模型应用于 "描述语言的自动机"。
		接着，他又把 "热力学中“熵”(entropy)"的概念引用于 "语言处理" 的 "概率算法" 中


		*/

//================== 时间段
//================== 时间段

~1956之前：
	1946年：第一台计算机
	1948年： Shannon 把 "离散马尔可夫过程 的 概率模型" 应用于描述语言的 "自动机"。
			 接着，他又把热力学中  "熵(entropy)" 的概念引用于  "语言处理" 的概率算法中。


1957~1970：
			（1）基于规则：
					语言学家建立模型，设置语法规则，行不通。基于概率，
					//语法规则：

			（2）基于概率：
					//粗爆：从现在的大量的语料库中提取概率
					//如：这两个词搭配的概率是多少
					//	  名词后面跟动词的概率是多少
					//	  名词后面跟形容词的概率是多少
					//采用的是统计学思路
			//=========================
			不管是传统的机器学习，还是深度机器 学习，都是 "基于概率" 的模型更加优秀
			//都是基于概率的技术路线
			//=========================此时分为两个学派
			基于规则：符号学派	//听上去简单，但是实际上不可行
			基于概率：随机派	//朴素贝叶斯




1971~1993：
			隐马尔可夫模型（Hidden Markov Model，HMM），用得少

1994~至今：
			2001 年： 神经网络模型
			2008 年： 多任务学习
			2013 年： 词嵌入

			2013 年： NLP的神经网络
			2014 年： 序列到序列模型//主要用于翻译
			2015 年： 注意力机制//chatgpt的基础
			2015 年： 基于记忆的神经网络
			2018 年： 预训练语言模型

//===================================================== 事实证明，效果越好
//===================================================== 事实证明，效果越好





//===================================================== 困难与挑战
//===================================================== 困难与挑战

（1）歧义
（2）不同语言结构
（3）不可预知


