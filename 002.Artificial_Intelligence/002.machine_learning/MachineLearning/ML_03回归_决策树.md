# 决策树回归



```javascript
//========================机器学习分类：
（1）基于 "模型的学习"
		 根据 "数据的分布状态"，找到一个数学公式去表达它。
         如：线性回归、岭回归、Lasso回归、多项式回归。
（2）基于 "实例的学习"
		 "相似的输入" 必定产生 "相似的输出"。
         如：决策树。
         

//======================== 决策树
决策树————最初不是解决 "回归问题的"，它是解决 "分类问题的"，
		不过后来发现， 它也能解决回归问题。

//======================== 决策树（定义）

决策树是一种树形结构的监督学习模型，
通过特征条件不断分裂数据，
最终用于分类或回归任务。

//======================== 决策树（重要）
决策树不是用“梯度下降”训练的
它是基于“信息增益 / 基尼指数”做贪心划分
一次构建完成

//======================== 决策树（重要）
决策树：
    不需要“学习率、轮数”这种梯度下降训练方式。
它不是靠反复迭代更新参数的。

但它 仍然需要训练。
    根据训练数据
    选择最优特征
    选择最优切分点
    递归生成树结构
所以它依然需要训练：模型训练

只是它的训练方式是“贪心划分”，不是“参数优化”。

```



### 决策树

#### 基本算法原理

```java

核心思想：“相似的输入”必会产生“相似的输出”。

例如预测某人薪资：

```



年龄：1-青年，2-中年，3-老年
学历：1-本科，2-硕士，3-博士
经历：1-出道，2-一般，3-老手，4-骨灰
性别：1-男性，2-女性

| 年龄 | 学历 | 经历 | 性别 | ==>  | 薪资        |
| ---- | ---- | ---- | ---- | ---- | ----------- |
| 1    | 1    | 1    | 1    | ==>  | 6000（低）  |
| 2    | 1    | 3    | 1    | ==>  | 10000（中） |
| 3    | 3    | 4    | 1    | ==>  | 50000（高） |
| ...  | ...  | ...  | ...  | ==>  | ...         |
| 1    | 3    | 2    | 2    | ==>  | ?           |



```javascript

（1）我找到 "100个和我特征相同的" 样本（平均值）
（2）样本数据有100W条，有行有列（更换数据结构：树）
		//根节点、子节点、叶节点：等等概念。
		//（1）二叉树：每个节点，只分出 "两个子节点"。
		//（2）完全二叉树：每个节点，一定会有两个子节点。

//======================================================= sklearn 与 "树形结构"
//======================================================= sklearn 与 "树形结构"


sklearn 的决策树实现为二叉树结构。


sklearn 中的 决策树底层是二叉树结构
每个节点只分成两个分支（左 / 右）。


//=======================================================
//=======================================================

为了提高搜索效率，使用树形数据结构处理样本数据：

```


$$
年龄=1\left\{
\begin{aligned}
学历1 \\
学历2 \\
学历3 \\
\end{aligned}
\right.
\quad\quad
年龄=2\left\{
\begin{aligned}
学历1 \\
学历2 \\
学历3 \\
\end{aligned}
\right.
\quad\quad
年龄=3\left\{
\begin{aligned}
学历1 \\
学历2 \\
学历3 \\
\end{aligned}
\right.
$$


```javascript

//======================================================= 决策树的基本决策过程
//======================================================= 决策树的基本决策过程

（1）首先从  "训练样本矩阵"  中选择一个 "特征" 进行子表划分，使每个子表中 "该特征的值全部相同"。
			// "相同特征的样本" 放到 "同一个节点下"
（2）然后再在 "每个子表" 中选择 "下一个特征" 按照 "同样的规则" 继续划分 "更小的子表"，

（3）"不断重复" 直到 "所有的特征" 全部使用完为止， 此时便得到 "叶级子表" ，其中 "所有样本的特征" 值全部相同。


对于 "待预测样本" ，根据其 "每一个特征的值"，选择 "对应的子表"，逐一匹配，直到找到与之完 "全匹配的叶级子表"，
用该子表中 "样本的输出"，通过：
				（1）"平均(回归)" 
				（2）或者 "投票(分类)为待预测样本" 提供输出。

```





**首先选择 "哪一个特征" 进行子表划分决定了 "决策树的性能"。这么多特征，使用哪个特征先进行子表划分？**



```javascript
//====================================核心
"优先划分" 特征属性比较强的 "特征"，
		//=================================什么叫 "特征属性" 
		（1）基尼不存度
		（2）信息熵
		（3）信息增益
		（4）增益率
		（5）MSE
		（6）......
		
        
//================================= 回归问题  与 分类问题 =====对于 决策树
        回归问题  与 分类问题 都会用到 决策树 ，但是它们的 "优先划分" 的特征是不一样的。



//====================================================================（一）  "划分决策树" 的目的： 
//====================================================================（一）  "相同特征的样本" 放到 "同一个节点下"
 		"先划分谁" 能够 "最快的" 把 "同一类样本" 划分到 "同一个节点下"，谁的特征属性就比较强。

//================================= 问题1： 如何 "选取最优特征"？
        
//================================= 示例： 快速的在 "所有的类别" 中找到 "鸟类对象"
         鸡  鸭  鹅  猫  狗  猪  人  鸟
        （1）是不是两条腿 ————  5： 3
        （2）是不是卵生   ————  4: 4
        （3）有没有羽毛   ————  4: 4
        （4）有没有眼睛   ————  8: 0
        （5）会不会飞     ————  7: 1

所以：
	会不会飞：  最强
    有没有眼睛：最弱
		
//================================= sklearn 里面 "特征属性强弱" 的划分方式：
通过谁划分节点，预测的比较准，谁的 "特征属性就强"
    准不准 就使用 y - y ' 来评估， 也就是均差误差（MSE）： ∑(Y -Y')^2 / n 来评估


//================================= 决策树 两大问题
//================================= 决策树 两大问题

问题1： 如何 "选取最优特征"？
问题2： 何时 "停止分裂"？

    
    
```

sklearn提供的决策树  "底层" 为 "cart树（Classification and Regression Tree）（中文名 "回归" 与  "分类" 树）"，

​			

```javascript
除了 "cart树" ，还有 "ID3"、"C4.5"等等。
CART
    二叉树（每次只分两支）
    分类用 基尼指数
    回归用 平方误差
    可做分类 + 回归
    ✅ sklearn 使用的是 CART

ID3
    多叉树
    用 信息增益（熵）
    只能做分类
    不支持连续值（原始版本）

C4.5
    ID3 改进版
    用 信息增益率
    支持连续特征
    可剪枝

```



```javascript
//========================================= "cart回归树" 在解决 "回归问题" 时的步骤如下：
//========================================= "cart回归树" 在解决 "回归问题" 时的步骤如下：

1. 原始数据集S，此时树的深度depth=0； 
		//此时没有划分所以深度是0 ，在 "根节点" 存放 "全部的数据"

2. 针对集合S，遍历 "每一个特征"的"每一个value" (遍历数据中的所有离散值 (12个)   )

   用 "该value" 将原数据集S分裂成 "2个集合" ：   左集合left(<=value的样本)、
   										右集合right(>value的样本)，
   					//两个集合：就是前面说的 "二叉树"，两个子节点。

   分别计算 "这2个集合的mse(均方误差)"，
   找到使（left_mse + right_mse）最小的那个value，
   记录下 "此时的特征名称" 和 "value"，
   这个就是 "最佳分割特征" 以及 "最佳分割值"；

3. 找到 "最佳分割特征" 以及 "最佳分割value" 之后，
	用该value将集合S分裂成2个集合，depth+= 1；

4. 针对集合left、right分别重复步骤2,3，直到达到 "终止条件" 。
		//=============================================================（重点）
		//=============================================================（重点）
		//注意： "已经划分过" 的特征，但是它们的 "划分范围" 不一样，还可以再划分
	    //如："工作经验" 划分为  <= 2 和 >2的，
        //再次划分经验： 可以对<=2 里面的再划分 ： <=1 和 >2的
        //能不能划分 ，要根据圴方误差值，来决定。

		//=============================所以：
		假设一组数据有4个特征，那么它划分的树，可能不止有4层

       

```





决策树底层结构 为二叉树

```javascript

//========================================= 什么是终止条件：
//========================================= 什么是终止条件：
终止条件有如下几种：
1、"特征已经" 用完了：没有可供使用的特征再进行分裂了，则树停止分裂；
2、子节点中 "没有样本" 了：此时 "该结点" 已经没有样本可供划分，该结点 "停止分裂"；
			//某个节点停止分裂，其它有样本的节点不停止分割。
			//这就是：预剪枝。
3、树达到了人为预先设定的 "最大深度"：depth >= max_depth，树停止分裂。
			// 如果不设置就会出现：穿过所有样本，过拟合。
			// 这就是：预剪枝。 max_depth，指最大模型深度。它也是一个 "超参数"
			// *** 注意：这里指 "深度"。


4、节点的 "样本数量" 达到了 "人为设定的阈值"：样本数量 < min_samples_split ，则该节点停止分裂；
			// 样本数量
			// *** 注意：这里指 "样本数量"。
			// 这就也是：预剪枝。

5、最小样本需要数
			// min_samples_leaf



//================== 第三、四点一般设置一个。
//================== 第三、四点一般设置一个。
但也可以设置两个。



//=========================================  决策树 优化：
//=========================================  决策树 优化：
（1）预剪枝： max_depth 、min_samples_split
		在建树过程中提前停止分裂。
        （）最大深度（max_depth）
        （）最小样本数（min_samples_split）
        （）最小叶子样本数（min_samples_leaf）
        （）👉 防止树长太深，避免过拟合。
（2）后剪枝：
        先把树完整长出来，
        再删除一些“贡献不大的分支”。
        （）比较剪枝前后模型效果
        （）如果剪掉后误差更小或差不多 → 就剪


```





决策树回归器模型相关API：

```python
import sklearn.tree as st

# 创建决策树回归器模型  决策树的最大深度为4
model = st.DecisionTreeRegressor(max_depth=4)
# 训练模型  
# train_x： 二维数组样本数据
# train_y： 训练集中对应每行样本的结果
model.fit(train_x, train_y)
# 测试模型
pred_test_y = model.predict(test_x)
```



案例：预测波士顿地区房屋价格。

1. 读取数据，打断原始数据集。 划分训练集和测试集。

```python
import sklearn.datasets as sd
import sklearn.utils as su
# 加载波士顿地区房价数据集
boston = sd.load_boston()
print(boston.feature_names)
# |CRIM|ZN|INDUS|CHAS|NOX|RM|AGE|DIS|RAD|TAX|PTRATIO|B|LSTAT|
# 犯罪率|住宅用地比例|商业用地比例|是否靠河|空气质量|房间数|年限|距中心区距离|路网密度|房产税|师生比|黑人比例|低地位人口比例|
# 打乱原始数据集的输入和输出
x, y = su.shuffle(boston.data, boston.target, random_state=7)
# 划分训练集和测试集
train_size = int(len(x) * 0.8)
train_x, test_x, train_y, test_y = \
    x[:train_size], x[train_size:], \
    y[:train_size], y[train_size:]
```

1. 创建决策树回归器模型，使用训练集训练模型。使用测试集测试模型。

```python
import sklearn.tree as st
import sklearn.metrics as sm

# 创建决策树回归模型
model = st.DecisionTreeRegressor(max_depth=4)
# 训练模型
model.fit(train_x, train_y)
# 测试模型
pred_test_y = model.predict(test_x)
print(sm.r2_score(test_y, pred_test_y))


```



```javascript
//===========================  决策树 可视化
//===========================  决策树 可视化

sklearn 里面的 "决策树" ,还封装了 matplotlib的。
		决策树 怎么划分的，用图表可以画出来。
        //但是：它只封装了怎么 "画图" ，但是没有封装 "显示"
        //所以还是需要导入 matplotlib.pyplot as plt


//=========================== 决策树 名字由来
//=========================== 决策树 名字由来
（1）树：用树形结构来保存数据
（2）决策：把 "需要预测的数据" 带到  "样本" 中进行匹配 的过程，就叫决策。


//======================================================== 决策树每次也会保证 "泛化能力"
//======================================================== 决策树每次也会保证 "泛化能力"
决策树每次也会保证 泛化能力，所以结果会不一样
它具有一定的随机性：

model = st.DecisionTreeRegressor(max_depth=4,
                                 random_state = 7  # #模型的随机种子
                                 );
注意区别：训练集 和 测试集 里面的随机种子
（1）一个是样本的
（2）一个是模型的

```



#### 集合算法​

```javascript
 "单颗决策树" 效果不是很好， 可以利用  "两颗"决策树 、 "三颗"决策树 、 一片森林。。。

 三个臭裨将(将军)，顶个朱葛亮。
 
 
//============================================ 集合学习
//============================================ 集合学习
单个模型得到的预测结果总是片面的，根据多个不同模型给出的预测结果，

利用 "平均(回归)" 或者 "投票(分类)" 的方法，得出最终预测结果。



//============================================ “决策树”-集合学习
//============================================ “决策树”-集合学习

基于 “决策树的集合算法”，就是按照 "某种规则"，构建多棵 "彼此不同" 的 "决策树模型"，
"分别给出" 针对 "未知样本" 的预测结果，
最后通过 "平均" 或 "投票" 得到 "相对综合" 的结论。
常用的 "集合模型" 包括:
			(1)"Boosting类模型（AdaBoost、GBDT、XGBoost）" 
										//指（提升），多颗树，每颗树和每颗树之间是 “有一定的联系” 的。
								        //提升 是指"每棵树" 是有联系的,
                    /**
                        AdaBoost-自适应提升算法:通过不断调整样本权重，让后续模型更关注错误样本。
                        			// AdaBoost 意思是：正向激励、自适应、增强

                        GBDT-梯度提升决策树:通过梯度下降思想，逐步叠加多棵弱树优化误差。
                        		// GBDT     意思是：梯度提升
                        XGBoost-极端梯度提升（优化版梯度提升）：在 GBDT 基础上加入正则化与工程优化，速度更快、效果更好。
                    */
			(2)"Bagging（自助聚合、随机森林-RandomForest）类模型"。
            					        // 指（打包）：多颗树，每颗树和每颗树之间是 “没有联系” 的
                    /**
                        Bootstrap Aggregating-自助聚合：通过有放回抽样训练多个模型，再综合结果降低方差。
                        RandomForest-随机森林：基于 Bagging 思想，构建多棵随机特征选择的决策树进行集成。
                                // 随机森林
                    */

```









##### 集合算法（一）、AdaBoost-自适应提升算法（正向激励）

```javascript

//========================================================== 具体步骤
//========================================================== 具体步骤
（1）首先为 "样本矩阵" 中的样本分配 "初始权重"，由此构建一棵 "带有权重的决策树"，
   再由 "该决策树" 提供 "预测输出" 时，通过 "加权平均" 或者 "加权投票" 的方式产生预测值。
   
已经构建好一个决策树  通过1322 找到所有的女博士  一个4个  6000  8000 9000 10000
由于正向激励，对每个样本都分配了"初始权重" 权重为:1/4 1/4 1/4 1/4  预测： 加权均值
		// 初始样本都是 1/N。

（2）将训练样本代入模型，预测其输出，对那些 "预测值" 与 "实际值不同的样本"，提高其权重，
由此形成 "第二棵决策树"。重复以上过程，构建出 "不同权重的若干棵决策树" 。
		// 提高其权重，是为了与实际情况匹配
		// 相当于关注错题本
实际值：10000  但是你预测的为6000   构建第二个决策树，提高10000样本的权重
		
		//这样每个样本，在不同的树中的权重是不同的。
		

（3）得到 "每颗树" 也就有了自己的权重值。
     最后的结果，通过各个树分别预测*权重来得到。
      G1(x) * 0.67 + G2(2) * 0.97 + G3(3) * 0.53
	 //================================= "每颗树" 的权重怎么算
      "每颗树的误差率"   =  每棵树 "分错的样本" 的权重值 之和
      通过 "每颗树的误差率" 再算出 "每颗树的权重"
```







```

```



正向激励相关API：

```python
import sklearn.tree as st
import sklearn.ensemble as se
# model: 决策树模型（一颗）
model = st.DecisionTreeRegressor(max_depth=4)
# 自适应增强决策树回归模型	
# 
model = se.AdaBoostRegressor(model, n_estimators=400, random_state=7)

正向激励 的基础模型 ： 决策树
n_estimators：构建400棵不同权重的决策树，训练模型

# 训练模型
model.fit(train_x, train_y)
# 测试模型
pred_test_y = model.predict(test_x)
```

案例：基于正向激励训练预测波士顿地区房屋价格的模型。

```python
# 创建基于决策树的正向激励回归器模型
model = se.AdaBoostRegressor(
	st.DecisionTreeRegressor(max_depth=4), n_estimators=400, random_state=7)
# 训练模型
model.fit(train_x, train_y)
# 测试模型
pred_test_y = model.predict(test_x)
print(sm.r2_score(test_y, pred_test_y))
```

```javascript
//============================================== 注意：

（1）注意： 这里有400棵树，不方便可视化，集成学习一般不支持可视化，
		//可以画，但是不能显示，因为树太多了。
（2）而 "特征重要性" 是可以可视化的
//==============================================

```

**特征重要性**

```javascript
作为 "决策树模型" 训练过程的 "副产品"，根据划分子表时选择特征的顺序标志了 "该特征的重要程度"，此即为 "该特征重要性指标" 。
训练得到的  "模型对象" 提供了属性：feature_importances_ 来存储  "每个特征的重要性"。
  //先划分 "谁" ，"谁" 就更重要


//=======================================
feature_importances_ 拿到是一个比例
			//如果其中有0值，说明某些特征重要性，没有用到。





```





获取样本矩阵特征重要性属性：

```python
model.fit(train_x, train_y)
fi = model.feature_importances_
```

案例：获取普通决策树与正向激励决策树训练的两个模型的特征重要性值，按照从大到小顺序输出绘图。

```python
import matplotlib.pyplot as mp

model = st.DecisionTreeRegressor(max_depth=4)
model.fit(train_x, train_y)
# 决策树回归器给出的特征重要性
fi_dt = model.feature_importances_
model = se.AdaBoostRegressor(
    st.DecisionTreeRegressor(max_depth=4), n_estimators=400, random_state=7)
model.fit(train_x, train_y)
# 基于决策树的正向激励回归器给出的特征重要性
fi_ab = model.feature_importances_

mp.figure('Feature Importance', facecolor='lightgray')
mp.subplot(211)
mp.title('Decision Tree', fontsize=16)
mp.ylabel('Importance', fontsize=12)
mp.tick_params(labelsize=10)
mp.grid(axis='y', linestyle=':')
sorted_indices = fi_dt.argsort()[::-1]
pos = np.arange(sorted_indices.size)
mp.bar(pos, fi_dt[sorted_indices], facecolor='deepskyblue', edgecolor='steelblue')
mp.xticks(pos, feature_names[sorted_indices], rotation=30)
mp.subplot(212)
mp.title('AdaBoost Decision Tree', fontsize=16)
mp.ylabel('Importance', fontsize=12)
mp.tick_params(labelsize=10)
mp.grid(axis='y', linestyle=':')
sorted_indices = fi_ab.argsort()[::-1]
pos = np.arange(sorted_indices.size)
mp.bar(pos, fi_ab[sorted_indices], facecolor='lightcoral', edgecolor='indianred')
mp.xticks(pos, feature_names[sorted_indices], rotation=30)
mp.tight_layout()
mp.show()
```



##### GBDT-梯度提升决策树

```javascript

GBDT（Gradient Boosting Decision Tree 梯度提升树）通过:
		 （1）"多轮迭代"，每轮迭代产生一个"弱分类器（决策树）"，
		 （2）"每个分类器" 在上一轮分类器的残差**（残差在数理统计中是指实际观察值与估计值（拟合值）之间的差）**基础上进行训练。
              基于 "预测结果的残差" 设计 "损失函数"。 "GBDT训练的过程" 即是 "求该损失函数最小值" 的过程。
              //

```





### 案例

![GBDT案例](.\images\GBDT案例.png)





### 集合算法（二）、GBDT原理



原理1：

![GBDT原理1](.\images\GBDT原理1.png)

```javascript
//==================================== "残差计算"：第一棵树
//以A样本为例
（1）A的金额为1000，所以走 "左子节点"，
（2）左子节点有两个样本，两个样本的年龄为14、16，它们的平均值为15，所以A带进去预测得到15
（3）14-15，"实际值" 减去 "预测值" = -1,也就是 "残差"

（4）下一颗树，都是在上一颗树 "残差" 的基础上进行的训练。
	 所以现在：消费金额  +  上网时长  = x， 而残差 = y。来构建第二颗树

```

---------------

原理2：

![GBDT原理2](.\images\GBDT原理2.png)

```javascript

//==================================== "残差计算"：第二棵树
//以A样本为例
（5） A样本为例，A样本上网时长为 1   ，走左节点，左节点两个节点，A和C ，两个样本 "残差" 为 0 。
（5） B样本为例，B样本上网时长为 1.2 ，走右节点，右节点两个节点，B和D ，两个样本 "残差" 为 0 。
（5） C样本为例，C样本上网时长为 0.9 ，走左节点，左节点两个节点，A和C ，两个样本 "残差" 为 0 。
（5） D样本为例，D样本上网时长为 1.5 ，走右节点，右节点两个节点，B和D ，两个样本 "残差" 为 0 。

//不断拟合残差，让它为0，
//残差为0，说明预测非常准，但它只是针对于训练集。
//但是这也就出现了过度拟合，所以也需要限制树的深度。


```



--------------

原理3：

![GBDT原理3](.\images\GBDT原理3.png)

```javascript
//==================================== 预测
（6） 先带到第一颗树，预测值是25
（7） 再带到第二颗树，预测值是1
（8）  两个值相加，得到26

//========================================================================核心
这就相当于 "第一颗树" 得到一个年龄，然后 "后面的树"  ，"利用残差" 进行修正。
		//所以第一棵树也比较关键

```





```python
import sklearn.tree as st
import sklearn.ensemble as se
# 自适应增强决策树回归模型	
# n_estimators：构建400棵不同权重的决策树，训练模型
model = se.GridientBoostingRegressor(
    	max_depth=10, n_estimators=1000, min_samples_split=2)
# 训练模型
model.fit(train_x, train_y)
# 测试模型
pred_test_y = model.predict(test_x)
```

```
boosting : Adaboost  GBDT
```







##### 集合算法（三）、Bootstrap Aggregating-自助聚合



```javascript
每次从 "总样本矩阵" 中以 "有放回抽样的方式" 随机抽取部分样本构建决策树，
这样形成 "多棵包含不同训练样本" 的决策树，以削弱 "某些强势样本" 对模型预测结果的影响，提高 "模型的泛化特性" 。
```



##### 集合算法（四）、RandomForest-随机森林



```javascript
在 "自助聚合" 的基础上，每次构建 "决策树模型" 时，
不仅随机 "选择部分样本"，
而且 "还随机选择部分特征"，这样的集合算法，不仅规避了 "强势样本对预测结果" 的影响，
而且也削弱了 "强势特征" 的影响，
使模型的预测能力 "更加泛化"。

```

随机森林相关API：

```python
import sklearn.ensemble as se
# 随机森林回归模型	（属于集合算法的一种）
# max_depth：决策树最大深度10
# n_estimators：构建1000棵决策树，训练模型
# min_samples_split: 子表中最小样本数 若小于这个数字，则不再继续向下拆分
model = se.RandomForestRegressor(
    max_depth=10, n_estimators=1000, min_samples_split=2)
```

案例：分析共享单车的需求，从而判断如何进行共享单车的投放。

```
1。加载并整理数据集
2.特征分析
3.打乱数据集，划分训练集，测试集
```



```python
import numpy as np
import sklearn.utils as su
import sklearn.ensemble as se
import sklearn.metrics as sm
import matplotlib.pyplot as mp

data = np.loadtxt('../data/bike_day.csv', unpack=False, dtype='U20', delimiter=',')
day_headers = data[0, 2:13]
x = np.array(data[1:, 2:13], dtype=float)
y = np.array(data[1:, -1], dtype=float)

x, y = su.shuffle(x, y, random_state=7)
print(x.shape, y.shape)
train_size = int(len(x) * 0.9)
train_x, test_x, train_y, test_y = \
    x[:train_size], x[train_size:], y[:train_size], y[train_size:]
# 随机森林回归器
model = se.RandomForestRegressor( max_depth=10, n_estimators=1000, min_samples_split=2)
model.fit(train_x, train_y)
# 基于“天”数据集的特征重要性
fi_dy = model.feature_importances_
pred_test_y = model.predict(test_x)
print(sm.r2_score(test_y, pred_test_y))

data = np.loadtxt('../data/bike_hour.csv', unpack=False, dtype='U20', delimiter=',')
hour_headers = data[0, 2:13]
x = np.array(data[1:, 2:13], dtype=float)
y = np.array(data[1:, -1], dtype=float)
x, y = su.shuffle(x, y, random_state=7)
train_size = int(len(x) * 0.9)
train_x, test_x, train_y, test_y = \
    x[:train_size], x[train_size:], \
    y[:train_size], y[train_size:]
# 随机森林回归器
model = se.RandomForestRegressor(
    max_depth=10, n_estimators=1000,
    min_samples_split=2)
model.fit(train_x, train_y)
# 基于“小时”数据集的特征重要性
fi_hr = model.feature_importances_
pred_test_y = model.predict(test_x)
print(sm.r2_score(test_y, pred_test_y))
```

画图显示两组样本数据的特征重要性：

```python
mp.figure('Bike', facecolor='lightgray')
mp.subplot(211)
mp.title('Day', fontsize=16)
mp.ylabel('Importance', fontsize=12)
mp.tick_params(labelsize=10)
mp.grid(axis='y', linestyle=':')
sorted_indices = fi_dy.argsort()[::-1]
pos = np.arange(sorted_indices.size)
mp.bar(pos, fi_dy[sorted_indices], facecolor='deepskyblue', edgecolor='steelblue')
mp.xticks(pos, day_headers[sorted_indices], rotation=30)

mp.subplot(212)
mp.title('Hour', fontsize=16)
mp.ylabel('Importance', fontsize=12)
mp.tick_params(labelsize=10)
mp.grid(axis='y', linestyle=':')
sorted_indices = fi_hr.argsort()[::-1]
pos = np.arange(sorted_indices.size)
mp.bar(pos, fi_hr[sorted_indices], facecolor='lightcoral', edgecolor='indianred')
mp.xticks(pos, hour_headers[sorted_indices], rotation=30)
mp.tight_layout()
mp.show()
```

 



## 