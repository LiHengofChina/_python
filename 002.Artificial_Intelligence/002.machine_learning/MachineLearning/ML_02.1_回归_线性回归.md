# 一、线性模型

## 1. 概述

**线性模型**是自然界最简单的模型之一，它描述了一个（或多个）自变量对另一个因变量的影响是呈简单的比例、线性关系.例如： 

```javascript
（1）住房每平米单价为1万元，
        100平米住房价格为100万元，
        120平米住房为120万元；
        y = 10000 * x
        //y 就是因变量
        //x 就是自变量
（2） 一台挖掘机每小时挖 100m^3沙土，
	 工作4小时可以挖掘400m^3 沙土. 
     	y = 100 * x
        //y 就是因变量
        //x 就是自变量

```



```javascript
线性模型在 "二维空间" 内表现为一条直线，
 在 "三维空间" 内表现为一个平面，
 		/**
            当我们进入到三维空间中，线性模型预测的依然是这种线性关系，但现在我们有两个自变量。所以模型变为 
            z=ax+by+c，
            其中 z 是因变量，
            x 和 y 是自变量，
            a 和 b 是系数，c 是截距。
            在三维空间中，这个方程描述的是一个平面。
 		*/
 "更高维度" 下的线性模型很难用几何图形来表示（称为 "超平面"）.
如下图所示：
```



![lieaner_1](img/lieaner_1.png)

<center><font size=2>二维空间下线性模型表现为一条直线</font></center>
![lieaner_2](img/lieaner_2.png)

<center><font size=2>三维空间下线性模型表现为一个平面</font></center>
```javascript
用一条直线（或一个平面）去拟合数据。

"线性回归" 是要根据一组输入值和输出值（称为样本），寻找一个 "线性模型"，
能 "最佳程度" 上拟合于 "给定的数值分布"，从而再给定新的输入时预测输出.

		//========================================= 线性回归 要做的就是
		//========================================= 线性回归 要做的就是
		怎么去把 "这个模型" 给它 "找出来"

			/**
				什么叫 "拟合"，假设我有一组数据，
				我想用一个 "线性模型" 来表达这组数据，
				这个过程就叫 "拟合"
				
			*/

            /**
            我们要把 "数据" 带到 "线性模型" 里面去，找到k和b 的值的过程，就是 "线性回归"的过程

                "回归"： 在有监督的基础之上得到连续输出 

                "线性回归" 是要根据 一组 "输入值和输出值" ，
                寻找一个线性模型:
                    能 "最佳程度"上 "拟合" 于 "给定的数值" 分布，
                    从而再 "给定新的输入时" 预测输出，
            */


样本如下表所示:
	//首先有这样一组数据
```



| 输入(x) | 输出(y) |
| ------- | ------- |
| 0.5     | 5.0     |
| 0.6     | 5.5     |
| 0.8     | 6.0     |
| 1.1     | 6.8     |
| 1.4     | 6.8     |

根据样本拟合的线性模型如下图所示：

![linear_3](img/linear_3.png)

```javascript
		//然后用一个散点图，画出了这一组数据
		//然后用一个线性模型拟合这组数据
		//然后带入新的值，预测它的结果
```



## 2. 线性模型定义

设给定一组属性$x, x=(x_1;x_2;...;x_n)$，线性方程的一般表达形式为：
$$
y = w_1x_1 + w_2x_2 + w_3x_3 + ... + w_nx_n + b
$$
```javascript
					// x 是自变量
					// y 是因变量
					// w 是权重，每个 自变量 都有一个权重，权重越高，影响越大。
	//===========================================================
```

写成向量形式为：
$$
y = w^Tx + b
$$
```javascript
				// X乘以W的T次方  + b
				/**
					w转字乘以x  加B。
					这个和上面的公式表达内容相同 
					用矩阵 相乘 便可以证明
					列 乘以 行
				*/
```

其中，$w=(w_1;w_2;...;w_n), x=(x_1;x_2;...;x_n)$，w和b经过学习后，模型就可以确定. 

```javascript
			模型经过不断学习之后，模型就可以确定 w 和 b当前取值多少，
			就可以确定模型了。
```

```javascript
	//===========================================================
	当 "自变量数量为1" 时，上述线性模型即为平面下的直线方程：
			//意思是：只有一个X
```

$$
y = wx + b
$$
```javascript
					// 当自变量只有一个的时候，
					//		公式就变成了 y = w1x1 + b
					//		w1 也就是k，y = kx + b
                    /**
                        y  = kx + b		//k叫斜率  b 叫截距
                        y  = wx + b		//w叫权重  b 叫偏置
                        有多少个x,就有多少个w
                    */

					// 自变量可以有一个，也可以有多个
					// 但是自变量的 "最高次幂" 只有是1 ，就是最高是x的1次方
					// 如果是2，如x的2次方，这条线就不是直线了

```

```javascript
//===============================================================这段话很重要
"线性模型" 形式简单、易于建模，却蕴含着 "机器学习"中一些重要的 "基本思想".
	//麻雀虽小，五脏俱全
    许多 "功能强大" 的 "非线性模型" 可以在 "线性模型" 基础上 "引入层级结构" 或 "高维映射"而得. 
    此外，由于$w$直观表达了各属性在预测中的重要性，因此线性模型具有很好的可解释性.例如，判断一个西瓜是否为好瓜，可以用如下表达式来判断：
```

$$
f_{好瓜}(x) = 0.2x_{色泽} + 0.5x_{根蒂} + 0.3x_{敲声} + 1
$$
上述公式可以解释为，一个西瓜是否为好瓜，可以通过色泽、根蒂、敲声等因素共同判断，其中根蒂最重要(权重最高)，其次是敲声和色泽.



```java
	/** 数组和数组相乘：对应位置相乘
		>>> import numpy as np
		>>> ary = np.arange(1,10).reshape(3,3)
		>>> ary
		array([[1, 2, 3],
			   [4, 5, 6],
			   [7, 8, 9]])
		>>> print(ary)
		[[1 2 3]
		 [4 5 6]
		 [7 8 9]]
		>>> ary *ary
		array([[ 1,  4,  9],
			   [16, 25, 36],
			   [49, 64, 81]])
		>>>
	*/


//================================================================== 矩阵
//================================================================== 矩阵
另一种写法
m = np.mat('1 2;3 4')

//================================================================== 矩阵 与 矩阵  相乘
//================================================================== 矩阵 与 矩阵  相乘

	/**
		示例2：
		>>> a = np.mat([[1,2],[3,4]])
		>>> b = np.mat([[1,2],[3,4]])
		>>> a * b
		matrix([[ 7, 10],
				[15, 22]])
		>>>
	*/
//==================================================================
//==================================================================
矩阵 是二维的，有行有列的值

1 2		1 2
3 4		3 4
a		b

"a的所有行" 乘以 "b的所有列"
"对应位置" 相乘之后 "再相加（也就是内积）"
   
/**
	"A的第一行  1 2" 乘以 "B的第一列 1 3" ，就是 1*1 + 2*3  = 7
	"A的第一行  1 2" 乘以 "B的第二列 2 4" ，就是 1*2 + 2*4  = 10

	"A的第二行  3 4" 乘以 "B的第一列 1 3" ，就是 3*1 + 4*3  = 15
	"A的第二行  3 4" 乘以 "B的第二列 2 4" ，就是 3*2 + 4*4  = 22

	最后得到
		7 10
		15 22
*/

//================================================================== 并不是  "所有矩阵" 都能相乘
//================================================================== 并不是  "所有矩阵" 都能相乘

		（1）//"A的列数" 和 "B的行数" 相等才能相乘
			(2,2) (2,2)		//中间两个值相等才能相乘

		（2）相乘结果：A的行数，B的列数
			(2,2) (2,2)		//相乘结果，由两边决定

		//======================== 举例：
		//======================== 举例：
		(3861,24) 与 (24,18) 两个矩阵

		中间相同：可以相乘，
		相乘结果：3861行，18列
//================================================================== 没有 乘法交换律
//================================================================== 没有 乘法交换律

// 矩阵 和 矩阵 之间没有 "乘法交换律" 的
	a * b  != b * a

	(2,4) * (4,3)  //可以相乘，结果为 (2,3)形状

	而(4,3) * (2,4) //不可以相乘

	//但是：有 乘法结合律
    // ( a * b ) * c = a * ( b * c) 

//================================================================== 矩阵相乘 和线性回归有什么关系
//================================================================== 矩阵相乘 和线性回归有什么关系


线性模型 写成向量的形式， y= x*w^T +b    //X乘以W的T次方  + b

	如：
		x1   x2   x3  			W1   W2   W3
		//====================================
		//然后把 w转字 			//转字可以减少对 "原始矩阵" 的访问次数（重点）

		x1   x2   x3			
								w1
								w2
								w3

		//这样就变成了行转列，也就是矩阵的乘法。
		//"a的所有行" 乘以 "b的所有列"
		//这里只有一行一列，所以就变成 了
		//y = w1x1 + w2x2 + w3x3 + b
		//所以说 矩阵相乘 就可以实现线性回归。
//==================================================================
//矩阵相乘  和 线性回归有什么关系，是不是 "矩阵相乘  " 可以更加形象的表示 线性回归
//==================================================================
"矩阵相乘" 在 "线性回归" 中起着 "核心作用"，
    因为它提供了：一种 "计算效率高" 且 "表达简洁的方式" 来描述和计算线性回归模型。



```







## 3. 模型训练

```javascript
在 "二维平面" 中，"给定两点" 可以确定 "一条直线.

但在实际工程中，可能有 "很多个样本点" ，无法找到 "一条直线精确穿过所有样本点"，
只能找到一条与样本"足够接近" 或 "距离足够小" 的直线，近似 "拟合" 给定的样本.如下图所示：

```

![linear_4](img/linear_4.png)

```javascript

我们的目的是让 "这条直线" 到达 "所有样本点" 的  "距离" 达到最小值




			怎么表达："每一个样本" 到 "这条直线" 的 "距离" 呢？
					// "真实值"  和 "预测值" 之间的差异，
					// 就是 "真实值" -  "预测值"  = 距离
					  // Y1 - Y1' 得到这个距离  // Y1 表示真实值，Y1' 表示预测值
					  // Y2 - Y2' 得到这个距离
					  // Y3 - Y3' 得到这个距离

			怎么让 "所有距离" 都  "同时达到"  最小的值？
					  // 其实就是： 针对 "所有样本它的真实值" 到 "模型预测值" 的距离，
            		  //=========================（重点）=========================================
            		  //=========================（重点）=========================================
           			  //=============================(重点：平均值)
					  // 它们（指上面所有这些距离）的 "平均值" 达到最小 

					  //现 在就可以把每一个样本点，它们的 "真实值" 和 "预测值"  的 "差异" ，
					  //加到一起，求一个平均值：
								∑(Yi - Yi')^2 / n
									//∑(Yi - Yi')表示所有点的误差加到一起
                                  	// "平方"是为了防止负数,也可以使用绝对值，
									//  除以 n，是为了求平均值，
									// 这里为什么使用平方，因为 "平方它的曲线" 的最低点，
									// 是可V的。因为后面要对它进行求导

				            		//=========================（重点） =======（核心）=======================
				            		//=========================（重点） =======（核心）=======================
				            		//=========================（这就是机器学习的核心）===============
									//	(Yi - Yi') 的平方 求和 然后除以n
									//它表达的含义，就是 "当前这个模型" 距离 "每一个样本点" 的 "平均距离"
									//让它达到 "极小值"，就是 "模型" 针对所有样本预测的 "最准的状态"。
 
 				            		//=========================（问题转换）===============
 				            		//=========================（问题转换）===============
                                    ∑(Yi - Yi')^2 / n 
                                    我们就是为了找到 "最优的" 模型参数，让这个参数最准，
                                    "寻找最优模型参数" 的问题，转换成了: 求 "这个公式" 极小值的问题
   
```



### 1）损失函数

```javascript
其实 ∑(Yi - Yi')^2 / n  这种函数叫：损失函数

如何确定 "直线到所有样本" 足够近呢？
		可以使用 "损失函数" 来进行度量
		
损失函数用来度量 "真实值（由样本中给出）" 和 "预测值（由模型算出）" 之间的差异.
损失函数值越小，表明模型预测值和真实值之间差异越小，模型性能越好；
损失函数值越大，模型预测值和真实值之间差异越大，模型性能越差.
     /**
     	 “损失函数”是机器学习中的一个 “广义概念”，用于衡量 “模型的预测值” 与 “真实值” 之间的 “不一致程度”。
     	 而“均方差函数”（Mean Squared Error, MSE）是一种 “具体的损失函数” ，通常用于 “回归问题”。
     	
     */
在回归问题中，"均方差" 是常用的损失函数，其表达式如下所示：
```

$$
E = \frac{1}{2}\sum_{i=1}^{n}{(y - y')^2}
$$
```javascript
								"乘以 1/2"  和  "乘以 1/n"   
								//乘以 1/2 指缩小两倍，乘以 1/N 缩小N倍
								单调性不会变，
                                另外：2分之1，求导更加方便。
                                //=========================为什么求导方便：
                                //因为：1/2 * W0^ 2 ，可以约掉，两个2
                                
                                
/**
"均方差"
            //均：指平均，求和除以个数，差指误差
            //均方差: 名字由 "平均、平方、误差" 而来。
*/
                                
                                
当我们拿到损失函数之后，我们就要去求极它的极小值了

```



```javascript
其中，y为模型 "预测值"，y' 为真实值. 
"均方差" 具有非常好的几何意义，对应着常用的 "欧几里得距离"（简称欧式距离）. 


"线性回归" 的任务是要 "寻找" 最优 "线性模型"，
			 是的 "损失函数" 值最小，即：

```


$$
(w^*, b^*) = arg min \frac{1}{2}\sum_{i=1}^{n}{(y - y')^2}   \\ 
    = arg min \frac{1}{2}\sum_{i=1}^{n}{(y' - wx_i - b)^2}
$$


```javascript
//==========================================================================
//==========================================================================

	loss = 1/2 * ∑(Yi' - Yi)^2			 // 这就是损失函数，现在就要求它的极小值
		 = 1/2 * ∑((WX + B) - Y )^2
				// Yi' 是预测值，因为 y = wx +b ，所以把 Yi' 替换成 (wx + b)
				// Yi' 就是预测值
				// loss 就是因变量 ，我们要求 loss 的极小值，
				// 当 loss 的值达到多少的时候，
				// 当我们的 "自变量" 是多少时，loss 才能达到极小值
				// 1/2 * ∑((WX + B) - Y )^2 
				// 影响 loss 的  "自变量"  有哪些
				//  		w 和  b   //  Y 和 x 都是真实样本数据 
				//========================（重点）========================================
				// w 和 b 的值是多少的时候， lose损失函数 能达到极小值
				// 不断的更新w 和 b，让lose函数达到极小值，
				// 学习就是为了改善性能，
				// 机器学习的套路，就是求 "损失函数" 的极小值，


一个 w 和 b 决定 loss，那就是两个x决定一个y，它们就是一个 "3维关系"
```

```javascript
//=============================================================== 最小二乘法
//=============================================================== 最小二乘法

基于 "均方误差最小化" 来进行 模型求解的方法称为 "最小二乘法". 
线性回归中，最小二乘法：就是试图找到一条直线，
是所有样本到直线的 "欧式距离" 之和最小.  可以将损失函数 "对w和b分别求导" ，
得到损失函数的 "导函数" ，并令 "导函数为0" 即可得到 "w和b的最优解".
		//======================
		其实，如果想求 w 和 b 的最小值的时候，
        我们会把数据转成矩阵，就是把x和y转成矩阵，通过矩阵求逆的方式，
        把矩阵和W解出来，如果矩阵有逆，可以直接求
        如果矩阵没有逆则不可以求

//=============================================================== 最小二乘法 与 导数
//=============================================================== 最小二乘法 与 导数     
        最小二乘法通常会用到 "导数"。
        最小二乘法是一种 "数学优化技术"，
        用于找到一组参数，使得这些参数的模型与数据之间的 "误差平方和最小"。
在 "最小二乘法" 的应用中，通常涉及到 "以下几个步骤"：
（1）定义误差函数：首先，定义一个误差函数，通常是 "预测值" 与 "实际值" 之间差的平方和。
（2）应用导数：为了找到使误差函数最小的参数，需要对 "误差函数" 关于 "每个参数" 求导。
（3）求导数的零点：找到这些 "导数等于零" 的点。这些点可能是 "误差函数的极小值点" 。
（4）解方程组：通过解这些 "导数等于零" 的方程组，可以找到使误差最小化的参数值。

在实际应用中，尤其是在 "处理线性回归问题" 时， "最小二乘法" 的使用非常广泛。
"求导" 在这个过程中扮演着 "关键角色"，因为它 "帮助确定了在哪些参数值" 下，"误差函数" 达到最小值。
        
//=============================================================== 最小二乘法 除了求导还有 "其它解法" 吗
//=============================================================== 最小二乘法 除了求导还有 "其它解法" 吗
"最小二乘法" 确实不仅限于 "求导数的方法" 来找到 "误差函数的极小值"
尽管在许多情况下，使用 "导数" 是一种有效的方式来找到这些极小值点，
但还有 "其他方法" 可以用于 "特定的情形或者更加复杂的模型"。这些方法包括：
	（1）解析解法
	（2）数值优化方法
	（3）迭代法：
    （4）蒙特卡洛方法
	（5）正则化方法：
//=============================================================== 为什么叫 "最小二乘法"
//=============================================================== 为什么叫 "最小二乘法"
 "最小二乘法（Least Squares Method）" 这个名字的含义直接源于 "其核心原理"：
 	 "最小化误差" 的 "平方和"。
     这里的 "二乘" 指的是 "平方"（即"数的二次方"），而 "最小" 指的是 "寻找参数值" 使得这个 "平方和" 尽可能小。
     具体来说：
    	（1）误差的平方：在最小二乘法中，误差通常是指 "预测值" 与 "实际观测值" 之间的差异。
    	（2）和（Sum）：将所有数据点的误差平方求和，得到一个总的误差度量。
        （3）最小化（Minimization）："最小二乘法" 的目标是找到 "一组参数"，使得这个 "误差平方和最小"。
     因此，"最小二乘法" 这个名称直接反映了 "该方法的核心目标" 和 "操作"：通过 "最小化误差的平方和" 来找到 "最佳的模型参数"。

```



```javascript
 但是：在 "人工智能领域" 里面求损失函数的极小值，使用的是是 "梯度下降法"
```

### 2）梯度下降法

#### ① 为什么使用梯度下降

```javascript

在实际计算中，通过 "最小二乘法" 求解 "最优参数" 有一定的问题：

```

（1）最小二乘法需要计算逆矩阵，有可能逆矩阵不存在；

```javascript

在实际计算中，通过 "最小二乘法" 求解最优参数有一定的问题：

（1）最小二乘法需要计算逆矩阵，有可能 "逆矩阵" 不存在；
			/**
					//=================== 什么叫 "单位矩阵"
					主对角线为1，
					其它 "对角线" 都为0的矩阵
					如：
						1 0
						0 1

					"一个矩阵" * "单位矩阵" = "原矩阵"
					这就好像： 6 * 1 = 6

					//=================== 什么叫 "逆矩阵"
					如果：  A * B = E //E表示单位矩阵
					A矩阵 * B矩阵 = 单位矩阵
					那就称 B为A的逆
					//=================== 
					A 矩阵 * 单位矩阵  = A矩阵
					1 2            1  0
					3 4            0  1
					        *
					        等到
					        1 2 
					        3 4
					//=================== 示例
					m = np.mat('1 4 7;2 3 8; 1 4 5')
					m
					m.I
						//I表示求逆矩阵

					//=================== 但不是所有矩阵都有逆
			*/
（2）当样本特征数量较多时，计算逆矩阵非常耗时甚至不可行. 
		/**
			逆矩阵 本身非常的慢，
			比如：ChatGPT模型，就有几亿个参数
				 意思就是说这几亿个参数在取什么值的时候，
				 损失函数能达到最小值
		*/
		/**
		 	比如：深度学习中，假设有一张图像，大小为 200 * 200
		 			它有 40000 个像素值，
		 			它就代表 x 为 40000 个
		 			
		 		全连接神经网络，里面有5个神经元
		 		//这是人工智能里面的 "神经元"， 又叫感知机制
		 		//它是对生物学进行模仿的结果
		 		
		 		//生物神经元的工作机器，它是怎么调用钾钠离子进行放电的?
		 		
		 		当前有4000个x 和 5个神经元，
		 		它每一个x都会分别和 每个神经元进行计算。
		 		所以它就是计算20万次，
		 		就代表它表20万条线，20万个权重(w)。
		 */

所以，在实际计算中，通常采用 "梯度下降法" 来求解损失函数的极小值，从而找到模型的最优参数.
		/**

		用 "梯度下降法" 求损失函数的极小值

		不管是深度学习，还是机器学习，它都是同样的套路，
		都是求损失函数的最小值的过程。
		
		只不过不同的任务，它的损失函数不一样。

		*/

```



#### ② 什么是梯度下降

```javascript

求损失  "函数的极小值" ,使用 "梯度下降法"

```

```javascript
"梯度(gradient)下降" 它其实就是一个简单的基本公式


梯度（gradient）是一个向量（矢量，有方向），
表示 "某一函数" 在该点处的 "方向导数" 沿着 "该方向" 取得 "最大值"，
				//===================================== 什么是梯度？
				//===================================== 什么是梯度？
				该方向的 "导数" ，沿着该方向 "取得最大值"
                /**
           		 梯度 ：指的就是导数， //指函数的 "导数"， 也就是 "导函数"
                    只不过不会让导数为0
                    而是使用 "另一种方式" ，沿着 "导数（覆梯度）"相反方向，
                    去更新参数
                */




即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大.
"损失函数" 沿 "梯度相反方向" 收敛最快（即能最快找到极值点）.
当 "梯度向量为零"（或接近于零），
说明到达一个 "极值点"，这也是 "梯度下降算法" 迭代计算的 "终止条件".


```



```javascript
//======================================================== 核心
//======================================================== 核心
这种按照 "负梯度" 不停地 "调整函数权值" 的过程就叫作 "梯度下降法".
		/**
			什么是负梯度：
				（1）如果导数是5，负梯度就是-5
                    //如梯度是5， 负梯度就是 -5 ，往小的走就是-5(减5)
            	 	"减去梯度" 就是 ”负梯度“
            
            	（2）如果导数是-5，负梯度就是5

			//================================= 这里还是比较巧妙的
			//================================= 这里还是比较巧妙的
            "负梯度"比较小的时候，你会往大的加 
            你比极小值大的时候，你会往小的减
            不管怎样，你都会向 "极小值的方向" 出发的。
		*/
通过这样的方法， "改变权重" 让损失函数的值 "下降得更快"，
进而将值收敛到 "损失函数" 的 "某个极小值".



梯度下降法算法描述如下：
（1）损失是否足够小？如果不是，计算损失函数的梯度.
（2）按梯度的反方向走一小步，以缩小损失.
（3）循环到（1）

```

 

```javascript

//==============================================  "梯度下降法" 的 "外号"
//==============================================  "梯度下降法" 的 "外号"
/**
	梯度下降法有个外号： "蒙眼下山法"
		蒙住眼睛手拿木棍，下山
*/
//============================================== 更新公式


如果不按梯度下降法处理，那可能就会错过极小值
如：按固定步长2来取
	第一次取10
	第一次取8
	第一次取6
    	 极小值在这里
	第一次取4
    //=================
    	按固定步长去寻找，可能会错过极小值，
        所以我们会按照梯度的方向去找
    //================= 更新公式 
        按照梯度的 "方向去找"
        所以我们会有一个 "更新公式"
        x = x - α * f'(x)
       	  = 12 - 0.1 * 7.00 = 11.30
          // x 指初始值，当一步的值是多少
          // α 是一个常量
          // f'(x) 指导数，当前x的梯度
          // - f'(x)就是指 "负梯度"
		  /**
		   意思是说： x现在要取多少，就等于x上一步的值12减去它导数*α
		  */

		  //====================================================== 学习率（重要）
		  //====================================================== 学习率（重要）
           * α 或 η (zata)，它的中文叫 "学习率",它的作用是 "控制学习速度"
				// 如：0.1，乘以 0.1，相当于把原来的值缩小了，通过这种方式控制 "它的下降速度"
          学习速度由我们去控制，如果这个值比较大，一次就减的比较多，
          // 那我下降的更快，学习的更快，更快的找到极小值
          // 学习率设的小，学习的慢，找到"损失函数的极小值"就慢一些
          // 学习率 是一个常量，可以设置0.1，
          // 也可以设置0.01，也可以设置0.001

          "学习率" 是我们建立人工智能模型中的 "非常重要的超参数"

		 //====================================================== 超参数
		 //====================================================== 超参数

 
         超参数 指：当我们在构建模型时，需要提前设定一些参数，
		          "这些参数" 可以决定 "模型的精度" ,这些参数就叫 "超参数"。
		       
		        ****** 超参数的取值一般来自于经验。******
		         
		         // 它与模型的 "性能" 和  "精度" 有关
		        
		        假设我们要训练50轮，意思就是我们要更新50次参数
		        "训练多少轮" 就是指 "我们要更新多少次参数"
		        
		        此时，如果 "学习率" 太小，可能没有达到 "极小值"，
		        50轮就做完了，精度就不高
		        如果 如果 "学习率" 太大，可能已经超过极小值了。
 
		     	//====================================================== 
		     	//======================================================
		     	****** 超参数的取值一般来自于经验。******   
		     	//所以说它没有固定值
		     	如果设置错误，可以调整，
		     	其实很多现成的模形早就有了，通过一行代码都能实现了。
		     	我们要做的就是，用 "这个模型" 匹配 "我们的数据"
		     	当 "你的数据" 不同的时候，"这些参数" 也是不同的
		     	
		     	那就要解决"针对这个模型"，我们的参数设置为多少的时候
		     	才能  "更好的匹配" 这组数据，说的简单点，谁的参数调的好，
		     	谁的模型就更厉害
		     	
		     	如果你以后做了 "图像算法工程师"，
		     	你们用的那些算法都是 被 "科学家们" 
		     	和 "一些工程师们"研究好的，我们拿过来直接用
		     	
		     	我们做的是：怎么让它更好的匹配 "我们的数据"
		     	
		     	我们的参数设置为多少，我们需要调整
		     	
		     	所以我们现在要学习 "它的原理"
		     	
		     	//=======================
		     	当然我们也可以设计模型，但是我们还达不到它的高度。
		     	我们是站在巨人的肩上学习，
		     	什么问题用什么模型，科学家们已经设计好了，
		     	咱们就拿到用就可以了。（现在大部分人都是做这个事情）
		     	
		     	但是你必须知道这些模型的 "特点" 和 "使用场景"，
		     	怎么使用
		     	我们其实就在学习别人的模型如何使用。

         */
          
	x = 12，下一步取多少，是通过梯度更新公式算出来的。
    x = x - α * f'(x)
      = 12 - 0.1 * ( 12 - 5 )
      = 11.3

下一步x = 11.3
    x = x - α * f'(x)
      = 11.3 - 0.1 (11.3 - 5)
      = 10.67

下一步x = 10.67

    ..直到  "损失函数不再变化了" ，或者 "变化微乎其微" 了。
    就认为 当前达到了 "损失函数" 的极小值了
    
//======================== α 取1.5
  α 取3，会越来越大，这种情况叫 "梯度爆炸"
  α 取2，会 "形成死循环"
  α 取1.5 我们管它叫："收敛的比较震荡"
  α 取0.1 而0.1那种是 "比较平缓"，比较理想的状态。
//======================== 学习率一般设置
  不同数据它的大小范围不同的
  一般设置： 
  			0.1 
			0.01 
			0.001
//======================== 动态变化的学习率

现在学习率比较简单，是一个固定的值
后面情况复杂，学习率会变
	如：我训练500轮
    		100-200轮的时候，学习率用0.1
    		200-300轮的时候，学习率用0.01，学习率会衰减。
//======================== 2014年 大规模图像挑战赛的亚军
2014年 "大规模图像挑战赛" 的亚军：vgg
它的学习率就是：0.000001
//======================== 

12 的时候降低了 0.7
8  的时候降低了 0.3 
6  的时候降低了 0.1
5  的时候降低了 0.05

//======================== 重要
说明
越远下降的越多
越近下降的越少

因为你越接近你的极小值，你的梯度越接近于0，
减的值就越来越少
//========================一句话

"梯度下降" 就是沿着 "梯度的反方向" 进行 "参数更新"

怎么更新？
	使用 "参数更新" 公式： x = x - α * f'(x)
	"当前值" - "学习率" * "导数"

使用 "参数更新公式" 来更新我们的参数？

```

```javascript

//===============================  真正的需求
实际当中，"真正的需求" 并不是一个  "二维"的，之前示例

（1）目标函数：
Y = W1*X  + W0
或
Y = W1*X  + b

（2）根据 "目标函数" 写出它的 "损失函数" 为：

loss = 1/2 * ∑((W1*X + W0) -Y)^2
       //在这里要更新的是 w1 和 w0 两个 "自变量"//两个参数

（3）通过两个"更新公式"，更新 w1 和 w0 
//之前是一个  "自变量" 可以用 求导的方法
//现在是 "两个自变量"
w1 的更新方法是：w1 - α * f'(w1)
w0 的更新方法是：w0 - α * f'(w0)

w1 和 w0是两个初始参数，自己设定的
两个 α也是我们自己设定的

//现在就要求 f'(w1) 和 f'(w0) 的导数
（4）当前的梯度
    这里 "梯度下降" 中 "梯度" 就是指： 
              "w0的导数" 和 "w1的导数" 组成的向量
              [w0的导数,w1的导数] //
                //梯度里面有 "几个值" ，有几个 "自变量"，
                //要更新 "几个参数"，它里面就有 "几个值"
                /**
                    这个值是什么？
                       是 每一个自变量的 "偏导" 
                */
	当前的梯度：		"w1的偏导数" 和 "w0的偏导数" 组成的向量
    公式表示为：
```

$$
[\frac{\Delta loss}{\Delta w_0},\frac{\Delta loss}{\Delta w_1}]
$$

```javascript
当我们有了"梯度"之后，
我们就可以用 "w1的偏导数" 更新w1的值，
用 "w0的偏导数" 更新w0的值了

//========================= 怎么更新？

依据 "参数更新方法"，
在直线方程中，有两个参数 "需要学习"， w0 和 w1 ，
梯度 "下降过程" 中，分别对 "这两个参数" 单独进行调整:
```

$$
w_0 = w_0 - \eta \frac{\Delta loss}{\Delta w_0}   \\

w_1 = w_1 - \eta \frac{\Delta loss}{\Delta w_1}
$$

```javascript

w0 = w0  - eta * w0的导数
w1 = w1  - eta * w1的导数

其中，η（eta）称为学习率，    Δloss/Δw1为梯度（即损失函数关于参数wi的偏导数）

/**

机器学习就是：使用"梯度下降法" 求损失函数的极小值的  "过程"

*/

w0 和 w1可以自己设置
η 也可以自己设
现在只需要求出 w0的导数 和 w1的导数

求偏导？和直接求导没有太大区别
	针对 "损失函数" 去求偏导
//=======================================================  "求偏导数" 和 "求导数" 
//=======================================================  "求偏导数" 和 "求导数" 
  "求偏导数" 和 "求导数" 都是微积分中的基本概念
    	（1）求导数（Derivative）
        		1）应用于 "单变量函数" ：当我们谈论求导数时，
                     通常指的是 "单变量函数的导数"。
                     比如，给定一个关于变量x的函数f(x)，其导数描述了f在x点的变化率。 
         		2）物理意义：在物理学中，导数可以表示速度、加速度等概念。
                	 例如，位置 "关于时间的导数" 是速度。
                3）数学表示：导数通常表示为 f'(x) 或 df/dx。
		（2）求偏导数（Partial Derivative）：
                1）应用于 "多变量函数"：偏导数用于 "多变量函数"。
                	例如，对于函数f(x,y)，我们可能对x的偏导数感兴趣，同时将 y 视为常数。
				2）描述函数在特定方向的变化率：
                	"偏导数" 描述了当 "其他变量" 保持不变时，
                    函数沿着 "某一特定变量" 的变化率。
                    例如， ∂f/∂x描述了在y保持固定时，f随x变化的速度。
                3）数学表示：偏导数通常表示为∂f/∂X或fx。
 
 总结来说：
 	（1）求导数是 "单变量函数" 的概念，关注函数随一个变量的变化率；
 	（2）而 "求偏导数" 是 "多变量函数的概念"，关注函数在保持其他变量不变时，
    	 沿某一个特定变量的变化率。
 在多变量函数的分析中，偏导数是一个重要的工具，因为它帮助我们理解函数在多维空间中如何变化。

//=======================================================           	
//=======================================================              
    
    
 			//损失函数
    		loss = 1/2 *∑ (w1*x + w0 -y)^2
    		
    		求w1的时候，直接把w0当成常数项就好了。w0常数项直接忽略不计
    			
    		求w0的时候，把w1当成常数项就可以了。
    		//===================（1）先求w1的偏导，变下公式//求谁就把谁单独列出来
            loss = 1/2 *∑ (w1*x + w0 -y  )^2  
            		//因为 + - 是同级运算，所以可以优先计算w0-y
            	 = 1/2 *∑ (w1*x + (w0 -y) )^2 //W0 - y 加上括号， 把w1的那一级，单独列出来
            		//此时  w1*x 看成一部分
            		// 而 (w0 - y)看成一部分，
            		// 相当于 (a + b )^2   //初中学的完全平方公式
            		// 其实它就是 a^2 + 2ab + b^2
            	 = 1/2 * ∑( w1^2*x^2 + 2*w1*x*(w0-y) +(w0-y)^2 )
					//此时， (w0-y)^2 又出现一个完全平方公式
 					//但是这里不能展开，因为需要把w0当常数处理。

            	 = ∑( 1/2*w1^2*x^2 + w1*x*(w0-y) + 1/2*(w0-y)^2)
					//此时，把1/2移动到括号里面去。
                    //中间的 1/2 * 2 约了
    		//=================== 求偏w1导数
			∂loss/∂w1  = ∑(w1*x^2  + x*(w0 -y))
                	//∂表示偏导的意思，读dee，德
					//============第 一 项
					// 1/2 * w1^2 * x^2 中有w1
					//指数是: 2
					//系数是: 1/2*X^2  //X^2也是已知数，样本数据
					//指数 * 系数 ，降次 w1^2 降成 w1^1 就是w1
					// 2 * 1/2 = 1，
					// 最后变成 1 * X^2 * w1 
					//相当于w1^2中的平方给降次和 1/2相乘被 约掉了
					//============第 二 项
					//w1*x*(w0-y) 里面也有w1
					//指数：w1，它的指数为1
					//系数：x*(w0-y)
					//指数 * 系数 ，降次 w1^1 降成 w1^0 就是1
					//最后 变成 1 * x * (w0 -y)
					//============第 三 项
					//1/2(w0-y)^2 里面没有w1，直接干掉
				    = ∑(w1*x^2  + x*(w0 -y))
					//里面两部分都有x，提取一个x
					= ∑( x *(w1*x   + w0 - y) )
    		//===================（2）求w0的偏导，变下公式//求谁就把谁单独列出来
            loss = 1/2 * ∑ (w1*x + w0 -y  )^2  
                 = 1/2 * ∑ (w0 + (w1*x -y)  )^2 //换一下位置， 把w0的那一组，单独列出来
                 = 1/2 * ∑ (w0^2 + 2* w0 * (w1*x -y) + (w1*x -y)^2) //完全平方公式
                 =  ∑ (1/2 * w0^2 +  w0 * (w1*x -y) + 1/2* (w1*x -y)^2) //1/2放进去
    		//=================== 求偏w0导数
			∂loss/∂w0  = ∑ (w0 + w1*x -y)
					//============第 一 项："w0的指数" * 系数 * w0降次
                	//1/2 * w0^2  其中有w0
                	// w0 指数：2
                	//系数是： 1/2
					// 2 * 1/2 = 1，w0^2降次为w0^1
                	//得到w0
					//============第 二 项
                    // w0 * (w1*x -y) 里面有w0，
                    // w0 指数为1
                    // 系数为:(w1*x -y)
                    // W0^1 B降次为w0^0 = 1
                    // 得到 (w1*x -y)
					//============第 三 项
                    //第三项里面没有w0，直接干掉
            	 	= ∑ ( w1*x + w0   -y) //换一位置得到


//============================================== 到这里
//============================================== 到这里
我们现在只需要设定 w0 和 w1，再设置 η
然后带到下面的公式去，就可以按照 "参数更新公式" 去更新，模形参数了。
```

$$
w_0 = w_0 - \eta \frac{\Delta loss}{\Delta w_0}   \\

w_1 = w_1 - \eta \frac{\Delta loss}{\Delta w_1}
$$

```javascript

/**注意：
   不要以为求一下面的内容，就能解决所有问题了
   前面只是求的 y = w1*x  + w0 的目标函数
   
   如果有两个x决定一个y ： y = w1*x1 + w2*x2+ w0
   这时的的损失函数：
   				loss = 1/2 * ∑((w1*x1 + w2*x2+ w0) - y )^2
   						//这里除以2，也可以除以n
   				这时就有 "3个自变量" 了，w0、w1、w2
   			此时，就需要根据这个损失函数，去求导，
   				 求它们这3个参数的 导数
   				 
   		  所以前面求的是针对：一个x决定一个y的这种线性模型，
   		  并不是一劳永逸的
   		  
   		  所有的代码在写的时候，都要自己设计损失函数，
   		  自己去求导?自己去实现参数更新公式?
   		  其实不用，因为接口都已经写好了的。   				

*/

```



![gradent_decent](img/gradent_decent.png)

梯度下降法中通过沿着梯度负方向不断调整参数，从而逐步接近损失函数极小值所在点. 如下图所示：

![gradent_decent2](img/gradent_decent2.png)

#### ③ 参数更新法则

在直线方程中，有两个参数需要学习，$w_0$和$w_1$，梯度下降过程中，分别对这两个参数单独进行调整，调整法则如下：
$$
w_0 = w_0 + \Delta w_0\\
w_1 = w_1 + \Delta w_1
$$
$\Delta w_0$和$\Delta w_1$可表示为：
$$
\Delta w_0 = -\eta \frac{\Delta loss}{\Delta w_0}\\
\Delta w_1 = -\eta \frac{\Delta loss}{\Delta w_1}\\
$$
其中，$\eta$称为学习率，$\frac{\Delta loss}{\Delta w_i}$为梯度（即损失函数关于参数$w_i$的偏导数）. 损失函数表达式为：
$$
loss =\frac{1}{2}\sum(y - y')^2 =  \frac{1}{2}\sum((y-(w_0+w_1x))^2)
$$
对损失函数求导（求导过程见补充知识），可得$w_0, w_1$的偏导数为：
$$
\frac{\Delta loss}{\Delta w_0} = \sum((y - y')(-1)) = -\sum(y - y')\\
\frac{\Delta loss}{\Delta w_1} = \sum((y - y')(-x)) = -\sum(x(y - y'))
$$

## 4. 实现线性回归







#### ① 自己编码实现

以下是实现线性回归的代码：

```Python
# 线性回归示例
import numpy as np
import matplotlib.pyplot as mp
from mpl_toolkits.mplot3d import axes3d
import sklearn.preprocessing as sp

# 训练数据集
train_x = np.array([0.5, 0.6, 0.8, 1.1, 1.4])  # 输入集
train_y = np.array([5.0, 5.5, 6.0, 6.8, 7.0])  # 输出集

n_epochs = 1000  # 迭代次数
lrate = 0.01  # 学习率
epochs = []  # 记录迭代次数
losses = []  # 记录损失值

w0, w1 = [1], [1]  # 模型初始值

for i in range(1, n_epochs + 1):
    epochs.append(i)  # 记录第几次迭代

    y = w0[-1] + w1[-1] * train_x  # 取出最新的w0,w1计算线性方程输出
    # 损失函数(均方差)
    loss = (((train_y - y) ** 2).sum()) / 2
    losses.append(loss)  # 记录每次迭代的损失值

    print("%d: w0=%f, w1=%f, loss=%f" % (i, w0[-1], w1[-1], loss))

    # 计算w0,w1的偏导数
    d0 = -(train_y - y).sum()
    d1 = -(train_x * (train_y - y)).sum()

    # 更新w0,w1
    w0.append(w0[-1] - (d0 * lrate))
    w1.append(w1[-1] - (d1 * lrate))
```

程序执行结果：

```
1 w0=1.00000000 w1=1.00000000 loss=44.17500000
2 w0=1.20900000 w1=1.19060000 loss=36.53882794
3 w0=1.39916360 w1=1.36357948 loss=30.23168666
4 w0=1.57220792 w1=1.52054607 loss=25.02222743
5 w0=1.72969350 w1=1.66296078 loss=20.71937337
......
996 w0=4.06506160 w1=2.26409126 loss=0.08743506
997 w0=4.06518850 w1=2.26395572 loss=0.08743162
998 w0=4.06531502 w1=2.26382058 loss=0.08742820
999 w0=4.06544117 w1=2.26368585 loss=0.08742480
1000 w0=4.06556693 w1=2.26355153 loss=0.08742142
```

可以给数据加上可视化，让结果更直观.添加如下代码：

```Python
###################### 训练过程可视化 ######################
# 训练过程可视化
## 损失函数收敛过程
w0 = np.array(w0[:-1])
w1 = np.array(w1[:-1])

mp.figure("Losses", facecolor="lightgray")  # 创建一个窗体
mp.title("epoch", fontsize=20)
mp.ylabel("loss", fontsize=14)
mp.grid(linestyle=":")  # 网格线：虚线
mp.plot(epochs, losses, c="blue", label="loss")
mp.legend()  # 图例
mp.tight_layout()  # 紧凑格式

## 显示模型直线
pred_y = w0[-1] + w1[-1] * train_x  # 根据x预测y
mp.figure("Linear Regression", facecolor="lightgray")
mp.title("Linear Regression", fontsize=20)
mp.xlabel("x", fontsize=14)
mp.ylabel("y", fontsize=14)
mp.grid(linestyle=":")
mp.scatter(train_x, train_y, c="blue", label="Traing")  # 绘制样本散点图
mp.plot(train_x, pred_y, c="red", label="Regression")
mp.legend()

# 显示梯度下降过程(复制粘贴即可，不需要编写)
# 计算损失函数曲面上的点 loss = f(w0, w1)
arr1 = np.linspace(0, 10, 500)  # 0~9间产生500个元素的均匀列表
arr2 = np.linspace(0, 3.5, 500)  # 0~3.5间产生500个元素的均匀列表

grid_w0, grid_w1 = np.meshgrid(arr1, arr2)  # 产生二维矩阵

flat_w0, flat_w1 = grid_w0.ravel(), grid_w1.ravel()  # 二维矩阵扁平化
loss_metrix = train_y.reshape(-1, 1)  # 生成误差矩阵（-1,1）表示自动计算维度
outer = np.outer(train_x, flat_w1)  # 求外积（train_x和flat_w1元素两两相乘的新矩阵）
# 计算损失：((w0 + w1*x - y)**2)/2
flat_loss = (((flat_w0 + outer - loss_metrix) ** 2).sum(axis=0)) / 2
grid_loss = flat_loss.reshape(grid_w0.shape)

mp.figure('Loss Function')
ax = mp.gca(projection='3d')
mp.title('Loss Function', fontsize=14)
ax.set_xlabel('w0', fontsize=14)
ax.set_ylabel('w1', fontsize=14)
ax.set_zlabel('loss', fontsize=14)
ax.plot_surface(grid_w0, grid_w1, grid_loss, rstride=10, cstride=10, cmap='jet')
ax.plot(w0, w1, losses, 'o-', c='orangered', label='BGD', zorder=5)
mp.legend(loc='lower left')

mp.show()
```

数据可视化结果如下图所示：

![linear_5](img/linear_5.png)

<center><font size=2>回归得到的线性模型</font></center>
![](img/linear_loss.png)

<center><font size=2>损失函数收敛过程</font></center>
![linear_6](img/linear_6.png)

<center><font size=2>梯度下降过程</font></center>
#### ② 通过sklearn API实现

```javascript

在sklearn中有一个模块叫做：linear_model ，线性模型，
				//关于线性模型的，不只有 "线性回归"，还有其它的
				//其实关于线性的模型，都在linear_model 模块下。


```



同样，可以使用sklearn库提供的API实现线性回归.代码如下：

```Python
# 利用LinearRegression实现线性回归
import numpy as np
import sklearn.linear_model as lm  # 线性模型
import sklearn.metrics as sm  # 模型性能评价模块
import matplotlib.pyplot as mp

train_x = np.array([[0.5], [0.6], [0.8], [1.1], [1.4]])  # 输入集
				#注意：输入数据必须是二维数据
train_y = np.array([5.0, 5.5, 6.0, 6.8, 7.0])  # 输出集
				#输出数据：输入数据必须是一维数据，也可以是二维数据。

# （1）创建线性回归器
model = lm.LinearRegression() 
					# y= w1x1 + w2x2 + ... + wnxn + w0
    				# 这个线性模型，不管有几个 "自变量" 它都可以接收。
        			# 此时，只有模型，并没有参数

# （2）用已知输入、输出数据集训练回归器
model.fit(train_x, train_y) #把 "数据" 交给 "模型" 进行 "训练"  
					        #此时能得到w1是多少，w0是多少......等等等
    						#此时 "模型参数" 就在 model 上面
        					#此时，就能拿模型，进行新的参数进行预测

# （3）根据训练模型预测输出
pred_y = model.predict(train_x)  #进行预测，把训练数据拿去预测

'''
	学习率 和 训练次数什么的不用我们自己设置，
	
	因为在sklearn里面并不是使用 "梯度下降法" 来求参数最优值（"极小值"）的
			//所以没有学习率 和 轮数
	但是在后面 "深度学习" 里面使用的就是 "梯度下降法"
	同理：不管使用什么样的方式，只要能把 "最优模型参数" 求出来就可以了。
	所以：上面三行解决就能解决前面所有的问题
	
	
'''


print("coef_:", model.coef_)  # 系数， 所有的w  	#训练完之后，就能拿到所有的w
print("intercept_:", model.intercept_)  # 截距  也就是w0或b #



# 可视化回归曲线
mp.figure('Linear Regression', facecolor='lightgray')
mp.title('Linear Regression', fontsize=20)
mp.xlabel('x', fontsize=14)
mp.ylabel('y', fontsize=14)
mp.tick_params(labelsize=10)
mp.grid(linestyle=':')

# 绘制样本点
mp.scatter(train_x, train_y, c='blue', alpha=0.8, s=60, label='Sample')

# 绘制拟合直线
mp.plot(train_x,  # x坐标数据
        pred_y,  # y坐标数据
        c='orangered', label='Regression')

mp.legend()
mp.show()
```

执行结果：

![linear_10](img/linear_10.png)

```javascript

人工智能里面 "很多机器模型" : 在sklearn里面都有实现

以后实现不同的模型，步骤其实都是一样的：
（1）整理输入和输出
（2）构建模型
（3）训练模型
（4）测试模型


不同模型之间：构建模型不一样。

//=============================================
model = lm.LinearRegression()
这里模型的超参数呢？
		学习率 和 轮数 ，它们都是基于 "梯度下降法" 来求极小值才有的 "超参数"
如果不是梯度下降的话：就不涉及太多超参数

它这里使用的是 "最小2乘法" ，它通过解方程得到的就是 "最优模型参数"
意味着，你能够拿到 "最优模型参数" 了。

那我们当前的模型是不是可以使用了？
		/**
			举例：比如，你喜欢一个女生，你说你给了她全部，你们就可以结婚吗？
			     肯定不可以
		*/
当前你确实已经达到了最优的一个参数了

但是能不能满足要求是另一回事

所以，还需要做专门的 "模型评估"

//======================划分 "训练集" 和 "测试集"
一般情况，拿到全部样本数据的时候，并不会把全部数据都去训练
会把数据划分成两份：
		一份叫：训练集，比较多的
        一份叫：测试集，比较少的
        它们的比例：一般是9:1、8:2、3这种

    "训练集" 不能用来测试，而 "测试集" 不能用来训练
    
    但是 "训练集"  和 "测试集" 必须是同一批数据，同一个规则的数据
    
    另外数据量不能太少，比如：只有10条、30条少量数据，就不能进行划分了，太少。
    
//====================== 另一种划分：划分成3份
     "训练集"
	 "验证集"：
     		训练一轮，验证一轮，训练一轮，验证一轮，
            训练完之后，再用整体的 "测试集" 评估一下。
	 "测试集"
	

```
 