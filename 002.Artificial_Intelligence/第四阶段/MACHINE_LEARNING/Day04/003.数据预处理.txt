

一组 "好的数据"，可以建立 "好的模型"。

一组 "不好的数据"，一定不能 "建立好"


//====================================================== 数据预处理的目的
//====================================================== 数据预处理的目的

（1）
（2）
（3）对数据的范围、量纲、格式、类型进行统一处理，更容易进行后续计算

//====================================================== 预处理方法
//====================================================== 预处理方法
	（1）标准化（也叫：均值移除）
			让样本矩阵中的  " 每一列的平均值为0" ， "标准差为1" ，
			目的：缩小列与列之间的差异？
				//以 "列" 为单位

			/**
				应用场景： 列与列之间差异太大
				如：根据  "身高" 和 "体重"，预测身体健康指数。

				身高（1.6m ~ 1.8m） 			//这个是个位
				体重（50 ~ 100kg）				//这个是10位，
				y = w1x1 + w2x2 + b
				让 模型学习，倾向于差异比较大的一边。也就是体重
				不是说 "体重" 本身 不是对健康影响就大于身高吗

				但是如果变化一下数据：

				身高（1600mm ~ 1800mm） 		//这个是千位
				体重（50 ~ 100kg）				//这个是10位，

				身高以 mm为单位，这样就化了， 

				所以 "标准化" 的目的就是要解决这个问题：
				目的：消除数据本身，"特征" 与 "特征" 差异过大的情况

				特征 与 特征 之间处理成同一个 "数值量级单位"。
				这样  "机器学习" 的结果就会 "更加准确"

			*/

			/** 具体步骤

			1. 用当前列的数据 - 当前列的平均值，得到 "离差"
					//因为每个值是：真值 + 误差 ，减去平均值就是误差，误差加一起就是0
			2. 用 "离差" / 原始数据的每列的标准差，就会变成标准差为1的情况。
					//通过数学公式可以证明，这里就不证明了。

				17   230   5600
				20   320   6800
				23   660   7200

			*/

			//============================================
			//============================================
			机器学习中有一个框架叫: sklearn
			在它里面有一个 preprocessing 库 //数据预备处接口都在这里面
			res = sp.scale(raw_sample)		//接口
			//============================================
			//============================================

	（2）范围缩放
		 将每一列的最小值和最大值设定为相同的区间
		 最小值一般设置为0
		 最大值设置为1
		/**
		特征归一化
			（1）线性函数归一化（范围缩放）
			（2）零均值归一化（均值移除）
		*/

	（3）归一化 normalize
		把数值转成占比的一个样式，
		求每一行的占比（一般求以每行占比），
		也可以求每一列的占比，

		一般对加上绝对值。



	（4）二值化
		 根据一个事先给定的阈值，用0或1 来表示特征值是否超过阈值，
		 //主要用来处理图像
		 会导致编码信息损失，是不可逆的数值转换，

	（5）独热编码
		 //数据是离散值，才能去做。
		 稀疏矩阵


	（6）标签编码
		 将"字符串"转成数值类型


