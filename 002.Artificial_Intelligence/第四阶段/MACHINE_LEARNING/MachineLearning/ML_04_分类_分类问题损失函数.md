

### 3）分类问题的损失函数

```javascript

//==============================  "分类问题" 与 "回归问题" 的评估方式不一样
//==============================  "分类问题" 与 "回归问题" 的评估方式不一样

 "分类问题" 与  "回归问题" 的评估方式一定不一样。
 
 "分类问题" 里面， "对了" 就好，"不对" 就不好。
 "回归问题" 里面是要 "越接近越好"。
 
//==============================
//==============================
（1）对于 "回归问题"，可以使用 "均方差" 作为 "损失函数"，

（2）对于 "分类问题"， 如何度量预测值与真实值之间的差异？
	 分类问题采用 "交叉熵" 作为 "损失函数"，
     //================================================
     //================================================ 
	 当只有 "两个类别" 时，损失函数表达式为：
     
```

$$
E(y, \hat{y}) = -[y \ log(\hat{y}) + (1-y)log(1-\hat{y})]
$$

```javascript

log 指对数

//=========================================
//=========================================

"交叉熵"  是度量  "两个类别"  之间的  "概率"  的差异信息的。

这个值越小越准

//========================================= 演示交叉熵函数的计算过程
//========================================= 演示交叉熵函数的计算过程
//============ 举例说明:
//============ 举例说明:
真实类别为0 ： 概率[1.0 0.0]  	 //[1.0 0.0]  表示: "是0的概率为100%"，"是1的概率为0%" 
真实类别为1 ： 概率[0.4 0.6]     //[0.4 0.6]  表示: "是0的概率为40%"， "是1的概率为60%"
//============ 那么结果：
//============ 那么结果：
真实类别0 ： 概率1.0
预测类别0 :  概率0.6  得到 0.51
        //============ 带到公式里面去
          - 1 * log(0.6) + (1-1) *log(1-0.6)
        = - 1 * log(0.6) +  0 
        = - math.log(0.6)
        = - -0.5108256237659907
        = 0.51
预测类别0 :  概率0.9  得到 0.10
        //============ 带到公式里面去
          - 1 * log(0.9) + (1-1) *log(1-0.9)
        = - 1 * log(0.9) +  0 
        = - math.log(0.9)   //这里的log是以e为底的
        = - -0.10536051565782628
        = 0.10
//======================= 说明：
//======================= 说明：
这个值越小越准
0.6 的概率是 0 时，得到0.51
0.9 的概述是 0 时，得到0.10

预测越准，"损失函数" 的值越小。


//=======================计算熵的时候
一般是以 e 为底 //2.71828
或者以2为底
或者以10为底


```

其中，y为真实值，$\hat{y}$为预测值.  

- 当$y=1$时，预测值$\hat{y}$越接近于1，$log(\hat{y})$越接近于0，损失函数值越小，表示误差越小，预测的越准确；当预测时$\hat{y}$接近于0时，$log(\hat{y})$接近于负无穷大，加上符号后误差越大，表示越不准确；
- 当$y=0$时，预测值$\hat{y}$越接近于0，$log(1-\hat{y})$越接近于0，损失函数值越小，表示误差越小，预测越准确；当预测值$\hat{y}$接近于1时，$log(1-\hat{y})$接近于负无穷大，加上符号后误差越大，表示越不准确.
