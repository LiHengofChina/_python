

（1）感知机无法解决 异或问题


（2）一层 "神经网络" 无法解决 "线性不可分"，
	 多增加几层，让模型变得 线性可分了。


	 //====================== 但是导数越多，更新参数也是问题


	 //====================== BP算法， 反向传播算法：
	 怎么更新

	 反向传播一定应用于 "有明确的求导函数" 的

	 但是隐含层里面没有 "误差"，
	 没有误差就没有 "损失函数"
	 
	 没有 "损失函数" 就没支求导，
	 
	 没法求导，就没法计算梯度
	 
	 没发计算梯度，就没梯度下降

	 没法梯度下降 ，里面的参数就没更新



	 //======================
	 BP算法， 反向传播算法：它就能够通过反向的方式方式来更新隐藏层、隐含层这些权重值。

	 但这样更新之后，它的计算速度特别的慢



//=======================================================================
//=======================================================================

"层数" 更多的 "网络"，通常具有 "更强的抽象能力"（即: "数据表征能力"）

通过 "一些特征" 来表达 "这组数据"
层次越深、越能表达这组数据，
表达能力越强，越能区分这组数据。





//======================================================================= 在自然语言处理领域
//======================================================================= 在自然语言处理领域

（1）在自然语言处理领域、模型规模越大，对人类语言理解能力越强，


（2）对人类语言理解能力越强（大模型存在 "涌现" 和 "顿悟" 现象）。




	2018年 06月 GPT1 这个模型是 1.17亿参数，
	2018年 10月 BERT-3.4  亿参数：
	2019年 02月 GPT2-15   亿参数：
	2020年 05月 GPT2-1750 亿参数：
				//1750个参数，能决定损失函数的大小，
				//y = kx + b 是两个参数，
	2023年 03月 GPT4-10000 一万多亿参数


（3）








