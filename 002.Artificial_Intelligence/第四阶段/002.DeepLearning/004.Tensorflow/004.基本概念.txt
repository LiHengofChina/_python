

//======================================= 张量
//======================================= 张量

Tensor ：多维数组或向量，张量是数据的载体，
		 包括名字、形状、数据类型等等属性

		//张量：每一个张量都有自己的名字的。
	没有其它数据类型，只有张量

//===== 张量的阶
//===== 张量的阶
	和一维数组、二维数组、三维数据是一样的
	张量里面叫阶
	数广播号的层数

//===== 张量的形状
//===== 张量的形状
	有0维：普通数字，1 2 3、字符串等等。用()表示
	1维: (5)， 1行5个元素
	2维: (2,3)，2行3列
	3维: (2,3,4)，两个3行4列的矩阵

//===== 数据类型
//===== 数据类型
	和数组一样的

tf.constant([1,2,3,4,5],dtype='int32')
tf.constant([1,2,3,4,5],dtype=tf.int32)

//===== 基本属性
//===== 基本属性
graph  #所属的默认图

op    #张量的操作名
		name   名字
		shape  形状  
		dtype  元素类型

//===== 创建张量
//===== 创建张量


tensor1d.eval()
tensor2d.eval()	# eval 只执行指定的张量


//最常用的几个


# 一般用来构建偏重的初始值
# 一般用来构建偏重的初始值

tensor1d = tf.constant(
ones 	 = tf.ones(
zeros	 = tf.zeros(


# 一般用来构建权重的初始值，
# 一般用来构建权重的初始值，
# tensornd = tf.random_normal(shape=(2,2), mean=1.8, stddev=1.0) #1.14写法
tensornd = tf.random.normal(shape=(2, 2), mean=1.8, stddev=1.0)  #2.15写法



//===== 张量的类型转换
//===== 张量的类型转换

整数转浮点、浮点转整型等等。
//===== 指定转换
string_to_number(string_tensor)
tf.to_double(x)
tf.to_float(x)

tf.to_int32(x)
tf.to_int64(x)

//===== 指定转换
//===== 指定转换
tf.cat(x,dtype)

//===== 改变张量的形状
//===== 改变张量的形状

在卷积的时候可能使用的是一维特征、二维特征。
但全连接层的时候，只能是一维特征，


所以就有	2 ——> 1		3 ——> 1 这些场景

//静态形状：在创建一个张量，初始状态的形状
tf.Tensor.get_shape():
tf.Tensor.set_shape():

		//====================================== 静态形状的区别
		//（1）不能跨阶去改变形状；1维只能到2维
		//		如：固定（3,2），就不能变了
		//		如：N,3 这个N行3列这个就没有固定
		//		如：本来是 N,3 ，现在固定成2,3是可以的。
		//（2）对于已经设置或者 "静态形状的张量"，不能再次设置静态形状。



//动态形状：在运行图时，动态形状才是真正用到的，
		"这种形状" 是一种描述原始系统架构在执行过程中的一种张量。
		//==========
		但是动态形状：可以跨阶进行设置，
		动态形状才是搭建网络、搭建模型需要用到的形状
		（1）tf.reshape(t.Tensor,shape)创建一个具有"不同动态形状的新张量"
		（2）跨纬度转换： 1D --> 2D , 1D--> 3D

//===== 张量的数学运算
//===== 张量的数学运算

#常用的5个

tf.add(x,y) 			#张量相加 
						//对应位置相加

tf.matmul(x,y)			#张量相乘
						//矩阵乘法。

tf.log(x)				#求系统架构的自然对数
						//以e为底，x的对数

tf.reduce_sum(x,axis)	#计算张量指定维度上总和
						//求每一行的和或者每一列的和
						//axis = 0，求每一列的和
						//axis = 1，求每一行的和

tf.segment_sum(data, segment_ids)	#计算张量片段总和




//======================================= 变量
//======================================= 变量


//变量 和 张量的主要区别：
//  "变量保存的数据" 可以持久化，
//  "张量"保存的数据不能持久化。		



变量在 Tensorflow中，变量是一种操作，
变量一种特殊的 "张量"，能够进行存储持久化，它的值是张量
张量不能进行持久化。

w = tf.constant(100)#张量  # 普通数据

w = tf.Variable(100)#变量  # 可以保存到文件，#主要模型参数，可以持久化

//======== 使用场景
//======== 使用场景

"模型参数" 一般是通过变量来进行保存的


//======== 定义变量
//======== 定义变量

定义变量时，需要在会话中进行初始化

tf.Variable(initial_value=None,name=None)
			//initial_value是给初始值。



//======================================= 数据流
//======================================= 数据流

数据流图：用 "节点" 和 "线"来描述数学计算

节点 ：一般用来表示数据操作。

线：表示 "节点"之间的输入输出

一量输入端的所有系统架构准备好了，节点将被 分配到各种上计算设备完成 异步并行执行运行。


//===== 这个图方便对代码和计算进行理解的


//======================================= 操作
//======================================= 操作

操作：指专门执行计算的节点，tensorflow函数 或 API定义的都是操作，常用操作包括：

	- 标题运算，向量运算，矩阵运行
	- 带状态的运算
	- 神经网络组建
	- 存储、恢复
	- 控制流
	- 队列及同步运算

	//定义当中的内容都属于操作。 tf. 什么的，都 是 "操作"





//======================================= 图
//======================================= 图

怎么程序 结构

抽你概念
"定义部分" 可以看成一个图，可以看成一个大的容器，里面都是 "操作"，存放了所有的操作

//======== 说明1：这里执行是正常的
//======== 说明1：这里执行是正常的

x = tf.constant(100.0)
y = tf.constant(200.0)
res  = tf.add(x,y)
graph = tf.get_default_graph() 
	#默认的图，上面定义的都 在默认的图上面
tf.Session(graph = graph) 

//======== 说明2：#此时执行会报错
//======== 说明2：#此时执行会报错

x = tf.constant(100.0)
y = tf.constant(200.0)
res  = tf.add(x,y)
new_graph= tf.Graph()#新图
tf.Session(graph = new_graph) 



图（Graph） 描述了计算的过程，

TensorFlow程序通常被组织䎁个 "构建阶段"  和一个 "执行阶段"，
在构建阶段，"OP的执行步骤被 描述成一个图"，
在执行除非，使用 "会话执行" 执行图中的op.

TensorFlow Python库有一个默认(default graph),op构造器可以为其增加邛。
这个默认的图对诸多程序来说已足够用了，
		//一个程序可以有一个图，也可以有很多图，
		//但一般用一个默认的图，也就够了
也可以创建新的图来描述计算过程。

在Tensorflow中， op/session/tensor都有graph属性

//==== 默认图
//==== 默认图

(default graph)

新建立的OP都在默认的图上面

//==== 新建图（了解即可）
//==== 新建图（了解即可）
new_graph = tf.Graph()
#临时将 "新的图" 设置为默认的图，增加OP
with new_graph.as_default()  #等执行完with之后， 
	new_op = tf.constant('hello world')


//======================================= 会话
//======================================= 会话

用来  "执行图" 的运算
	//所有的 操作都 在图里面

	执行 图 中的运算


会话（session）用来 执行图中的计算，
并且保存了计算系统架构对象的上下文信息。
会议的作用主要有：
- 运行图结构
- 分配资源
- 掌握资源（变量、队列、线程）

//======== 创建Session
//======== 创建Session
tf.Session() #不指定使用默认图

//======== run
//======== run
session.run(fetches,feed_dict)
		#（1）fetches 要执行哪个op，单个或多个
		# # result = sess.run(res, x, y) #结果也会有多个

		#（2）feed_dict ，是给占位符号传参数的。





"一个session" 只能执行 "一个图运算"，但是 "多个Session" 可以执行 "同一个图"
		//可以在对象创建时， 指定运行的图。
		//未指定，则使用默认的图

//======== 常见错误和原因
//======== 常见错误和原因
RuntimeError
TypeError: 	Session处无效
ValueError:	fetchaes或feed_dict的键是不合适的值
ValueError: fetches或feed_dict键无效。



//======================================= 占位符
//======================================= 占位符

占位符 是变量的占位符号，
当不能确定变量值时，可以先声明一个占位符号，真正执行时再传入变量。
类似形参

sm.accuracy(占位符)
name = placeholder(dtype,shape=None,name=None)
 