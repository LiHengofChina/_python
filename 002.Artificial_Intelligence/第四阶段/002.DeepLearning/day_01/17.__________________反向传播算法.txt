

//=============================================================================================================================== 什么是 "正向传播网络"
//=============================================================================================================================== 什么是 "正向传播网络"



（正向前馈）"上一层的输出" 作为 "下一层的输入" ，
并且 "每一层神经元"  与  "下一层神经元" 进行全连接。
的 "逻辑结构"


层数"越来越深" 的这种模型、精度越来越高。


		/**
			在输出层有 y 和 y'，真实值 和 预测值。
			但是在隐藏层，有 "输出值"，没有 "真实值"

			为什么正向不行，要反向来
			//====================目的：使用 "梯度下降" 求 "损失函数" 的 "极小值"，
			（1）先求出 "梯度" ， 
				 "梯度" 是通过 "损失函数" 的极小值，找到 "影响损失函数的那些自变量"，
				 求导它们的导数，更新它们。 

			（2）"损失函数" 又是怎么来的？
						y-y' 的差异，根据差异来设计 "损失函数"，
								回归问题：均方误差
								分类问题：交叉熵
						设计损失函数之后，针对损失函数去求导。
						求出来，哪些参数能影响我们的 "预测值"
						用梯度下降求损失函数的极小值。

		*/


			但是在深度神经网络：
					只有在 "输出层"，才有 "预测值" y' ，才有 y'-y 构建出来差异

					隐含层：是看不见的东西，没有误差、没有损失函数、没办法求导。

			//================================
			//就像之前的线性回归，y=kx+b，直接到了输出层，直接作用于预测值，所以可以求导，它是浅层模型

			//而深度神经网络 ，有多个隐藏层，这些隐藏层并没有得到输出结果，而是将计算结果交给下一层计算。
			//到了输出层，才有最终结果，才有误差。

			//================================ 所以说
			隐含层没法梯度下降，输出层才可以




//=============================================================================================== 什么是反向传播
//=============================================================================================== 什么是反向传播


反向传播（Backpropagation algorithm）（BP）全称 "误差反向传播"，

是在 "深度神经网络" 中，根据 "输出层输出值"，来 "反向调整-隐藏层权重" 的一种方法。


		x ---> h1 ---> h2 ---> y'

			"输出层" 有y' 这里有 "误差"， 它是根据h2算出来的，
					所以可以把 "误差传给" 传给 "h2"， 这样"h2"也就有了 "误差"，
					h2 是通过 h1传出来的，再把误差传到h1里面去。
					同样的方式再传给 h1

		"正向" 一步一步算出来的，所以我们也可以 "反向" 着推

		//================================ 反向传播 传播的是什么
		//================================ 反向传播 传播的是什么
		反向传播 "误差"。

		将输出层的误差 ，反向传播到隐含层中。

		有了误差，就可以求 "每一个权重"  和 "每一个误差" 它的梯度了。


 




