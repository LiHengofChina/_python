
前面这些就是通过正向传播，算出它的极小值的过程


现在开始反向传播： "反向传播的误差"

//============================================================================ 求w5的梯度（简单的情况）
//============================================================================ 求w5的梯度（简单的情况）


//============================ 前情提示
//============================ 前情提示

（1）计算预测值
		o1值的由来 （正向计算）
			out h1(0.593269992)---w5(0.40)----->
												+ (b2(0.6) * 1) -----> ( net o1 -> out o1(0.75136507))  //0.01
			out h2(0.596884378)---w6(0.45)----->

		同理：o2值的由来 （正向计算）


（2）有真值 和 预测值，
		根据 "输出值" 和 "预测值" 构建 "损失函数"
					损失函数为：Σ 1/2(target - output)^2   

		两部分损失函数加起来：
		E_total = E_o1 + E_o2


//============================ 现在要求w5的偏导（梯度）
//============================ 现在要求w5的偏导（梯度）


w5 并没有直接作用于 "损失函数Σ 1/2(target - output)^2  "
它是 w5 算出 net o1,
net o1 算出来 out o1，
out o1 才作用于损失函数。


所以：
	要用 损失函数求  out o1
	out o1 求 net o1
	net o1 求 w5


所以要求的是：
		"总的损失函数（E_total = E_o1 + E_o2）" 针对于 "w5" 的梯度


但是 "总的损失函数" 和 "w5" 有关系，
因为 总的操作函数 是  E_o1 + E_o2 两部分的，
而 w5 只参与了E_o1的计算，没有参与E_o2计算

所以只需要通过 E_o1 的误差去算w5，

E_o1 针对 w5就是总的误差
//=======

所以 
 "E_o1 关于 out o1 的偏导" *  "out o1 对 net o1  的偏导" * " net o1 对w5 的偏导"
			//这就是链式求导，这个式子约分，就得到  ∂E_o1/∂w5 ，E_o1关于 w5的偏导。


			它得到就是的  "损失函数Σ 1/2(target - output)^2  " 针对 w5的偏导。
			直接求，求不了，所以分三步 乘到一起。


//============
带入数字计算得到 ：	0.082167041 ，这也就w5的梯度

//============================================== 梯度更新
//============================================== 梯度更新

有了梯度，现在就可以更新 w5 的值了。

	   w5 = w5 -  η * (∂E_o1/∂w5)
		  = 0.4 -  0.5 * 0.082167041
		  = 0.35891648


所以w5 初始化 0.4  ，更新一次后变成了 0.35891648

同理：w6、w7、w8都可以求了。
// W5 和 w6 用 E_o1算
// W7 和 w8 用 E_o2算



W5 和 w6 , W7 和 w8 更新相对比较简单，
你越往前面，更新越复杂，难度也越大。

//============================================================================ 求w1的梯度（复杂情况）
//============================================================================ 求w1的梯度（复杂情况）

w1、w2、w3、w4 求梯度相对 复杂：


//============== 求w1的梯度
//============== 求w1的梯度

从正向推：
	w1 作用于 net h1,
	net h1作用于out h1, 


	out h1作用于net o1，net o1作用于out o1，out o1作用于 损失函数E_o1
	同时，
	out h1作用于net o2，net o2作用于out o2，out o2作用于 损失函数E_o2

			//===这里就有点区别了，就是说 out h1 参与了 "两个误差的计算"
			//神经元越往前，前面链接的越多，加的也就越多
			//所以反向传播算法：神经元比较多的话，也是比较复杂的。

	所以：现在的  "总的损失函数" 是：E_total = E_o1 + E_o2

	"总的损失函数 E_total = E_o1 + E_o2"  针对 "w1 的偏导"，

	∂E_total / ∂w5 = Σ(∂E_total/∂out_o * ∂out_o/∂net_o * ∂net_o/∂out_h1)  *  (∂out_h1/∂net_h1) * (∂net_h1/∂w1)
													//注意：out_h1 参与了 "两个误差的计算"

				 =(
					(∂E_total/∂out_o1 * ∂out_o1/∂net_o1 * ∂net_o1/∂out_h1) 
					+
					(∂E_total/∂out_o2 * ∂out_o2/∂net_o2 * ∂net_o2/∂out_h1)
				   )
				   *  (∂out_h1/∂net_h1) * (∂net_h1/∂w1)

	//===这就是 "总的损失函值" 针对 "w1的偏导"

	 所以过程是："总的损失函数" 针对 "激活函数" 求导
				 "激活函数" 	针对 "out_o" 	求导
				 "out_o" 		针对 "net_o" 	求导
				 "net_o" 		针对 "out_h" 	求导
				 "out_h" 		针对 "net_h" 	求导
				 "net_h" 		针对 "w1"   	求导

	//================================

	W1 = W1 - (η *  ∂E_total / ∂w5 ) = 0.149780716



	同理可以算出w2、w3、w4 的偏导

 