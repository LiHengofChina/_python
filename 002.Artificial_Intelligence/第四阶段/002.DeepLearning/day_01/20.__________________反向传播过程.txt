



//==================== 写法一：之前的表示法：
//==================== 写法一：之前的表示法：


	  (1)	//(1)表示第一层
	W
	  1 2

	  // 2表示前一层第2个
	  // 1表示后一层第1个


//==================== 写法二：
//==================== 写法二：


//第一层
W
 (x1) 1

W
 (x1) 2
 
W
 (x1) 3




//第一层
W
 1 4

W
 2 4

W
 3 4

这种是挨着排号的。





//============================================================================ 正向传播
//============================================================================ 正向传播


从 "输入" 到 "第一层" ===> "第二层" ===> "输出层"  
			//当 我们计算到最后的 "输出层" 了，
			//我们就可以得到Y，也就是 "预测值"。


			"预测值" 叫 y
			真实值是 z 

//============================================================================ 反向传播： δ 读着 delta
//============================================================================ 反向传播： δ 读着 delta

			在最后一层根据 y 和 z 的差异，计算出来这里的 "误差是多少" 

			计算出了 "误差δ" 之后，进行反向传播

			//=================================== 最后一层

			f4(e)		------w46--------->
											f6(e)  δ
			f5(e)		------w56--------->



			因为：
				"最后一层的误差δ"是: "f4(e)神经元" * "自己的权重w46"  +   "f5(e)神经元" * "自己的权重w56" 算出来（正向计算）的值得到的。
			所以：
				（1）我们可以把 f6(e) 算出来的 "误差δ" 反向传播给 f4(e)
				"误差δ"  *  w46  : 得到  "f4(e)的误差δ4"

				（2）我们可以把 f6(e) 算出来的 "误差δ" 反向传播给 f5(e)
				"误差δ"  *  w56  : 得到  "f5(e)的误差δ5"

			//=================================== 到第二层（往前推一层）
			
			//============神经元f4(e)
		   "神经元f4(e)"，正向传播时，谁计算了F4? 
			f1(e)		------w14--------->
			
			f2(e)		------w24---------> f4(e)  δ4
			
			f3(e)		------w34--------->
			
			//============神经元f5(e)
		   "神经元f5(e)"，正向传播时，谁计算了F5? 
			f1(e)		------w15--------->
			
			f2(e)		------w25---------> f5(e)  δ5
			
			f3(e)		------w35--------->


			现在要求 f1(e) 的误差，

						f1(e) 参与了： f4(e) 和 f5(e) 的计算
						所以: "f4(e)的误差δ4"  和  "f5(e)的误差δ5"  都和  f1(e) 有关系



						（1）所以在计算 "F1误差δ1" 的时候，使用:
							"f4(e)的δ4"*w14 + "f5(e)误差"*w15  = "f1(e)的误差δ1"

						（2）所以在计算 "F2误差δ2" 的时候，使用:
							"f4(e)的δ4"*w24 + "f5(e)误差"*w25  = "f2(e)的误差δ2"

						（3）所以在计算 "F3误差δ3" 的时候，使用:
							"f4(e)的δ4"*w34 + "f5(e)误差"*w35  = "f3(e)的误差δ3"

//============================================================================ 求第一层的梯度
//============================================================================ 求第一层的梯度
 


	到这里，每个 "神经元" 上面都有 "误差" 了。

	"f1(e)的误差δ1" ，

	//==============（1）
	从正向来说："f1(e)的误差δ1" 

	x1 --------w(x1)1------------->
									F1(e)
	x2 --------w(x2)1------------->


	f1(e) 是 通过：x1  、w(x1)1 、x2、 w(x2)1 算出来的
	既然现在有了 "f1(e)的误差值δ1"，
	而 x1 和 x2 是输入值，
	那我们就可以更新：w(x1)1 和 w(x2)1 的值了。
	这样就求出了这 "两个边的梯度" 了，

	//==============（2）
	同理：可以算出x1、x2和f2(e)两条边的梯度

	//==============（3）
	同理：可以算出x1、x2和f3(e)两条边的梯度

//============================================================================ 求第二层的梯度
//============================================================================ 求第二层的梯度


同理：可以算出 第二层 和 输出层的 梯度


现在 "每条边权重的梯度" 就都能算出来了。


//========================================================================================================================== 其它
//========================================================================================================================== 其它





