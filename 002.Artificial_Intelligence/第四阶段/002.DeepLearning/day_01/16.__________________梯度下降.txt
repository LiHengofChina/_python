


通过损失函数，我们将 "寻找最优参数" 问题，转换为了 "寻找损失函数，最小值" 问题。


（1）损失是否足够小？如果不是，计算损失函数的梯度。
（2）按梯度的反方向走一小步，以缩小损失。
（3）循环到（1）。


//============================================================
//============================================================
这种按照 "负梯度不停地调整函数权值" 的过程就叫作 "梯度下降法"。

通过这样的方法：
改变 "每个神经元" 与 "其他神经元的连接权重" 及 "自身的偏置" ，
让 "损失函数" 的值 "下降得更快" ，进而将值收敛到 "损失函数的某个极小值" 。




//========================================================================================= 导数
//========================================================================================= 导数

所谓 "导数"，就是用来分析函数 "变化率" 的一种 "度量"。

//======================= 其公式为：
//======================= 其公式为：

	f'(x0) = Δy / Δx
		让Δx无限趋近于0的过程。

		"用变化的y值" 除以 "变化的x值"，让 Δx无限趋近于0 
		让变化的x值（即Δx无限趋近于0 ）


f'(x0) 表示函数 f(x) 在 x0 处的导数

//========================= 示例：
//========================= 示例：

//============================= y = x^2 +1	 U形图
//============================= y = x^2 +1   U形图

有两个点 (x0,y0)、(x1,y1)
	在 y = x^2 +1 上面

现在要求x0的导数：
	"变化的y值" / "变化的x值"

	变化的y值 = y1-y0
	变化的y值 = x1-x0

	(y1-y0) / (x1-x0)

	"红色的线" 会不断退化，到"绿色的线"

	//================== 假设1：	//y是通过x处出来 的
	(x0,y0)为：x0= 1 		y0=2 
	(x1,y1)为：x1= 1.0001   y1=2.00020001


	Δy / Δx =  ( y1 - y0 ) 					/  	( x1 	- x0 )
			=  ( 2.00020001	 - 2 )    		/  	(1.0001 - 1)
			≈ 0.0002 / 0.0001 
			= 2


	//================== 假设2：
	(x0,y0)为：x0= 2 		y0=5 
	(x1,y1)为：x1= 2.0001   y1=5.00040001


	Δy / Δx =  ( y1 - y0 ) 					/  	( x1 	- x0 )
			=  ( 5.00040001	 - 2 )    		/  	(2.0001 - 1)
			≈ 0.0004 / 0.0001
			= 4


//============================= 求导方法：（根据 "幂的法则" 求导）：求导的一种
//============================= 求导方法：（根据 "幂的法则" 求导）：求导的一种
 
y =  x^2 +1

//===========（1）先求 "导函数"

（1）对 x^2 求导：指数乘以系数，再降次。导数为 2*x ，也就是2*1=2

（2）对常数项 1 求导： "常数项的导数为零"。

（3）得到：f'(x)=2*x 是  f(x)=x^2+1 的 "导函数"

//===========（2）求导数

假设 x=1 , 则导数 2*1 得到 2
假设 x=2 , 则导数 2*2 得到 4
假设 x=3 , 则导数 2*3 得到 6
假设 x=4 , 则导数 2*4 得到 8


//========================================================================================= 偏导
//========================================================================================= 偏导

"偏导" 的英文本意是 "partial derivatives"（表示局部导数）

对于 "多维变量函数" 而言，当 "求某个变量的导数" 时，就是把 "其他变量" 视为 "常量"

然后对整个函数求其导数
（相比于全部变量，这里只求一个变量，即为 "局部"）。

//============================= 求偏导
//============================= 求偏导

f = x^2 + 3xy + y^2 + z^3


//===========（1）先求 "导函数"
（1）求 x 偏导数公式
	f = x^2 + 3xy + y^2 + z^3
	  = x^2 + 3xy
	  = 2 * x^1 + 3 * x^0 * y 
	  = 2x + 3y

（2）求 y 偏导数公式
	f = x^2 + 3xy + y^2 + z^3
	  = 3xy + y^2
	  = 3*x*y^0 +2*y^1
	  = 3x + 2y

（3）求 z 偏导数公式
	f = x^2 + 3xy + y^2 + z^3
	  = z^3
	  = 3 * z^2

//===========（2）求导数
（1）求 x 偏导数的值，假设 x=1, y=2, z=3
   f_x = 2x + 3y
      = 2(1) + 3(2)
      = 2 + 6
      = 8

（2）求 y 偏导数的值，假设 x=1, y=2, z=3
   f_y = 3x + 2y
      = 3(1) + 2(2)
      = 3 + 4
      = 7

（3）求 z 偏导数的值，假设 x=1, y=2, z=3
   f_z = 3 * z^2
      = 3 * 3^2
      = 3 * 9
      = 27




//========================================================================================= 学习率
//========================================================================================= 学习率



"学习率" 是 "梯度下降" 过程中，在梯度值前面的系数，用来控制调整的 "步幅大小"


//========================================================================================= 梯度递减训练法则
//========================================================================================= 梯度递减训练法则


"神经网络" 中的 "权值参数是非常多的"，因此针对 "损失函数E的权值" 向量的 "梯度如以下公式所示"：

就是参数更新公司

wi = wi - 学习率 * 偏导数







