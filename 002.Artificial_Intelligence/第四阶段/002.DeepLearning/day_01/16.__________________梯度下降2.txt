




//============================================================ 梯度下降算法
//============================================================ 梯度下降算法

（1）BGD
（原始版本）批量梯度下降：
			批量梯度下降法（Batch Gradient Descent，BGD）是最原始的形式，
			它是指在每一次迭代时使用所有样本来进行梯度的更新。

			优点：对所有样本进行计算，实现了并行
			缺点：训练过程慢



（2）SGD
随机梯度下降
			随机梯度下降法（Stochastic Gradient Descent，SGD）每次迭代使用一个样本来对参数进行更新，
			使得训练速度加快。

			优点：样每一轮参数的更新速度大大加快
			缺点：准确度下降。


（3）MBGD（批次大小会决定模型精度）
小批量梯度下降 ：每次迭代 使用指定个（batch_size）样本来对参数进行更新。

			优点：通过矩阵运算，每次在一个batch上优化神经网络参数并不会比单个数据慢太多。
			缺点：batch_size的不当选择可能会带来一些问题。



			20000 个样本
			batch_size = 100 ：每次100个样本

			for i in range(100): 训练100轮
				for j in range(200): 做200个次，每次100个，那每轮就有20000个样本了。





//============================================================ 几种梯度下降算法收敛比较
//============================================================ 几种梯度下降算法收敛比较


（1）批量梯度下降稳健地向着最低点前进的
（2）随机梯度下降震荡明显，但总体上向最低点逼近
（3）小批量梯度下降位于两者之间
