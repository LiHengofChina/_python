

把 "线性模型" 提升成 "非线性模型"

	因为现实生活，
	绝大部分不是简单的线性问题

//================================================= 神经元
//================================================= 神经元

神经元之前叫感知机，
后来叫神经元


//================================================= 激活函数
//================================================= 激活函数

神经网络拓展成非线性模型：

	原来能画一条线，我加一个激活函数之后，它就能画一条曲线了
	原来能画一个面，我加一个激活函数之后，它就能画一个曲面了
	
	如果能车 一个曲线 或 一个曲面，它的的分类能力就变得更强

	所以激活函数实际上是提升了神经网络的能力的

（1）阶跃函数： 0 1
		有断点，不可导

（2）sigmoid ，也叫逻辑函数
		是阶跃函数的近似函数。
		每一点都可导的

		(0,1)
	 //===================
	 //但是它容易梯度消失
	 //所以在图像任务中，它使用的是relu


（3）tanH  
		(-1,1)
		//自然语言处理用的多

（4）softmax，是输出层使用

		//把每一个神经元输出的值，
		//通过softmax转换成0~1相对概率，所有概率之和是100%
		//主要用在分类问题



//=================================================
//=================================================
神经网络属于有监督学习：

//================================================= 损失函数：
//================================================= 损失函数：

均方误差：回归问题常用的损失函数
		"预测值 - 真实值" 求和  除n


梯度下降：
		度量概率之前的接近程序的

		k的真实概率 *log 预测概率 ，求和，取相反数。

//================================================= 梯度
//================================================= 梯度

//================================================= 导数
//================================================= 导数

//================================================= 偏导
//================================================= 偏导

//================================================= 反向传播算法
//================================================= 反向传播算法



//================================================= 梯度消失
//================================================= 梯度消失

//================================================= 卷积
//================================================= 卷积
提取特征
卷积 主要作用是发现特征、


卷积核：权重、过滤器、提取发现特征。




//================================================= 激活
//================================================= 激活
作一个非线性转换

//================================================= 池化
//================================================= 池化
降维

//================================================= CNN
//================================================= CNN




//================================================= RNN
//================================================= RNN
和CNN是表兄妹关系

声音：
文字 


//================================================= 经典CNN介绍
//================================================= 经典CNN介绍

（1）LeNet 奠定了卷积神经网络的基础。

（2）AlexNet: 集大成者。

（3）VGG 		反复卷积
（4）GoogleNet  并行卷积，残差网络，
				就像声音传播越远就越小
				把输入叠加到输出上面，就相当于在中间计算过程中，
				对数据进行了广大，有效解决了梯度消失的问题，就可以做得很深







