

//======================================== 张量
//======================================== 张量

（1）和tensorflow一样的


//======================================== LoDTensor
//======================================== LoDTensor

（1）Lod(Level-of-Detail) Tensor 是Paddle的高级特性，
	 //Level-of-Detail指细节的级别。它是一个高级张量
	 是对Tensor的一种扩充。
	 LoDTensor通过牺牲灵活性来提升训练的效率

（2）LoDTensor用来处理 "变长数据信息" ，将长度不一致的维度拼接为一个大的维度，
	 并引入了一个索引数据结构(LoD)来给张量分割成序列。
			//现在用不到，因为我们现在的数据都是定长的
			//NLP里面长度不等，所以会用到。


		//================================ 它其实就是在数据上面中上了索引。
		假设一个mini-batch中有3个句子，
		每个句子中分别包含3个、1个和2个单词，
		我们可以用 （ 3  + 1 + 2 ）xD维Tensor加上一些索引信息来表示这个mini-batch:
			3     1     2 
			|||   |  	||

		//================================
		假设存在一个mini-batch中包含3个句子、1个句子、和2个句子的文章，
		每个句子都由不同数量的单词组成，
		则这个mini-batch的可以表示为2-Level的LoDTensor:
			3     			  1    2
			3	  2	   4	  1	   2	3
			|||   ||   ||||   |	   ||	|||

//======================================== Layer
//======================================== Layer
Layer 表示一个独立的计算逻辑， //它其实就op
	  通常包含一个或多个operator（操作），
	  如: 
		layers.relu表示ReLU计算；
		layers.pool2d表示pool操作。 //2维卷积
		Layer的输入 和输出为 Variable


		//独立的计算逻辑都 在 layer 模块上面。
		//layer 在fluid模块上面

//======================================== Variable
//======================================== Variable

	Variable 表示一个变量，在paddle中，Variable基本等价于Tensor，
	Variable 进入 Layer计算，然后Layer返回Variable
	



	如：
		fluid.layers.fill_constant	//常量类型变量
		fluid.layers.data  			//占位符类型变量

		// paddlepaddle 里面 变量和张量没有太多区分。
		// paddlepaddle 里面所有数据都是变量，变量里面又分为
				（1）模型中可学习参数：权重、偏执	//tf.Variabls
						fluid.layers.create.parameter	
				（2）占位符Variabls		//tf.placeHold
				（3）常量 Variabls		//tf.Constant


//======================================== Program
//======================================== Program
Program包含Variable定义的多个变量和Layer定义的多个计算，
是一套完整的计算逻辑，
从用户角度来看：Program 是顺序的、完整的执行的。

Program 的作用是存储网络结构，但不 "存储参数"

	//模型 = 模型结构  + 参数

	//但这里 只保存结构，不保存参数

	exe 执行 Program 的结果就是：模型参数 //参数保存在Scop里面

	模型参数参数不保存在program里面。

	//================ 最常用的 Program 
	//================ 最常用的 Program 
	fluid.default_main_program
			//前身反向计算、
			//以及模型参数更新、优化器参数更新等各种操作
	fluid.default_startup_program
			//定义了模型参数初始化、
			//优化参数初始化，
			//reader初始化等各种操作。
			//该program可以由框架自动生成。
			它其实就是一个参数初始化


//======================================== Scope
//======================================== Scope
	Scope在paddle可以看作 变量空间，存储fluid创建的变量，
	//变量存储于unordered_map数据结构中，

	//该结构类似于python中的dict，
	// "键" 是变量的名字，"值" 是变量的指针。
	//==========================================
	一个paddler有一个默认的全局scope
			//可以通过 fluid.global_scope()获取。
			//如果没有主动创建scope并且通过 fluid.scope_guard()替换当前的scope，
			//那么所有参数都在全局的scope中，
			//参数创建的时机不是在组网时，而是在executor.run()执行时。
			就是说在运行时，这些参数才会被创建出来

	program和 scope 配合，才能表达完整的模型。

	模型 = 网络 结构 + 参数
	//==================== scope 的位置 
	//==================== scope 的位置 
	scope 在执行器上面，
	所以保存模型的时候，保存exe，执行器就可以了。 


//======================================== Executor
//======================================== Executor

// 它就是 "执行器" 。
用来接收并执行Program，
和tensorflow的 Session一样。


//======================================== Place
//======================================== Place

指定 在哪个设备上面运行：

place =fluid.CPUPlace() 	#指定CPU执行
place =fluid.CUDAPlace(0)	#指定GPU执行 0表示设备编号

//======================================== Optimizer
//======================================== Optimizer

优化器，用于优化网络，一般用来对损失函数做梯度下降优化，
从而求得最小操作值。
