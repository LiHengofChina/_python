

（1）
	artificial 			intelligence
	读：阿尔提（费学）	赢特ne（jin ce）

（2）两大方向
	1） 自然语言处理
	2） 图像处理

（2）两大技术
	1） 机器学习
	2） 深度学习

（4）Python中的几种类型
	1）列表
	2）元组
	3）字典

（5）numpy
	 numpy 是python的一个第三方库
	 供了高效的 "多维数组对象（ndarray）"
	 知识点：
		（1）创建 np.array
		（2）数组 与 值 进行计算
		（3）广播机制
		（4）对象操作，数据切片等等

（6）pandas
	 pandas它是为了数据分析而创建的
	 核心两种结构：
		 Series（一维）
		 DataFrame（二维表格）
	 pandas 是为了补充python当中的数值计算能力

（7）matplotlib
	matplotlib 也是一个python中的第三方库
	通过绘图了解机器学习的情况

（8）机器学习分类
		监督学习
		无监督学习

（9）人工智能与传统软件的区别
	传统软件：
		执行人的指令和想法，在执行之前人已经有了解决方案，
		无法超越 "人的思想" 和 "认识范围"。

	人工智能：
		尝试突破 "人的思想" 和 "认识范围"，
		让计算机学习到 "新的能力"，尝试解决 "传统软件的难题"。

（10）什么是机器学习
		如果一个系统，能够通过执 "行某个过程" ，就此 "改进了它的性能"，那么这个过程就是学习.由此可看出，学习的目的就是 "改善性能".
		赋予机器  "学习的能力"
		上学就是在学习。



（12）机器学习的形式：

	1） 建模问题
		 就是要找到一个 "算法" 、找到一个 "模型" ，用这个 “模型” 进行预测，

		 找一个 "接受特定输入X"，并给出 "预期输出Y" 功能函数f
		 //说白了就是找到一个公式

		 "函数" 以及 "确定函数的参数" 被称为 "模型".
	2） 评估问题
		根据 "误差" 进行判定函数的 "优劣" 
		
	3） 优化问题
		反复训练预测。
		优化 "模型" ，改善 "性能"
		
（13）机器学习的基本问题
	（1）确定是什么问题	
			"不同的问题" ，选择 "不同的模型" 来解决问题

	（2）数据分类 数据分两类
		（1）连续数据:  在某个 "区间范围内 任意的值" 都可能出现的 "情况" 就叫 "连续数据"
				典型的代表就是 "钱"，我们班同学今天吃饭花了多少钱? 0-200

		（2）离散数据：只有几个 "可选值" 的这种情况
				男 女 男 女 男  女  女 男  女 男 男 男 女  男 女 男 男 ......

（14）机器学习的分类（重点）
		有监督学习
			（1）就是当 "训练数据" 当中 "有输出数据" 的情况，就是 "有监督学习"
					"正确答案" 在 "监督" 你的预测
			（2）对方也不知道对不对
					这就叫 "无监督" ， 无法通过 "正确答案" 调整和改进。
			（3）"半监督"  就是先监督 ，后不监督，或者先不监督，后"监督"	
		无监督学习
			就是当 "训练数据" 当中 "没有正确答案" 的情况，就是 "无监督学习"
			没有正确答案，但是有好坏，
		半监督
			先通过无监督学习划分类别，再人工标记通过有监督学习方式来预测输出.例如先对相似的水果进行
		
（15）批量学习、增量学习

		（1）批量学习
			 将 "学习过程" 和 "应用过程" 分开， 预测、循环
			 有点像 "闭关修练"
		（2）增量学习
			 边训练、边预测


（16）基本问题分类
	（1）回归问题（ 这是"有监督学习"，有对错，在有监督的基础之上得到 "连续的输出" ）
			.例如：
			- 根据房屋面积、地段、修建年代以及其它条件预测房屋价格  //"价格"指钱，所以说是 "连续数据"
			- 根据各种外部条件预测某支股票的价格		//又是价格，还是"钱"，连续数据
			- 根据农业、气象等数据预测粮食收成		 //收成，以吨为单位，1吨，2吨。。。，所以是连续数据
			- 计算两个人脸的相似度				 //相似度 0 ~ 1 连续数据
	（2）分类问题
			- 手写体识别（10个类别分类问题）//0-9 ，10个数字，其实是想得到，写的到底是1，还是2，所以是分类问题
									   //结果，只会在这3个可 选值之一
			- 水果、鲜花、动物识别		//结果只能是这三个之一。 离散值
			- 工业产品瑕疵检测（良品、次品二分类问题） //不是好的，就是坏的。 离散值
			- 识别一个句子表达的情绪（正面、负面、中性）// 离散值

	（3）聚类问题（无"监督"，只有好坏，没有对错）
			/**
				只有输入，没有输出，
			*/
			- 根据 "一批麦粒" 的数据，判断哪些属于 "同一个品种" 				//聚类，分成一类，
			- 根据客户在电商网站的浏览和购买历史，判断哪些客户对某件商品感兴趣	 //聚类，
			- 判断哪些客户具有更高的相似度								 //	

	（4） 降维问题（主要是从数学角度变化数据）

（17）机器学习的一般过程
	  //"机器学习"代码量不大，因为有对应的框架封装了接口
	  //"深度学习"的代码量动不动就2000行，比较大
	 （1）数据收集，手段如手工采集、设备自动化采集、爬虫等
	 （2）数据清洗
	 （3）选择模型（算法）
	 （4）训练模型
	 （5）模型评估
	 （6）测试模型
	 （7）应用模型
	 （8）模型维护

（18）数据预处理的目的
	一组 "好的数据"，可以建立 "好的模型"。
	一组 "不好的数据"，一定不能建立 "好的模型"

（19）预处理方法
		一组 "好的数据"，可以建立 "好的模型"。
		一组 "不好的数据"，一定不能 "建立好"

	（1）预处理方法

	（2）标准化
		让样本矩阵中的  " 每一列的平均值为0" ， "标准差为1" ，
		目的：消除数据本身，"特征" 与 "特征" 差异过大的情况
		身高（1.6m ~ 1.8m） 			//这个是个位
		体重（50 ~ 100kg）				//这个是10位，

	（2）范围缩放：将每一列的 "最小值" 和 "最大值" 设定为  "相同的区间"
		 将每一列的 "最小值" 和 "最大值" 设定为  "相同的区间"
		 最小值一般设置为0，最大值设置为1。

	（3）归一化
		// 这是另一种归一化方法，通常称为L1和L2范数归一化。
		# l1:  l1范数，除以向量中 "各元素绝对值之和"
		# l2:  l2范数，除以向量中 "各元素平方之和"
	（4）二值化
		//根据一个事先给定的阈值，用0或1 来表示特征值是否超过阈值，

	（5）独热编码
		//想让数据可逆，不丢失信息细节，这里就可以使用 "独热编码"

	（6）标签编码
		将"字符串"转成数值类型 "针对离散值去做的"


（21）线性模型
		用一条直线（或一个平面）去拟合数据。
		线性模型定义：y = w_1x_1 + w_2x_2 + w_3x_3 + ... + w_nx_n + b
		// x 是自变量
		// y 是因变量
		// w 是权重，每个 自变量 都有一个权重，权重越高，影响越大。

		"线性模型" 形式简单、易于建模，却蕴含着 "机器学习"中一些重要的 "基本思想".
		许多 "功能强大" 的 "非线性模型" 可以在 "线性模型" 基础上 "引入层级结构" 或 "高维映射"而得. 
		此外，由于$w$直观表达了各属性在预测中的重要性，因此线性模型具有很好的可解释性.例如，判断一个西瓜是否为好瓜，可以用如下表达式来判断：

（22）矩阵
		矩阵 是二维的，有行有列的值
		1 2		1 2
		3 4		3 4
		a		b

（23）矩阵矩阵 与 矩阵  相乘
		"a的所有行" 乘以 "b的所有列"
		"对应位置" 相乘之后 "再相加（也就是内积）"
		注意：（1）并不是  "所有矩阵" 都能相乘 //"A的列数" 和 "B的行数" 相等才能相乘
			  （2）没有 乘法交换律		//a * b  != b * a
			  
			  
（24）矩阵相乘 和线性回归有什么关系

	 线性模型 写成向量的形式， y= x*w^T +b    //X乘以W的T次方  + b
	
（25） "矩阵相乘" 在 "线性回归" 中起着 "核心作用"，
    因为它提供了：一种 "计算效率高" 且 "表达简洁的方式" 来描述和计算线性回归模型。

//==================================================================================================
（26）模型训练
	在 "二维平面" 中，"给定两点" 可以确定 "一条直线.
	但在实际工程中，可能有 "很多个样本点" ，无法找到 "一条直线精确穿过所有样本点"，
	只能找到一条与样本"足够接近" 或 "距离足够小" 的直线，近似 "拟合" 
	
	我们的目的是让 "这条直线" 到达 "所有样本点" 的  "距离" 达到最小值

	//==================================================
	//==================================================
	怎么表达："每一个样本" 到 "这条直线" 的 "距离" 呢？
	// "真实值"  和 "预测值" 之间的差异，
	// 就是 "真实值" -  "预测值"  = 距离
	//==================================================
	//==================================================
	怎么让 "所有距离" 都  "同时达到"  最小的值？
	// 其实就是： 针对 "所有样本它的真实值" 到 "模型预测值" 的距离，

	//现 在就可以把每一个样本点，它们的 "真实值" 和 "预测值"  的 "差异" ，
	//加到一起，求一个平均值：
		∑(Yi - Yi')^2 / n
			//∑(Yi - Yi')表示所有点的误差加到一起
			// "平方"是为了防止负数,也可以使用绝对值，
			//  除以 n，是为了求平均值，
			// 这里为什么使用平方，因为 "平方它的曲线" 的最低点，
			// 是可V的。因为后面要对它进行求导

	//	(Yi - Yi') 的平方 求和 然后除以n
	//它表达的含义，就是 "当前这个模型" 距离 "每一个样本点" 的 "平均距离"
	//让它达到 "极小值"，就是 "模型" 针对所有样本预测的 "最准的状态"。

	∑(Yi - Yi')^2 / n 
	我们就是为了找到 "最优的" 模型参数，让这个参数最准，
	"寻找最优模型参数" 的问题，转换成了: 求 "这个公式" 极小值的问题

（28）损失函数
	 ∑(Yi - Yi')^2 / n  这种函数叫：损失函数

	如何确定 "直线到所有样本" 足够近呢？ 可以使用 "损失函数" 来进行度量

	损失函数用来度量 "真实值（由样本中给出）" 和 "预测值（由模型算出）" 之间的差异.
	损失函数值越小，表明模型预测值和真实值之间差异越小，模型性能越好；
	损失函数值越大，模型预测值和真实值之间差异越大，模型性能越差.

（29）回归问题，常用的损失函数是 "均方差" 公式。
	/**
		 “损失函数”是机器学习中的一个 “广义概念”，用于衡量 “模型的预测值” 与 “真实值” 之间的 “不一致程度”。
		 而“均方差函数”（Mean Squared Error, MSE）是一种 “具体的损失函数” ，通常用于 “回归问题”。
		 在回归问题中，"均方差" 是常用的损失函数
	*/
	 其中，y为模型 "预测值"，y' 为真实值. 
	 "均方差" 具有非常好的几何意义，对应着常用的 "欧几里得距离"（简称欧式距离）. 

	 "线性回归" 的任务是要 "寻找" 最优 "线性模型"，
				 是的 "损失函数" 值最小， 

（30）不断利用损失函数的梯度（导数）来更新参数，使损失变小。
	  //损失函数是一个函数，求它的导函数的值，来更新参数，使损失变小

（31）求均方误差（MSE）极小值常见有两种方式：
		① 最小二乘法（解析解 / 正规方程）
		② 梯度下降法（迭代优化）


（27）超参数
	 learning_rate = 0.01 # 超参数：学习率，不要设置太大，大大容易放大。
	 epoch = 200     	 # 超参数：学习轮数，50能不能达到极小值，100轮能不能达到极小值，


（20）机器学习 sklearn，也是一个库，也需要安装

	  python -m pip install scikit-learn
	  python -m pip install pandas

	  sklearn 框架接口


（21）模型评估
	 import sklearn.metrics as sm       # 评估模块
	 
	 coef_： [9449.96232146]
	 intercept_： 25792.20019866871
	 平均绝对误差-MAE： 4587.366522327396
	 均方误差-MSE： 29784216.41962167
	 中位数绝对偏差-MAD： 4895.445366109856
	 r2_score： 0.9645484959659238



（23）正则化的定义
	正则化是指，在 "目标函数后面" 添加一个 "范数" ，来 "防止过拟合" 的手段。


（22）线性回归模型变种
	 在 "线性模型" 的基础之上，做了一些 "稍微的修改" 。

	 Lasso 回归和岭回归（Ridge Regression）都是在标准线性回归的基础上修改了损失函数的回归算法.

（23） 以 "岭回归" 为例，所有系数，
		"所有系数平方之和" ，前面还有一个 λ， 叫 "正则化系数"。

	 λ 给的越大， "强势样本" 对 "模型" 的影响越小 //降低的 "拟合度" 就越低，

​					损失函数  + L1范数  ----》  Lasso回归

​					损失函数  + L2范数  ----》  岭回归	
（22）过拟合 与 欠拟合

	  过拟合：数据中存在强势样本，
	  欠拟合：而 "欠拟合"，模型 "选择比较简单"，数据比较"复杂"，模型比较"简单"了，

（）多项式回归
	而 "欠拟合"，模型 "选择比较简单"，数据比较"复杂"，模型比较"简单"了，
	所以需要 "把模型变得比较复杂"。
    //线性不会转变，可以让 线性变得会转弯
	与 "线性模型" 相比，多项式模型引入了 "高次项"，自变量的指数大于1，

	多项式回归可以理解为线性回归的扩展，在线性回归模型中添加了 "新的特征值".


（40） "机器学习" 分类
	（1）基于 "模型的学习"
			根据 "数据的分布状态"，找到一个数学公式去表达它。
				 如：线性回归、岭回归、Lasso回归、多项式回归。
	（2）基于 "实例的学习"
			 "相似的输入" 必定产生 "相似的输出"。

（41） sklearn 与 "树形结构"

//=============================== 决策树
//=============================== 决策树
（41）"决策树"
	决策树是一种树形结构的监督学习模型，
	通过特征条件不断分裂数据，
	最终用于分类或回归任务。

（42）基本的 "决策过程"
	 划分子表->不断重复 -> 直到 "全匹配的叶级子表"

（43）特征选择：
	 // 首先选择 "哪一个特征" 进行子表划分决定了 "决策树的性能"
		"优先划分" 特征属性比较强的 "特征"，
		什么叫 "特征属性" 
			（1）基尼不存度
			（2）信息熵
			（3）信息增益
			（4）增益率
			（5）MSE
			（6）......

（43）"回归-决策树" 与 "分类问题-决策树"
		回归问题  与 分类问题 都会用到 决策树 ，
		但是它们的 "优先划分" 的特征是不一样的。

（44） "划分决策树" 的目的： 
	   "相同特征的样本" 放到 "同一个节点下"

		"先划分谁" 能够 "最快的" 把 "同一类样本" 划分到 "同一个节点下"，
		谁的 "特征属性" 就比较强。

（45）问题1： 如何 "选取最优特征"？
		示例： 快速的在 "所有的类别" 中找到 "鸟类对象"
			 鸡  鸭  鹅  猫  狗  猪  人  鸟
			（1）是不是两条腿 ————  5： 3
			（2）是不是卵生   ————  4: 4
			（3）有没有羽毛   ————  4: 4
			（4）有没有眼睛   ————  8: 0
			（5）会不会飞     ————  7: 1
		//所以：
		//	会不会飞：  最强
		//	有没有眼睛：最弱

（46）sklearn 里面 "特征属性强弱" 的划分方式：
	 通过谁划分节点，预测的比较准，
	 谁的 "特征属性就强"
	 	准不准 就使用 y - y ' 来评估， 也就是均差误差（MSE）： ∑(Y -Y')^2 / n 来评估

（47） 决策树 两大问题
	 问题1： 如何 "选取最优特征"？
	 问题2： 何时 "停止分裂"？


（48）sklearn提供的决策树  "底层" 为 "cart树（Classification and Regression Tree）（中文名 "回归" 与  "分类" 树）"，
	除了 "cart树" ，还有 "ID3"、"C4.5"等等。

（49） "cart回归树" 在解决 "回归问题" 时的步骤如下
	（1）针对集合S，遍历 "每一个特征"的"每一个value" 
	（2）分别计算 "这2个集合的mse(均方误差)"，
	（3） 找到使（left_mse + right_mse）最小的那个value，
		  记录下 "此时的特征名称" 和 "value"，
		  这个就是 "最佳分割特征" 以及 "最佳分割值"；
	（4） 针对集合left、right分别重复步骤2,3，直到达到 "终止条件" 。


（50）什么是终止条件，终止条件有如下几种：
	（1）"特征已经" 用完了：没有可供使用的特征再进行分裂了，则树停止分裂；
	（2）子节点中 "没有样本" 了
	（3）树达到了人为预先设定的
	（4）节点的 "样本数量" 

（51）决策树 优化：
	（1）预剪枝： 
	（2）后剪枝：
//=============================== 集合算法​
//=============================== 集合算法​

（1）集合学习-多颗决策树
	基于 "决策树的集合算法"，就是按照 "某种规则"，
	构建多棵 "彼此不同" 的 "决策树模型"，
	"分别给出" 针对 "未知样本" 的预测结果，
	最后通过 "平均" 或 "投票" 得到 "相对综合" 的结论。
	常用的 "集合模型（RandomForest）" 包括:

（2）常用的 "集合模型" 包括:
		（1）"Boosting类模型（AdaBoost、GBDT）" 
		（2）"Bagging（自助聚合、随机森林）类模型"。
		/**
			AdaBoost-自适应提升算法:通过不断调整样本权重，让后续模型更关注错误样本。
						// AdaBoost 意思是：正向激励、自适应、增强

			GBDT-梯度提升决策树:通过梯度下降思想，逐步叠加多棵弱树优化误差。
					// GBDT     意思是：梯度提升
			XGBoost-极端梯度提升（优化版梯度提升）：在 GBDT 基础上加入正则化与工程优化，速度更快、效果更好。
		*/
		/**
			Bootstrap Aggregating-自助聚合：通过有放回抽样训练多个模型，再综合结果降低方差。
			RandomForest-随机森林：基于 Bagging 思想，构建多棵随机特征选择的决策树进行集成。
					// 随机森林
		*/



//=============================== 分类问题损失函数
//=============================== 分类问题损失函数

（1）分类问题的评估
	"分类问题" 与  "回归问题" 的评估方式一定不一样。
	"分类问题" 里面， "对了" 就好，"不对" 就不好。
	"回归问题" 里面是要 "越接近越好"。


（2）损失函数
	 对于 "回归问题"，可以使用 "均方差" 作为 "损失函数"，	
	 分类问题采用 "交叉熵" 作为 "损失函数"，

//=============================== 分类问题：逻辑回归
//=============================== 分类问题：逻辑回归

（1）逻辑回归

	 "二分类"（也称为逻辑分类）是 "常见的分类方法"，是将一批样本或数据划分到两个类别，
			//分类结果只有两类

	（1）例如一次考试，根据成绩可以分为及格、不及格两个类别。

（2）逻辑回归
	"逻辑回归" 是一种 "广义的线性回归"，其原理是利用线性模型根据输入计算输出（线性模型输出值为连续），
	并在 "逻辑函数" 作用下，将连续值转换为 "两个离散值（0或1）"，
	其表达式如下：

（3）"逻辑回归" 的套路
	（1）.根据样本数据，构建一个 "线性回归模型"，预测输出（连续）
	（2）.将连续的预测数据，带入到逻辑函数中
	（3）.逻辑函数，将预测值映射到 0-1 区间范围内（将线性转为非线性）
			// 为什么要 将 "线性" 转为 "非线性" ？
			// 因为 "线性模型" 的表达能力不是那么强。
			// "水果的直径" 示例，能说明这个问题
	（4）.找到一个阈值 0.5
			//
	（5）.大于0.5--->1   小于0.5 ---》 0

（4）逻辑函数
		
	"交叉熵"  是度量  "两个类别"  之间的  "概率"  的差异信息的。

	这个值越小越准

//=============================== （分类问题的）模型评估
//=============================== （分类问题的）模型评估

（1）性能度量
	 1） 错误率与精度
		 // 错误率和精度是分类问题中常用的性能度量指标，既适用于二分类任务，

	 2） 查准率（Precision）、召回率（Recall）、F1得分
		//	Precision 看“抓得准不准”
		//	Recall 看“抓得全不全”
		//  F1 得分就是把“准不准”和“全不全”压缩成一个分数。

		//它们也适用于多分类，只是计算方式有所不同呢

	 3） 混淆矩阵
		 "混淆矩阵" 也称 "误差矩阵"，
		 是表示 "精度评价" 的一种 "标准格式"，用n行n列的矩阵形式来表示。

		（1）每一行（数值之和）表示一个真实类别的样本， // "当前类别" 真实样本个数。	
		（2）每一列（数值之和）表示一个预测类别的样本。 // "当前类别" 预测的样本个数。


	4）#### 分类报告
		比"混淆矩阵"更全面，
		它有单独的 "查准率" 和 "召回率"
		//示例：

			 classification_report:
						  precision    recall  f1-score   support

					   0       1.00      1.00      1.00         5
					   1       0.86      0.86      0.86         7
					   2       0.67      0.67      0.67         3

				accuracy                           0.87        15
			   macro avg       0.84      0.84      0.84        15
			weighted avg       0.87      0.87      0.87        15

//=============================== 数据集划分
//=============================== 数据集划分

（1）比例有：9:1, 8:2, 7:3等. 

（2）交叉验证法

//=============================== 模型优化
//=============================== 模型优化

（1）"学习曲线" 与 "验证曲线"

（2）超参数优化
	（1）  网格搜索
			：穷举 "所有参数" 组合，找到 "最优参数" 。
	（1）  随机搜索
			：在参数范围内随机抽取若干组进行尝试。

//=============================== 模型优化
//=============================== 模型优化


支持向量机


