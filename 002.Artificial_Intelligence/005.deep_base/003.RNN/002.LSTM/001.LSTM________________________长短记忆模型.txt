LSTM 是RNN的变种
//==================================================
//==================================================

对RNN的改进，得到了 "长短期记忆网络模型" （Long Short-Term Memory，简称LSTM）

//================================================== LSTM
//================================================== LSTM



	"长短期记忆网络模型"的本质，还是 "循环神经网络"， 
	在其中做了更多的 "子神经网络"，
	通过子神经网络计算，解决了 "梯度消失" 的问题，

//================================================== LSTM 的门
//================================================== LSTM 的门

	"子神经网络" 在LSTM 叫做门，

	它有3门（3个子神经网络）

	【遗忘门】 决定丢弃什么信息//丢弃不重要的特征
	【输入门】 什么信息可以输入进来
	【输出门】 决定输出什么。


这些门计算是 "并行" 进行的
三个门的计算是并行进行的，它们不需要等待前一个门的计算结果。
每个门都有自己的权重参数和输入，通过这些参数来调整记忆细胞中的信息。
这种并行计算的方式有助于 LSTM 网络更有效地捕捉长期依赖关系。


//================================================== 对比RNN
//================================================== 对比RNN
LSTM在 RNN的基础上， 在某一个时刻当中，
又加上了输入门、遗忘门、输出门，
每个门的计算是不是都会更新 "细胞状态"


//================================================== 遗忘门
//================================================== 遗忘门
	遗忘门：决定从细胞状态中丢弃什么信息

			 //什么信息把它删除，门就是子神经网络，

			 //这个遗忘门，就是一个 "子神经网络"

			 //该门会读取H_t-1和X_t，输出一个在0~1之间的数值给每个在细胞状态CT-1中的数字，

			//根据它的计算结果，决定保留什么、丢弃什么
			//它这里使用的激活函数就是sigmoid，它能得到两个结果 0 或 1
			//1表示 "完全保留" ，0 表示"完全舍弃"。		


//================================================== 输入门
//================================================== 输入门
	//============================== 输入门			 
	输入门：决定什么信息输入进来，它包含两个子神经网络的计算。
		   
		   //========== 两个部分
		   //========== 两个部分
			（1）【找到】哪些需要更新的细胞状态。
			（2）是把需要更新的信息【更新】到细胞状态里。 
			
			  其中，tanh层就是要创建一个新的细胞状态值向量————Ct，会被加入到状态中。
			  tanh和sigmid结合乘在一起。
			
			//这两个神经网络结构是一样的，
			//输入数据都是一样的，权重不一样，
			//权重个数一样， 但是值不一样。
			最终会把这两路的值合并
			"细胞状态更新"  会流动到我们怎整个流程上面。
		   //========== 这里会使用tanh 和 sigmoid两个激活函数
		   //========== 这里会使用tanh 和 sigmoid两个激活函数


//================================================== 细胞状态
//================================================== 细胞状态
	在LSTM模型里面，它的核心就是 "细胞状态"，

	用 "贯穿细胞" 的 "水平线" 表示。细胞状态像传送带一样。
	它贯穿整个细胞却只有很少的分支，这样能保证信息不变的流过整个RNNs。

	//============================= 细胞状态的更新
	//============================= 细胞状态的更新
	细胞状态 是在不断的 "更新迭代中"，

	   //具体的更新过程
			//怎么记录的，怎么更新的
		"你能输入进来什么、我要遗忘什么、我最终输出了什么" 就相当于某次细胞状态的更新
		它永远是在算：我能输入什么、我能丢弃什么、输出什么，
		不断的更新迭代保存位置关系，数据顺序等等
		这个流程就是 "细胞状态的更新" 。

	//细胞状态是 "遗忘门" 和 "输入门" 联动的结果。



//================================================== 输出门
//================================================== 输出门
	输出门：决定输出什么。
	//================= 细胞状态
	//================= 细胞状态
	在LSTM模型里面，它的核心就是 "细胞状态"，

	"细胞状态" 就是 "数据怎么更新" 的，
			//怎么记录的，怎么更新的
		"你能输入进来什么、我要遗忘什么、我最终输出了什么" 就相当于某次细胞状态的更新
		它永远是在算：我能输入什么、我能丢弃什么、输出什么，不断的更新迭代
		保存位置关系，数据顺序等等
		这个流程就是细胞状态的更新。

   //========== 这里也会使用 tanh 和 sigmoid两个激活函数，但是 tanh位置不同
   //========== 这里也会使用 tanh 和 sigmoid两个激活函数，但是 tanh位置不同


//==================================================
//==================================================


所以：整个 遗忘门、输入门、输出门、细胞状态 整体计算
	  通过神经网络计算，它就是计算：那些特征重要的，那些特征不重
	  前后有什么依赖性，前后依赖性有多大，
	  这些全部都是每个子神经网络、权重、偏置说了算的。

	  谁和谁有关联，前后依赖性有多大，是模型要学习的。




