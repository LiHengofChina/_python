

//============================================== 
//============================================== 

深度学习还有一种基本模型 Transformer，


Transformer是一种 "注意力机制驱动的模型"，它引入了 "自注意力机制"，
允许模型在处理序列数据时关注不同位置的信息。
Transformer的架构不依赖于序列的顺序，允许并行处理输入序列。



发源于NLP，后来也引入到图像识别中



用自注意力机制，能够同时关注序列中的所有位置，
不依赖于序列的顺序。

适用于各种序列任务，如机器翻译、语言建模等。


