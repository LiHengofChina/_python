
//========================================== LeNet  1998  Yann LeCun 5层
//========================================== LeNet  1998  Yann LeCun 5层


LeNet  1998  Yann LeCun,用于解决 "手写 数字 识别 视觉任务（最后应用于了邮政编码识别上面）"，
自那里起，CNN的最基本的架构就定下来了：卷积层、池化层、全连接层。//它它CNN都是在这个上面优化

它也是第一个 "卷积神经网络"，开山鼻祖。



	//最后一层 "全连接的神经元" 个数：10个，分类业务里面，有几个输出，最后一层，就有几个神经元，


	//  一个 "卷积层、池化层" ，算一层，
	//  一个全连接算1层，
	所以 LetNet 是 1 + 1 + 3 = 5 层


//======== LeNet 
//======== LeNet 

输入 ： 32 * 32 大小 单通道图像
两个 ： 卷积-池化层
第一个  全连接层神经元数目为500，再接激活函数
第三个  全连接层神经元数目为10，得到10维特征向量，用于10个数字的分类训练，送入softmaxt分类，
		得到分类结果的概率。

然后根据分类结果，构建损失函数，使用反向传播算法，梯度下降，求损失函数的极小值。
..



//========================================== AlexNet  2012   冠军 8层
//========================================== AlexNet  2012   冠军 8层


使用ReLU作为激活函数
......

使用Dropout（丢弃学习）"随机忽略一部分防止过拟合" 。
						//绑沙袋都能跑了，那么不要沙袋，肯定跑得更好。

平均池化，修改为最大池化

局部正规化：

提出了LRN：强者更强，弱者更弱

使用CUDA加速深度卷积网络训练，
利用GPU强大的并行计算能力，处理神经网络训练悍大量的矩阵运算。
	/***
		传统机器学习，都用的是CPU训练模型，它的速度比较慢一些，
		到了深度学习中，参数量那么大，
	*/

	//============================面试题 ：
	问：训练模型的时候，使用的是什么设备：
			GPU
	问：什么设备、什么型号、整个模型用它训练了多长时间？
			一般公司有两个服务器：
				一个训练模型
				一个保存模型

			有些小型公司租云服务器：按时收费

//======== AlexNet 
//======== AlexNet 

输入数据： 224 * 224 * 3 大小。
			//参加图像比较的，都是这么大的图


输出层都是有 1000 个神经元


卷积核： 11 * 11 * 3 ，步长为4，因为算力的问题，使用的是大的卷积核，减少计算
			//卷积核越小，能卷出来的特殊越多，


前5层是 卷积-池化组，
后3层是 全神经连接


//========================================== VGG 2014年 ，亚军 （卷中卷） 19
//========================================== VGG 2014年 ，亚军 （卷中卷） 19


当前冠军是GoogLeNet， 
但是VGG结构简单，应用性强
	很多技术 人员都喜欢使用基于VGG网络的模型，如：YoLov2

网络深度： 11 13 16 19	//一般使用 16  或 19 层的


卷积配置相同：区域都是 3*3 ，步长为 1 ，填充为1的卷积
池化：步长为 2*2 ，步长为 2

" 5 个组卷积-池化层" + " 3 个全连接层"

	//但是VGG里面 5 个组卷积-池化层里面，并不是只卷一次。
	//而 AlexNet 只卷了一次。

	//======================= 怎么算层数
	//======================= 怎么算层数
	/**
		有参数变化的才算一层，
		有 "要学习的参数" 才算一层，没有就不算一层

		卷积算、
		全连接算

		//=============
		"池化" 不算
		"激活函数" 也不算
		"池化" 和 "激活函数" 没有要学习的参数
	*/
	//=======================
	//=======================
	" 5 个组卷积-池化层" 
	输出是： 7 * 7 * 512  的结果，
	把它交给全连接进行计算


	全连接 "还是只能接受一维" 特征，
	7 * 7 * 512 需要拉伸成一维，交给全连接进行计算
	//现在的 7 * 7 里面的的一个像素，就代表的是原始图像中的一大片区域了
	//这一大片区域是多大，就叫：感受野



//========================================== GoogLeNet 2014年 ，冠军 22层
//========================================== GoogLeNet 2014年 ，冠军 22层

 
GoogLeNet 包含大量的 "Inception 并行卷积结构" 。



	//Inception

	同一个输入数据，作了好几次并行输入结果。
			//====================
					地球是圆的，为什么我们看不到，因为我们的感受太小了。


			（1）几次 "并行输入" 结果，它有小图、也有大图、也有
				  用 1*1 看细节，
				  用 3*3 看中午图像
				  用 5*5 看全貌
				  然后进行特征融合
				  所以它理解越来，这个就清楚了。

			（2）在做  1*1  、 3*3 、 5*5 之前，
				 会进行1*1卷积，因为它可以降维。

//========================================== Res Net （残差网络）152 层
//========================================== Res Net （残差网络）152 层
太深容易梯度消失，
之所以能做到152层，

是因为它一定是有什么手段，缓解了它 "梯度消失"这么一个问题


Res Net 采用一个 "残差结构"，来缓解梯度消息的问题。

		//======================= 残差结构
		就像玩游戏里面的传声桶一样，
				第一个人拿到答案， 传给第二个人，
				第二个人传给三个人
				第三个人传给四个人
				第四个人传给五个人
				第五个个回答的时候，已经面目全非了。
		那么如果，
			第一个人传给二个人的时候：也把答案告诉第二个人
			第三个人传给四个人的时候：也把第一个人传给二个人的内容和答案给第四人
			......
			依次类推
			把输入数据和卷积结果融合一起去，
			共同交给下一层去计算。
			就是这样一个残差结构。
		//=======================
			残差结构 不光在 ResNet 里面有应用，
			在其它很多复杂的技术里面都是有应用的

			比如：yolov3模型（目标检测模型）
		//=======================ResNet
		ResNet 也有很多层的：
		18-layer
		34-layer
		50-layer
		101-layer
		152-layer
		//=======================
		ResNet 也经常作为骨干网络来提取特征

		5个卷积池化组，配上一个全连接，交给输出层
		//=======================残差，
		把输出数据和卷积结果融合到一起防止梯度消失的问题

		ResNet引入了残差块（residual block），这些块允许网络学习残差函数，即将输入直接添加到网络层的输出，
		而不是通过激活函数。这有助于缓解梯度消失问题，使得训练更加容易。
		//=======================对比  GBDT：梯度提升
		但在 ResNet 中，残差是指网络层的输入和输出之间的差异。
		在 GBDT 中，残差是指实际观测值和模型预测值之间的差异。

//==========================================
//==========================================

