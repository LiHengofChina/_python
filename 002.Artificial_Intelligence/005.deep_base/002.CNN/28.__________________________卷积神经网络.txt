

//============================================= "卷积神经网络"
//============================================= "卷积神经网络"


总体结构：
	通常情况下,
	"卷积神经网络" ：
			由若干个：
				卷积层(Convolutional Layer)、
				激活层(Activation Layer)、
				池化层(Pooling Layer)
				及全连接层(Fully Connected Layer) 
			组成。



//=============="深度神经网络"结构如下：
输入
——> 全连接层
——> 
//深度神网络需要把输入数据拉伸成一维数据，图像中没用的参数也会加进去


//=============="卷积神经网络"结构如下：
//=============="卷积神经网络"结构如下：

输入：
	——>{卷积层(Convolutional Layer)——>激活(Activation Layer)——>池化层(Pooling Layer)}
	——>{卷积层(Convolutional Layer)——>激活(Activation Layer)——>池化层(Pooling Layer)}
	——> 全连接层
	——> 全连接层



所以：

	（1）卷积神经网络 先进行  "卷积、激活、池化" 组，组处理之后，
		//其中：卷积最重要，卷积是提取特殊
		//激活层：对卷积结果套一个激活函数。
		//池化：降维

		 可以再次进行 "卷积、激活、池化" 组处理，
			//进行多次 "卷积、激活、池化" 组处理

	（2）最后交给 全连接层 进行计算，同样的 全连接层 想写几层写几层
		最后输出分类结果

	//================================ 总结：
	//================================ 总结：
	这就是 "卷积神经网络" 的结构，
	它是在 "n个全连接层" 前面加上了 "n个卷积、激活、池化 组"
	它相当于在 "全连接神经网络" 基础上面增加了 "卷积、激活、池化 组"


	//================================ 
	之前是直接交给 "全连接神经网络" 计算
	现在是先对数据提取特征、降维、再交给全连接进行计算



//============================================= 卷积、激活、池化 组 （是核心）
//============================================= 卷积、激活、池化 组 （是核心）


//=============== 卷积 （核心）:提取特征，降维处理
//=============== 卷积 （核心）:提取特征，降维处理


卷积是 "核心"，因为它能提取特殊。

通过 "卷积" ，能达到 "降维处理" 和 "提取特征(指特殊图)" 的目的。
							/*
			提取特征: 指特征图

			降维处理：  	（1）尺寸小了
						    （2）通道图少了，也叫降维

							 如 ：有 5 * 5 的特征图有18个
								 （？18个对应位置相加合在一起吗?），
								 然后用 10个 3*3 s=1 p=1 的卷积核卷积核进行卷积，
								  得到的结果是 5 * 5 ，通道数只有10个了
								  还是 5*5 ，但是通道数少了。
							  //特征图数量 ：有可能有多个 "卷积核"，得到多个特征图
							*/


//=============== 激活
//=============== 激活

将前一层的 "线性输出" 通过 "非线性的 '激活函数' 进行处理"，
这样用以模拟 "任意函数"，从而 "增强" 网络的表征能力。

	//相当于将 卷积之后的结果交给 ReLU 进行计算
	//ReLU是目前使用较多的激活函数，
	//主要原因是它 "收敛" 更快，主要原因在于它部分解决了 "梯度消失" 问题。

	只是将 "卷积之后的结果" 交给 "激活函数进行计算"，不能称之为一层

//=============== 池化层(Pooling Layer)}
//=============== 池化层(Pooling Layer)}

也称为 "子采样*（Subsampling）" 或 "下采样"，
目的是 "缩小高、长方向上的空间的运算" 、
以降低 "计算量"，提高 "泛化能力"。

//=======
"卷积 和 激活 之" 后的数据才做池化 

//=======
如：2 * 2 区域，步长为2的池化

做法：	拿到 2*2 区域的最大像素值，就得到了池化结果

所以池化就是指：要做几乘几，步长为几

最常用的就是： 2*2，步长为2的池化，它能达到的效果，就是将宽度和高度缩小为一半

//=======概念
将输入切割成 "若干大小相等" 的块，对每一个区域取 "最大值或平均值"，组成一个新的矩阵

Max池化：取最大值 //图像领域用的最多
Average池化：取平均值 //会让图像模糊

它也能起到降维的作用

//======= 池化层的特殊
//======= 池化层的特殊

（1）池化里面 "没有要学习的参数"
（2）"通道数" 不会变化，它只是一个取最大值，或最小值的过程
（3）对 "微小位置的变化具有鲁棒性（健壮）"，
		输入数据发生微小变化时， 池化仍会返回相同的结果。


//=============== 全连接层
//=============== 全连接层
//和之前的全连接是一样的。

这一层相当于 "多层感知机(MLP)"，其在整个 "卷积神经网络" 中起到 "分类器" 的作用

通过 "前面 多个 "卷积-激活-池化" 层的反复处理，
待处理的数据特性已有了 "显著提高"：
	（1）输入数据维度已下降
	（2）此时全连接层输入的数据已是经过反复提纯的结果
			不再是 "泥沙俱下、鱼龙混杂"	
因些输出的分类 的品质要高得多。
		//参数少了，精度高了


//============================================= 其它
//============================================= 其它

卷积 和 全连接 谁速度快
	//全连接 速度更快

	//卷积 比全连接更复杂

