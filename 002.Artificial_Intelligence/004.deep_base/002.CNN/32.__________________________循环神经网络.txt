
//======================================== 循环神经网络
//======================================== 循环神经网络

是针对  "神经网络" 的 "另外一个改进方向"

//================== CNN的不足
//================== CNN的不足
CNN的条件：
（1）假设数据之间是独立的，
			//就是每个像素值 和 其它像素值 是没有关系的
		在处理前后 依赖、序列问题（语音、文本、视频）时就显的力不从心。
		它们前后次序非常敏感。
		如：文本，前后有依赖的
					中华人民共 _ _
					//文本前后是有依赖的

（2）标准CNN网络：输入还必须都是 "等长的向量" 。
	 而序列数据 "长度是可变的"。

	 而模型是固定的，这就要求输入的图片大小也是一样的



"标准CNN" 对 "连续的输入数据" 处理并不好

当你的数据是 "语音、文本、视频" 的时候，它前后之间是有 "依赖关系" 的
		//这种数据叫序列数据

//======================================== 循环神经网络
//======================================== 循环神经网络

"循环神经网络RNN" 是一类具有 "短期记忆" 的 "神经网络"，
适用于处理: "视频、语音、文本等与时序相关的问题"

适合用于处理 "视频、语音、文本" 等与 "时序相关" 的问题。

t2时刻记录了t1的状态
t3时刻记录了t2的状态



（1）RNN的连接不仅存在于相邻的层之间，
（2）还存在于时间维度上的隐藏层与隐藏层之间（反馈连接，h1到ht），
		某个时刻t，网络的输入不仅和当前时刻输入相关，
		也和上一个时刻的隐状态相关



//======================================== 循环神经网络内部结构
//======================================== 循环神经网络内部结构

（1）t-1 时刻
		X_t-1	//t-1时刻的输入
		H_t-1	//t-1时刻的状态

（2）t 时刻
		"t-1时刻的输入" 和 "t-1时刻的状态" ，交给t时刻进行计算
		X_t		//t时刻的输入
		H_t		//t时刻的状态

（3）t+1 时刻
		"t时刻的输入" 和 "t时刻的状态" ，交给 t+1 时刻进行计算
		X_t+1		//t时刻的输入
		H_t+1		//t时刻的状态



所以：它就这样 "不断的循环"，不断的循环，
"下一时刻" 同样记录了 "上一时刻" 的状态。


//======================================== RNN模型 输入输出关系对应模式
//======================================== RNN模型 输入输出关系对应模式

（1）one to one
		//一对一

（2）one to many		//文本分类
		//一对多

（3）many to one		//情感分析
		//一对多

（4）many to many//异步 //机器翻译
		//一对多

（5）many to many//同步 //记性标注
		//一对多

//========================================
//========================================

（1）擅长
	语音识别、语言建模、翻译、图像字幕。
	它能根据近期一些信息来执行/差别当前任务

	白色的云朵漂浮在蓝色的____
	我和他中午一起吃了个____
	天空中飞过来一只____

	//通过前面的数据去预测后面的数据。

（2）不擅长
	RNN不擅长处理 "远期依赖性" 任务。

	例如：
		我生长在中国，家有三亩一分地，
			我是家里的老三，我大哥叫大狗子，
			二哥叫二狗子，
			我叫三狗子
			我弟弟叫狗升子，我的母语是____

	//太远了
	//================================================== LSTM
	//================================================== LSTM
	对RNN的改进，得到了 "长短期记忆网络模型" （Long Short-Term Memory，简称LSTM）


	"长短期记忆网络模型"的本质，还是 "循环神经网络"，
	在其中做了更多的 "子神经网络"，
	通过子神经网络计算，解决了 "梯度消失" 的问题，
	"子神经网络" 在LSTM 叫做门，


	//============================== 遗忘门
	遗忘门：决定从细胞状态中丢弃什么信息
			 //什么信息把它删除，门就是子神经网络，
			 //该门会读取H_t-1和X_t，输出一个在0~1之间的数值给每个在细胞状态CT-1中的数字，
			 //1表示 "完全保留" ，0 表示"完全舍弃"。
			 F_t = ....
	//============================== 输入门			 
	输入门：决定什么信息输入进来，它包含两个子神经网络的计算。
			  一部分：找到哪些需要更新的细胞状态。
			另一部分：是把需要更新的信息更新到细胞状态里。 
					  其中，tanh层就是要创建一个新的细胞状态值向量————Ct，会被加入到状态中。
					  tanh和sigmid结合乘在一起。
			//这两个神经网络结构是一样的，输入数据都是一样的，权重不一样，
								//权重个数一样， 但是值不一样。
			最终会把这两路的值合并

			"细胞状态更新"  会流动到我们怎整个流程上面。
			......




	输出门：决定输出什么。


	//================= 细胞状态
	//================= 细胞状态
	在LSTM模型里面，它的核心就是 "细胞状态"，

	"细胞状态" 就是 "数据怎么更新" 的，
			//怎么记录的，怎么更新的
		"你能输入进来什么、我要遗忘什么、我最终输出了什么" 就相当于某次细胞状态的更新
		它永远是在算：我能输入什么、我能丢弃什么、输出什么，不断的更新迭代
		保存位置关系，数据顺序等等
		这个流程就是细胞状态的更新。



