

//=============================================
//=============================================

有了 "神经网络" 之后，它在理论上可以解决 "所有问题"，

在实际中，可能有一些 "其它的问题"


"全连接神经网络" 有一定的 "局限性"

所以出现了 ：
		CNN：卷积神经网络
		RNN：循环神经网络





//=============================================================================== "局限性"
//=============================================================================== "局限性"

（1）未考虑数据形状：
		全连接 "神经网络" 它只接受 "一维的数据"

		//如：图片是二维数据，如果将二维数据降维成一维数据之后
		//数据里面的线性关系就破坏了。
		//灰度图像还好，彩色图像破坏的更严重

	 不管任何数据，语音、文字等等，
	 交给全连接神经网络，就需拉伸为一维数据，这就会损坏数据。

（2）参数庞大，需要降低参数量

	如： 800px * 800px 的图像 有64万个像素值。
		800 * 800 = 640000，
		每一个像素值都会作为一个X。

		假如有1024个神经元，64万 就会与 1024 个神经元进行全连接。

		连接出来的就是 (640000,1024)  1024行，1024列。

		640000 * 1024  还要加上1024个偏置
		......

	这么一张简单的图像就有 "6亿多个参数"

	其实主要还是 "输入值特征" 太多了，

	它只不过是一张图像，许多特征是没有用的。



（3）深度受限
		 全连接神经网络，一般不超过7层，
		 如果超过7层：过深会出现 "梯度消失"、"梯度过小"、"梯度过大"，无法进一步提升

		//因为输入数据，一般会做归一化处理，到 0~1 区间，所以梯度会越来越小
		//如果不做归一化，梯度会越来越大

//================================================================================================== 解决办法
//================================================================================================== 解决办法


增加卷积操作，不需要变维成向量，不破坏图像的空间结构，
		//交给卷积运算，对二维能卷，对三维也能卷

		//卷积 的主要作用就是提取特征

		通过卷积提取想要的特征

增加卷积、池化操作。

增加其它策略，解决梯度消失的问题。




