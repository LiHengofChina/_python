
"感知机" 无法解决异或问题

//============================================================================== 多层感知机
//============================================================================== 多层感知机

为了解决 感知机的 "异或" 问题： 1975年
	出现了多层感知机（也就是神经网络）

	每一个输入在进行计算的时候，它都 有一个自己的权重



//================== 正向传播
//================== 正向传播


 "上一层的输出" 作为 "下一层的输入"：

//============================================================================== 多层感知机
//============================================================================== 多层感知机

"多层感知机"：其实就是 "多个感知机组合"，（Multi-Layer Perceptron,MLP）
也就是 "神经网络"



//========================================================== 多层前馈神经网络
//========================================================== 多层前馈神经网络
"单个感知机" 功能比较简单，将若干个感知机连接在一起，
形成一个 "级联网络结构"，
这个结构称为 "多层前馈神经网络" （Multi-Layer Feedforward Neural Networks）


//
"多层" 指：输入层、隐藏层、输出层。
"前馈" 指： 上一层的输出作用下一层的输入，也就是正向
			"每一层神经元" 仅与 "下一层的神经元" 全连接。
				// "连接神经网络"



//========================================================== 神经网络
//========================================================== 神经网络


//========================================================== 通用近似定理
//========================================================== 通用近似定理


1989 年，奥地利学者库尔特·霍尼克（Kurt Hornik）等人发表论文证明:
对于 "任意复杂度的连续波莱尔可测函数（Borel Measurable Function）f"，
仅仅需要一个 "隐含层" ，只要 "这个隐含层" 包括 "足够多的神经元"，
（"前馈神经网络" 使用 "挤压函数（Squashing Function）作为激活函数"），
就可以 "以任意精度" 来 "近似模拟f"。


//如果想增加 "f的近似精度" ，单纯依靠 "增加神经元的数目" 即可实现。
• 这个 "定理" 也被称为：
	"通用近似定理（Universal Approximation Theorem）"，
	"万能近似定理"
该定理表明， "前馈神经网" 在理论上可 "近似解决" 任何问题。




//========================================================== 通用近似定理 使用场景
//========================================================== 通用近似定理 使用场景


（1）
	用 "神经网络" 拟合 "高次方程"



（2）
	f(猫) 			——————————> 猫

	f(一段语音)		——————————> Hello,deep learning

	f('哈哈哈哈')	——————————> 高兴


//========================= 神经网络的发展方向
//========================= 神经网络的发展方向


一层 "神经元" : 又浅又胖

减少 "单层神经元数量"，增加 "网络层数"
	//变成 又深、又瘦的网络模型

	微软研究人员针对 "以上两类网络性能" 展开了实验：
		结果表明，增加 "网络的层数" 会显著提升 "神经网络系统" 的 "学习性能"


//=========================  表示法（这种表示法不唯一、了解即可）
//=========================  表示法（这种表示法不唯一、了解即可）
 
	当 层数多的时候，
	每一层又有 很多 "权重和偏执"。
//=======写法

	  (1)	//(1)表示第一层
	W
	  1 2

	  // 2表示前一层第2个
	  // 1表示后一层第1个



//============================================================================================= 神经网络的计算公式
//============================================================================================= 神经网络的计算公式


//============================================== 举例说明 ：
//============================================== 举例说明 ：

两个x输入，3个神经元

//===================== 计算第一层 "三个神经元" 的输出
//===================== 计算第一层 "三个神经元" 的输出

（第1步：）	输入数据，有X1和X2。 也就是（1，2），1行2列
					//1行2列，指的是一条数据，有两个特征。
		    公式表示为：
						X = (X1   X2)




（第2步：）	W有2行3列，每个X都有一个W，2个X和3个神经元，就是（2，3），2行3列
				   //2行3列:指每个输入，在与每个神经元相乘的时候，都有自己的偏置。

					 (1)
		    表示为：W   =
							 (1)		 (1)		  (1)
							W 			W			 W
							 11           21           31

							 (1)		 (1)		  (1)
							W 			W			 W
							 12           22           32

（第3步：） 	（1，2）一行两列 和（2，3）两行三列 "矩阵相乘"， 得到(1，3)也就是 1行3列

		 (1)	  (1)
		A	 =  W      *  x 
		//矩阵相乘：  "a的所有行" 乘以 "b的所有列" ，相乘之后，对应位置再相加，
		//得到：	一行3列指：一个样本有3个输出。



（第4步：）矩阵相乘之后，再加上 "偏执"的值。




//=========================================================== 话外音
//=========================================================== 话外音         



所以 "神经网络" 怎么实现的：矩阵相乘 就能实现。

//===================== 计算第二层 "两个个神经元" 的输出
//===================== 计算第二层 "两个个神经元" 的输出
//=====================
（第5步：）把 "第一层的3个输出" 作为 "第2层的3个输入" 再进行计算
		(1,3) * (3,2)  = (1,2)  得到的结果
		//====================
		 (2)	  (2)
		A	 =  W      *  x 


//===================== 计算第三层 "两个个神经元" 的输出
//===================== 计算第三层 "两个个神经元" 的输出
//=====================
（第6步：）把 "第二层的2个输出" 作为 "第3层的2个输入" 再进行计算
		(1,2) * (2,2)  = (1,2)  得到的结果


		最后得到两个预测值：
		//=========
		为什么有两个预测值：
				一个猫类别是 		?%
				一个是狗类别是  	?%

//=====================
//=====================

不管是 "线性问题"，还是 "非线性问题"，都是通过它去计算


