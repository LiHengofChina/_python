

//============================================================ 传统分词技术
//============================================================ 传统分词技术

（1） 中文分词：
		（1）基本规则的分词 

		（2）基本统计的分词
			//基于统计规则 和语言模型

		（3）混合分词
			//





//============================================================
//============================================================
  中文分词：
		（1）基本规则的分词 
			//维护词典，去词典里面查询
			（1）正向最大匹配 FMM
				例如：吉林市长春药店//最长词为4个字
				吉林市长：词典无
				吉林市：词典有
				
				长春药店：词典无
				长春药：词典无
				长春：词典有，成词
				
				药店：词典有，成词
				
				分词结果：吉林市/长春/药店

			（2）逆向最大匹配 RMM
				例如：吉林市长春药店//最长词为4个字

				长春药店：词典无
				春药店：词典无
				药店：词典有，成词

				林市长春：词典无
				市长春：词典无
				长春：词典有，成词

				吉林市：词典有，成词

				分词结果：药店/长春/吉林市 --> 吉林市/长春/药店

			（3）双向
				（1）数量一样，结果相同，任意取
				（2）不一样取少的
				（3）数量一样，结果不同，看单字成词的情况，选择单字成词少的

			/**
				（1）（需要维护词典）

				（2）工作中直接使用第三方库 ：jieba库===========================================================================
						（1）安装
							 pip install jieba==0.42.1
 
				（3）案例：统计一篇文章当中的记频率
							把文章先做一个分词，
							再统计词语出现的次数，记录到字典里面，
							
							通过词语判断文章的主要意思。
							高频词汇能在一定程序上代表这篇文章的主体意思
							
			*/