

语言模型在文本处理、信息检索、机器翻译、语音识别中承担这重要的任务。

从通俗角度来说，语言模型就是通过给定的一个词语序列，预测下一个最可能的词语是什么。

传统语言模型有N-gram模型、HMM（隐马尔可夫模型）等，

著名的语言模型有神经网络语言模型（Neural Network Language Model，NNLM），
循环神经网络（Recurrent Neural Networks，RNN）等。



//=========================== 语言模型
//=========================== 语言模型

它能根据前一个词预测出下一下词是什么

//CHAT GPT 就是词语接龙，它是一个生成模型，
//它是大语言模型  LLM

语言模型的两种解释：
（1）计算句子的概率
（2）根据前面N个词预测下一个词





//=========================== （1）N-gram模型
//=========================== （1）N-gram模型



一个句子中，距离越远的词关系越弱，越近的时，才是关系越强的
			//所以只考虑一定范围内容的词
			//N-gram模型就是这样一个模型

			当 N=1 时，就好像每个词是不相互影响的。
			当 N=2 时，就只考虑前一个词，这个就叫二元模型。
			当 N=3 时，称为三元模型。

			N越长，


//=========================== （2）神经网络模型 NNLM
//=========================== （2）神经网络模型 NNLM 

它还是一个N元结构：

神经网络语言模型（NNLM）
NNLM 是利用视角网络对N元条件进行概率估计的一种方法，


//======================= 重点
//======================= 重点

它还是 "根据前面几个词" 根据神经网络，预测出下一个词，


它是一个全连接网络，自然语言处理里面很少有卷积出现。


//=========================== （3）Word2vec 库
//=========================== （3）Word2vec 库

Word2vec 库：专门用于词嵌入训练库
Word2vec是Goolge发布的、应用最广泛的词嵌入表示学习技术

//================== 它包含两个改进版本的 NNLM 模型
//================== 它包含两个改进版本的 NNLM 模型
（1）CBOW 连续词袋模型	
		呼伦贝__大草原
		//根据上下文预测中心词
（2）Skip-gram:跳字模型，	//更强
		根据中心词预测上下文
		______尔_____

//=========================== （4）RNN
//=========================== （4）RNN

	（1）原生RNN，具有短期的记忆能力
	（2）长短期记忆模型（LSTM）
				它有3个子神经网络
				输入门：
				遗忘门：
				输出门：
	（3）双向循环神经网络（BRNN）：
			我喜欢苹果，比安卓用起来更流畅些
			我喜欢苹果，基本上每天都要吃一个
		    //由两个循环神经网络组成，一个正向、一个反向，两个序列连接同一个输出层。
		    //正向RNN提取正向序列特征，反向RNN提取反向序列特征。

