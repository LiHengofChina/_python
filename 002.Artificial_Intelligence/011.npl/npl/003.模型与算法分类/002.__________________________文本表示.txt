
文本表示： 就是把文字、字符串、文本转换成 "数字、特征向量"


//===================================================== 文本表示方式
//===================================================== 文本表示方式

（1） One-hot 独热编码
		减肥：[1, 0, 0, 0, 0]
		瘦身：[0, 1, 0, 0, 0]
		增重：[0, 0, 1, 0, 0]

		//缺点
		长度为5的向量表示5个词
		长度为100万的向量才能表示100万个词
		有多少词就需要多大的向量

		"瘦身"  和 "增重"  语义完全相反
		余弦相似度 和欧式距离都为0
		所以但是它们不能表示语义相似度。

		表示效率比较差
（2） 词袋模型//类似于词频
		//把每个词语出现的次数统计出


		 （1）原句
		     我把他揍了一顿，揍得鼻青眼肿
		     他把我揍了一顿，揍得鼻青眼肿
		 （2）构建一个词典：
		     {"我":0, "把":1, "他":2, "揍":3, "了":4 "一顿":5, "鼻青眼肿":6, "得":7}
		 （3）原句
			 [1, 1, 1, 2, 1, 1, 1, 1]
			 [1, 1, 1, 2, 1, 1, 1, 1]

		  //缺点
			丢失了顺序的语义信息
		    也是稀疏矩阵
（3）TF-IDF 词频-逆文档频率
			一个词语在文档中出现的次数越多、出现的文档越少，语义贡献度越大

（4）共现矩阵
			I like deep learning.
			I like NLP.
			I enjoy flying.

			"共同出现" 的次数越多，它们的 "相似度" 越高。

			飞机、大炮
			苹果和梨
			马克思和恩格斯


（5） N-Gram
				N-Gram模型是一种基于统计语言模型
				N-Gram基本思想是将文本里面的内容按照字节进行大小为n的滑动窗口操作，形成了长度是n的字节片段序列	

			句子：L love deep learning
			Bi-gram: {I, love}, {love, deep}, {deep, learning}
			Tri-gram: {I, love, deep}, {love deep learning}


（6）词嵌入
词嵌入（word embedding）是一种词的向量化表示方式，
能保留丰富的语义信息。

		两个词的 "词向量" 相似度比较高，
		我们就可以判断 它们的语义相似度比较高

		//词嵌入 是一种 "词的向量化" 表示方式，
		//该方法将词语映射成为 "一个实数向量"
		//同时保留词语之间语义的 "相似性和相关性"


		//（2）语义级的计算
		男人-女人  约等于  国王-女人

		//稠密向量
		//能表征 相似度

		一定有一种自动化的算法，去生成这种词向量表示。












